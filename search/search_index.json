{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"gallifrey","text":""},{"location":"#introduction","title":"Introduction","text":"<p>gallifrey is a package for structure discovery in time series data using Gaussian Processes with explicit applications to exoplanet transit lightcurves.  It is a JAX-based python implementation of the julia package AutoGP.jl by Feras Saad.</p>"},{"location":"#installation","title":"Installation","text":"<p>Option 1:  Installation using pip (Recommended)</p> <pre><code>pip install gallifrey\n</code></pre> <p>Option 2: Installation from Source</p> <ol> <li> <p>Clone the repository:</p> <pre><code>git clone git@github.com:ChrisBoettner/gallifrey.git\ncd gallifrey\n</code></pre> </li> <li> <p>Install the package:</p> <p><pre><code>pip install .\n</code></pre> (or, for development mode: <code>pip install -e .</code>)</p> </li> </ol>"},{"location":"#citation","title":"Citation","text":"<p>If you use gallifrey in your research, please cite it as:</p> <pre><code>@article{https://doi.org/10.1051/0004-6361/202554518,\n  doi = {10.1051/0004-6361/202554518},\n  author = {Boettner, Christopher},\n  title = {gallifrey: JAX-based Gaussian Process Structure Learning for Astronomical Time Series},\n  year = {2025},\n  journal = {A\\&amp;A},\n  publisher = {EDP Sciences},\n  issn = {0004-6361, 1432-0746},\n  eprint = {2505.20394},\n  archiveprefix = {arXiv},\n  primaryclass = {astro-ph},\n  keywords = {Astrophysics - Earth and Planetary Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics},\n  copyright = {{\\copyright} 2025, ESO},\n}\n</code></pre> <p>And please also cite the original paper by Saad et al.</p> <pre><code>@article{https://doi.org/10.48550/arxiv.2307.09607,\n  doi = {10.48550/ARXIV.2307.09607},\n  url = {https://arxiv.org/abs/2307.09607},\n  author = {Saad,  Feras A. and Patton,  Brian J. and Hoffman,  Matthew D. and Saurous,  Rif A. and Mansinghka,  Vikash K.},\n  keywords = {Machine Learning (cs.LG),  Artificial Intelligence (cs.AI),  Methodology (stat.ME),  Machine Learning (stat.ML),  FOS: Computer and information sciences,  FOS: Computer and information sciences},\n  title = {Sequential Monte Carlo Learning for Time Series Structure Discovery},\n  publisher = {arXiv},\n  year = {2023},\n  copyright = {arXiv.org perpetual,  non-exclusive license}\n}\n</code></pre>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This package is a direct re-implementation of AutoGP.jl and would not be possible without it.  The Gaussian Procress implementation is strongly inspired by the fantastic packages GPJax and tinygp.</p>"},{"location":"comparison_with_simple_kernel/","title":"Comparison with simple kernel","text":"<p>In this notebook, we compare the performance of <code>gallifrey</code> models with that of more classical GP approaches. We will fit transit parameters using the learned covariance structure of <code>gallifrey</code> through SMC, and a simple RBF kernel where we only perform sampling over the continous parameters and not the kernel structure.</p> <p>Same as in the other tutorials, we first setup the notebook</p> In\u00a0[1]: Copied! <pre>import multiprocessing\nimport os\n\nos.environ[\"XLA_FLAGS\"] = (\n    f\"--xla_force_host_platform_device_count={multiprocessing.cpu_count()}\"\n)\n</pre> import multiprocessing import os  os.environ[\"XLA_FLAGS\"] = (     f\"--xla_force_host_platform_device_count={multiprocessing.cpu_count()}\" ) In\u00a0[2]: Copied! <pre># import libraries\nimport pathlib\nfrom collections import OrderedDict\n\nimport blackjax\nimport jax\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom blackjax.util import run_inference_algorithm\nfrom jax import numpy as jnp\nfrom jax import random as jr\nfrom jaxoplanet.light_curves import limb_dark_light_curve\nfrom jaxoplanet.orbits import TransitOrbit\n\nfrom gallifrey import GPConfig, GPModel, LinearSchedule\nfrom gallifrey.kernels import RBFAtom, SumOperator, ProductOperator\n</pre> # import libraries import pathlib from collections import OrderedDict  import blackjax import jax import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from blackjax.util import run_inference_algorithm from jax import numpy as jnp from jax import random as jr from jaxoplanet.light_curves import limb_dark_light_curve from jaxoplanet.orbits import TransitOrbit  from gallifrey import GPConfig, GPModel, LinearSchedule from gallifrey.kernels import RBFAtom, SumOperator, ProductOperator <pre>gallifrey: Setting flag `JAX_ENABLE_X64` to `True`\ngallifrey: Setting flag `OMP_NUM_THREADS` to `1`\n</pre> In\u00a0[3]: Copied! <pre># notebook settings\n\n# making the plots pretty\nsns.set_theme(\n    context=\"poster\",\n    style=\"ticks\",\n    palette=\"rocket\",\n    font_scale=1,\n    rc={\n        \"figure.figsize\": (16, 7),\n        \"axes.grid\": False,\n        \"font.family\": \"serif\",\n        \"text.usetex\": True,\n        \"lines.linewidth\": 5,\n        # \"axes.grid\": True,\n    },\n)\n\n# setting saving defaults\nsave_figures = True\nload_models = True  # load pre-trained models\nsave_models = False  # save trained models, only works if load_models is False\n\n# set saving paths\npath = pathlib.Path.cwd().parent\nfigure_directory = path / \"figures/kernel_comparison/\"\nif not figure_directory.exists():\n    figure_directory.mkdir(parents=True)\n\n# set a random key for for this notebook\nrng_key = jr.PRNGKey(77)\n</pre> # notebook settings  # making the plots pretty sns.set_theme(     context=\"poster\",     style=\"ticks\",     palette=\"rocket\",     font_scale=1,     rc={         \"figure.figsize\": (16, 7),         \"axes.grid\": False,         \"font.family\": \"serif\",         \"text.usetex\": True,         \"lines.linewidth\": 5,         # \"axes.grid\": True,     }, )  # setting saving defaults save_figures = True load_models = True  # load pre-trained models save_models = False  # save trained models, only works if load_models is False  # set saving paths path = pathlib.Path.cwd().parent figure_directory = path / \"figures/kernel_comparison/\" if not figure_directory.exists():     figure_directory.mkdir(parents=True)  # set a random key for for this notebook rng_key = jr.PRNGKey(77) <p>In this example, we create some toy data for the model comparison. We will try to challenge the Gaussian Processes a bit: We will vary the 'stellar' background on two time scales, and add noise generated by an autoregressive process rather than pure white noise. The AR(1) process is a commonly used model for correlated noise with exponentially decaying ('short memory') autocorrelation. We also create the transit model. For this example, we will treat the period, transit duration, transit timing and impact parameter as known quantities. We will fit the planet-to-star radius ratio and limb darkening coefficients.</p> In\u00a0[4]: Copied! <pre># create transit model\ndef transit_model(t, params, period=10.0, duration=0.2, t0=0.0, impact_param=0.0):\n    orbit = TransitOrbit(\n        period=jnp.array(period),\n        duration=jnp.array(duration),\n        time_transit=jnp.array(t0),\n        impact_param=jnp.array(impact_param),\n        radius_ratio=params[\"r\"],\n    )\n    return limb_dark_light_curve(orbit, jnp.array([params[\"u1\"], params[\"u2\"]]))(t)\n</pre> # create transit model def transit_model(t, params, period=10.0, duration=0.2, t0=0.0, impact_param=0.0):     orbit = TransitOrbit(         period=jnp.array(period),         duration=jnp.array(duration),         time_transit=jnp.array(t0),         impact_param=jnp.array(impact_param),         radius_ratio=params[\"r\"],     )     return limb_dark_light_curve(orbit, jnp.array([params[\"u1\"], params[\"u2\"]]))(t) In\u00a0[5]: Copied! <pre>key, noise_key = jr.split(rng_key, 2)\n\n# generate the deterministic data\nfull_time = jnp.linspace(-0.8, 0.8, 1000)\nbackground = 0.003 * (\n    5 * full_time**2 + jnp.sin(20 * full_time) + 0.3 * jnp.cos(50 * full_time)\n)\n\n# generate white noise\nwhite_noise_stddev = 0.001\nwhite_noise = white_noise_stddev * jr.normal(noise_key, (len(full_time),))\n# generate AR(1) noise\nnoise_auto_corr = 0.1\nar_noise = jnp.zeros(len(full_time))\nar_noise = ar_noise.at[0].set(white_noise[0])\nfor i in range(1, len(full_time)):\n    ar_noise = ar_noise.at[i].set(noise_auto_corr * ar_noise[i - 1] + white_noise[i])\n\n# generate the transit signal\ntransit_params = {\"r\": 0.1, \"u1\": 0.1, \"u2\": 0.3}\ntransit = transit_model(full_time, transit_params)\n\n# generate the light curve\nfull_ight_curve = transit + background + ar_noise\n\n# select a subset of the data as mock observations\nnum_train = 150\nobs_idx = jnp.sort(jr.choice(rng_key, len(full_time), (num_train,), replace=False))\ntime = full_time[obs_idx]\nlight_curve = full_ight_curve[obs_idx]\n\n# get transit mask\ntransit_mask = (time &gt; -0.12) &amp; (time &lt; 0.12)\n\nxtrain = time[~transit_mask]\nytrain = light_curve[~transit_mask]\n</pre> key, noise_key = jr.split(rng_key, 2)  # generate the deterministic data full_time = jnp.linspace(-0.8, 0.8, 1000) background = 0.003 * (     5 * full_time**2 + jnp.sin(20 * full_time) + 0.3 * jnp.cos(50 * full_time) )  # generate white noise white_noise_stddev = 0.001 white_noise = white_noise_stddev * jr.normal(noise_key, (len(full_time),)) # generate AR(1) noise noise_auto_corr = 0.1 ar_noise = jnp.zeros(len(full_time)) ar_noise = ar_noise.at[0].set(white_noise[0]) for i in range(1, len(full_time)):     ar_noise = ar_noise.at[i].set(noise_auto_corr * ar_noise[i - 1] + white_noise[i])  # generate the transit signal transit_params = {\"r\": 0.1, \"u1\": 0.1, \"u2\": 0.3} transit = transit_model(full_time, transit_params)  # generate the light curve full_ight_curve = transit + background + ar_noise  # select a subset of the data as mock observations num_train = 150 obs_idx = jnp.sort(jr.choice(rng_key, len(full_time), (num_train,), replace=False)) time = full_time[obs_idx] light_curve = full_ight_curve[obs_idx]  # get transit mask transit_mask = (time &gt; -0.12) &amp; (time &lt; 0.12)  xtrain = time[~transit_mask] ytrain = light_curve[~transit_mask] In\u00a0[21]: Copied! <pre># visualize the transit\ndata_plot = sns.scatterplot(\n    x=time[~transit_mask],\n    y=light_curve[~transit_mask],\n    label=\"Training Data\",\n    color=\"C0\",\n)\nsns.scatterplot(\n    x=time[transit_mask],\n    y=light_curve[transit_mask],\n    label=\"Masked Transit\",\n    color=\"C5\",\n    ax=data_plot,\n)\n\nsns.lineplot(\n    x=full_time,\n    y=transit,\n    color=\"grey\",\n    alpha=0.3,\n    ax=data_plot,\n    label=\"Underlying Transit\",\n)\n\ndata_plot.set_xlabel(\"Time [Arbitrary Units]\")\ndata_plot.set_ylabel(\"Flux [Arbitrary Units]\")\ndata_plot.set_xlim(-0.8, 0.8)\n\ndata_plot.legend(\n    loc=\"lower right\",\n    fontsize=30,\n    markerscale=1.2,\n)\n\nif save_figures:\n    plt.savefig(figure_directory / \"data_plot.pdf\", bbox_inches=\"tight\")\n</pre> # visualize the transit data_plot = sns.scatterplot(     x=time[~transit_mask],     y=light_curve[~transit_mask],     label=\"Training Data\",     color=\"C0\", ) sns.scatterplot(     x=time[transit_mask],     y=light_curve[transit_mask],     label=\"Masked Transit\",     color=\"C5\",     ax=data_plot, )  sns.lineplot(     x=full_time,     y=transit,     color=\"grey\",     alpha=0.3,     ax=data_plot,     label=\"Underlying Transit\", )  data_plot.set_xlabel(\"Time [Arbitrary Units]\") data_plot.set_ylabel(\"Flux [Arbitrary Units]\") data_plot.set_xlim(-0.8, 0.8)  data_plot.legend(     loc=\"lower right\",     fontsize=30,     markerscale=1.2, )  if save_figures:     plt.savefig(figure_directory / \"data_plot.pdf\", bbox_inches=\"tight\") <p>For this tutorial, we will use the default <code>gallifrey</code> GPConfig, which uses the Linear, RBF, and Periodic kernel as base kernels.</p> In\u00a0[7]: Copied! <pre># create GP model instance\nkey, gallifrey_key = jr.split(rng_key)\nlearned_model = GPModel(\n    gallifrey_key,\n    x=xtrain,\n    y=ytrain,\n    num_particles=8,\n    config=GPConfig(),\n)\n</pre> # create GP model instance key, gallifrey_key = jr.split(rng_key) learned_model = GPModel(     gallifrey_key,     x=xtrain,     y=ytrain,     num_particles=8,     config=GPConfig(), ) In\u00a0[8]: Copied! <pre># run the model\nif load_models is False:\n    key, smc_key = jr.split(key)\n    final_smc_state, history = learned_model.fit_smc(\n        smc_key,\n        annealing_schedule=LinearSchedule().generate(len(xtrain), 30),\n        n_mcmc=75,\n        n_hmc=10,\n        verbosity=0,\n    )\n    if save_models:\n        learned_model.save_state(\n            str(path / \"model_checkpoints/kernel_comparison_learned/final_state.pkl\"),\n            final_smc_state,\n        )\n        learned_model.save_state(\n            str(path / \"model_checkpoints/kernel_comparison_learned/history.pkl\"),\n            history,\n        )\nelse:\n    final_smc_state = learned_model.load_state(\n        str(path / \"model_checkpoints/kernel_comparison_learned/final_state.pkl\")\n    )\n    history = learned_model.load_state(\n        str(path / \"model_checkpoints/kernel_comparison_learned/history.pkl\")\n    )\n\n# update the model with the new state\nlearned_model = learned_model.update_state(final_smc_state)\n</pre> # run the model if load_models is False:     key, smc_key = jr.split(key)     final_smc_state, history = learned_model.fit_smc(         smc_key,         annealing_schedule=LinearSchedule().generate(len(xtrain), 30),         n_mcmc=75,         n_hmc=10,         verbosity=0,     )     if save_models:         learned_model.save_state(             str(path / \"model_checkpoints/kernel_comparison_learned/final_state.pkl\"),             final_smc_state,         )         learned_model.save_state(             str(path / \"model_checkpoints/kernel_comparison_learned/history.pkl\"),             history,         ) else:     final_smc_state = learned_model.load_state(         str(path / \"model_checkpoints/kernel_comparison_learned/final_state.pkl\")     )     history = learned_model.load_state(         str(path / \"model_checkpoints/kernel_comparison_learned/history.pkl\")     )  # update the model with the new state learned_model = learned_model.update_state(final_smc_state) <p>As a comparison, we fit a model with <code>max_depth</code>=0 and only containing the RBF atom. We include the two operators, but since the max_depth is 0 and their probabilities are set to 0, they are just dummies. We effectively skip the MCMC moves over the kernel structure and perform only HMC over the kernel parameters.</p> In\u00a0[9]: Copied! <pre># create GP model instance\nkey, rbf_key = jr.split(rng_key)\n\nconfig = GPConfig(\n    max_depth=0,\n    atoms=[RBFAtom()],\n    operators=[SumOperator(), ProductOperator()],\n    node_probabilities=jnp.array([1.0, 0.0, 0.0]),\n)\n\nrbf_model = GPModel(\n    rbf_key,\n    x=xtrain,\n    y=ytrain,\n    num_particles=8,\n    config=config,\n)\n</pre> # create GP model instance key, rbf_key = jr.split(rng_key)  config = GPConfig(     max_depth=0,     atoms=[RBFAtom()],     operators=[SumOperator(), ProductOperator()],     node_probabilities=jnp.array([1.0, 0.0, 0.0]), )  rbf_model = GPModel(     rbf_key,     x=xtrain,     y=ytrain,     num_particles=8,     config=config, ) In\u00a0[10]: Copied! <pre># run the model\nif load_models is False:\n    key, smc_key = jr.split(key)\n    final_smc_state, history = rbf_model.fit_smc(\n        smc_key,\n        annealing_schedule=LinearSchedule().generate(len(xtrain), 30),\n        n_mcmc=75,\n        n_hmc=10,\n        verbosity=0,\n    )\n    if save_models:\n        rbf_model.save_state(\n            str(path / \"model_checkpoints/kernel_comparison_rbf/final_state.pkl\"),\n            final_smc_state,\n        )\n        rbf_model.save_state(\n            str(path / \"model_checkpoints/kernel_comparison_rbf/history.pkl\"),\n            history,\n        )\nelse:\n    final_smc_state = rbf_model.load_state(\n        str(path / \"model_checkpoints/kernel_comparison_rbf/final_state.pkl\")\n    )\n    history = rbf_model.load_state(\n        str(path / \"model_checkpoints/kernel_comparison_rbf/history.pkl\")\n    )\n\n# update the model with the new state\nrbf_model = rbf_model.update_state(final_smc_state)\n</pre> # run the model if load_models is False:     key, smc_key = jr.split(key)     final_smc_state, history = rbf_model.fit_smc(         smc_key,         annealing_schedule=LinearSchedule().generate(len(xtrain), 30),         n_mcmc=75,         n_hmc=10,         verbosity=0,     )     if save_models:         rbf_model.save_state(             str(path / \"model_checkpoints/kernel_comparison_rbf/final_state.pkl\"),             final_smc_state,         )         rbf_model.save_state(             str(path / \"model_checkpoints/kernel_comparison_rbf/history.pkl\"),             history,         ) else:     final_smc_state = rbf_model.load_state(         str(path / \"model_checkpoints/kernel_comparison_rbf/final_state.pkl\")     )     history = rbf_model.load_state(         str(path / \"model_checkpoints/kernel_comparison_rbf/history.pkl\")     )  # update the model with the new state rbf_model = rbf_model.update_state(final_smc_state) <p>We can know visualise how well boh models fit the data:</p> In\u00a0[11]: Copied! <pre># create predictive distributions and mean/stddev\nxall_norm = []\npredictive_gmms = []\nmeans = []\nstddevs = []\n\nfor model in [rbf_model, learned_model]:\n    xall_norm.append(model.x_transform(full_time))\n    predictive_gmms.append(model.get_mixture_distribution(xall_norm[-1]))\n    means.append(predictive_gmms[-1].mean())\n    stddevs.append(predictive_gmms[-1].stddev())\n</pre> # create predictive distributions and mean/stddev xall_norm = [] predictive_gmms = [] means = [] stddevs = []  for model in [rbf_model, learned_model]:     xall_norm.append(model.x_transform(full_time))     predictive_gmms.append(model.get_mixture_distribution(xall_norm[-1]))     means.append(predictive_gmms[-1].mean())     stddevs.append(predictive_gmms[-1].stddev()) In\u00a0[12]: Copied! <pre># plot the data\nmodel_comparison_lc_plot = sns.scatterplot(\n    x=time[~transit_mask],\n    y=light_curve[~transit_mask],\n    label=\"Training Data\",\n    color=\"C0\",\n)\nsns.scatterplot(\n    x=time[transit_mask],\n    y=light_curve[transit_mask],\n    label=\"Masked Transit\",\n    color=\"C5\",\n    ax=model_comparison_lc_plot,\n)\n\n# plot the two models\nlabels = [\"RBF\", \"Learned\"]\ncolors = [\"C2\", \"C0\"]\nfor i in range(len(means)):\n    predictive_mean = means[i]\n    predictive_stddev = stddevs[i]\n\n    sns.lineplot(\n        x=full_time,\n        y=learned_model.y_transform.unapply(predictive_mean),\n        ax=model_comparison_lc_plot,\n        color=colors[i],\n        alpha=0.7,\n        label=labels[i],\n    )\n\n    for offset in [predictive_stddev, -predictive_stddev]:\n        sns.lineplot(\n            x=full_time,\n            y=learned_model.y_transform.unapply(predictive_mean + offset),\n            linewidth=2,\n            linestyle=\"--\",\n            alpha=0.7,\n            color=colors[i],\n            ax=model_comparison_lc_plot,\n        )\n\n    model_comparison_lc_plot.fill_between(\n        full_time,\n        learned_model.y_transform.unapply(predictive_mean + predictive_stddev),\n        learned_model.y_transform.unapply(predictive_mean - predictive_stddev),\n        color=colors[i],\n        alpha=0.3,\n    )\nmodel_comparison_lc_plot.set_xlabel(\"Time [Arbitrary Units]\")\nmodel_comparison_lc_plot.set_ylabel(\"Flux [Arbitrary Units]\")\nmodel_comparison_lc_plot.set_xlim(-0.8, 0.8)\nmodel_comparison_lc_plot.legend(\n    ncols=2,\n    loc=\"lower right\",\n    fontsize=25,\n    markerscale=1.2,\n)\n\nif save_figures:\n    plt.savefig(figure_directory / \"model_comparison_lc_plot.pdf\", bbox_inches=\"tight\")\n</pre> # plot the data model_comparison_lc_plot = sns.scatterplot(     x=time[~transit_mask],     y=light_curve[~transit_mask],     label=\"Training Data\",     color=\"C0\", ) sns.scatterplot(     x=time[transit_mask],     y=light_curve[transit_mask],     label=\"Masked Transit\",     color=\"C5\",     ax=model_comparison_lc_plot, )  # plot the two models labels = [\"RBF\", \"Learned\"] colors = [\"C2\", \"C0\"] for i in range(len(means)):     predictive_mean = means[i]     predictive_stddev = stddevs[i]      sns.lineplot(         x=full_time,         y=learned_model.y_transform.unapply(predictive_mean),         ax=model_comparison_lc_plot,         color=colors[i],         alpha=0.7,         label=labels[i],     )      for offset in [predictive_stddev, -predictive_stddev]:         sns.lineplot(             x=full_time,             y=learned_model.y_transform.unapply(predictive_mean + offset),             linewidth=2,             linestyle=\"--\",             alpha=0.7,             color=colors[i],             ax=model_comparison_lc_plot,         )      model_comparison_lc_plot.fill_between(         full_time,         learned_model.y_transform.unapply(predictive_mean + predictive_stddev),         learned_model.y_transform.unapply(predictive_mean - predictive_stddev),         color=colors[i],         alpha=0.3,     ) model_comparison_lc_plot.set_xlabel(\"Time [Arbitrary Units]\") model_comparison_lc_plot.set_ylabel(\"Flux [Arbitrary Units]\") model_comparison_lc_plot.set_xlim(-0.8, 0.8) model_comparison_lc_plot.legend(     ncols=2,     loc=\"lower right\",     fontsize=25,     markerscale=1.2, )  if save_figures:     plt.savefig(figure_directory / \"model_comparison_lc_plot.pdf\", bbox_inches=\"tight\") <p>We can see that both models fit the data pretty well. In the regions where training data is available, they are pretty much indistinguishable. But in the the masked region where the transit occurs, we can see clear deviations. The model that uses the RBF kernels predicts signficantly larger values compared to the kernel with a learned covariance structure.</p> <p>We now sample the posterior probability distribution of the transit parameter. We do this in the same way as in the Transit fitting tutorial. We use the probability distribution obtained via the SMC sampling as the distribution for the underlying time series without a transit, and use it as the sampling objective. We again assume flat priors over all transit parameters for simplicity. However this time we explicitly constrain the planet-to-stellar radius ratio and the two limb darkening coefficients to lie between 0 and 1. We'll use the quick-and-dirty approach of simply setting the probability to zero, if any of the parameters is outside of this bound.</p> In\u00a0[13]: Copied! <pre>def objective(params, predictive_gmm, gpmodel):\n    transit_light_curve = transit_model(time, params)\n    residuals = light_curve - transit_light_curve\n\n    log_prob = predictive_gmm.log_prob(gpmodel.y_transform(residuals))\n\n    # enforce (flat) priors\n    log_prob = jnp.where(\n        (params[\"r\"] &lt; 0.0)\n        | (params[\"r\"] &gt; 1.0)\n        | (params[\"u1\"] &lt; 0.0)\n        | (params[\"u1\"] &gt; 1.0)\n        | (params[\"u2\"] &lt; 0.0)\n        | (params[\"u2\"] &gt; 1.0),\n        -jnp.inf,\n        log_prob,\n    )\n    return log_prob\n\n\ninitial_guess = OrderedDict(\n    {\n        \"r\": jnp.array(0.05),\n        \"u1\": jnp.array(0.5),\n        \"u2\": jnp.array(0.5),\n    }\n)\n</pre> def objective(params, predictive_gmm, gpmodel):     transit_light_curve = transit_model(time, params)     residuals = light_curve - transit_light_curve      log_prob = predictive_gmm.log_prob(gpmodel.y_transform(residuals))      # enforce (flat) priors     log_prob = jnp.where(         (params[\"r\"] &lt; 0.0)         | (params[\"r\"] &gt; 1.0)         | (params[\"u1\"] &lt; 0.0)         | (params[\"u1\"] &gt; 1.0)         | (params[\"u2\"] &lt; 0.0)         | (params[\"u2\"] &gt; 1.0),         -jnp.inf,         log_prob,     )     return log_prob   initial_guess = OrderedDict(     {         \"r\": jnp.array(0.05),         \"u1\": jnp.array(0.5),         \"u2\": jnp.array(0.5),     } ) <p>We sample the posterior using <code>blackjax</code>'s NUTS sampler for both of our models:</p> In\u00a0[14]: Copied! <pre>mcmc_chains = []\n\nfor model in [rbf_model, learned_model]:\n    predictive_gmm = model.get_mixture_distribution(model.x_transform(time))\n    model_objective = jax.jit(lambda params: objective(params, predictive_gmm, model))\n\n    # parameter adaption and burn-in\n    warmup = blackjax.window_adaptation(\n        blackjax.nuts,\n        model_objective,\n        progress_bar=True,\n    )\n    key, warmup_key, sample_key = jax.random.split(key, 3)\n    (burned_in_state, nuts_parameters), _ = warmup.run(\n        warmup_key,\n        initial_guess,\n        num_steps=1000,\n    )\n\n    # sampling\n    nuts_sampler = blackjax.nuts(model_objective, **nuts_parameters)\n\n    final_state, (history, info) = run_inference_algorithm(\n        rng_key=sample_key,\n        inference_algorithm=nuts_sampler,\n        num_steps=10000,\n        initial_state=burned_in_state,\n        progress_bar=True,\n    )\n\n    # save mcmc chains\n    mcmc_df = pd.DataFrame(\n        jnp.stack(list(history.position.values())).T,\n        columns=list(initial_guess.keys()),\n    ).rename(\n        columns={\n            \"r\": \"Radius Ratio\",\n            \"u1\": \"$u_1$\",\n            \"u2\": \"$u_2$\",\n        }\n    )\n    mcmc_chains.append(mcmc_df)\n\nmcmc_chains[0][\"Kernel\"] = \"RBF Kernel\"\nmcmc_chains[1][\"Kernel\"] = \"Learned Kernel\"\n\nmcmc_df = pd.concat(mcmc_chains).reset_index(drop=True)\n</pre> mcmc_chains = []  for model in [rbf_model, learned_model]:     predictive_gmm = model.get_mixture_distribution(model.x_transform(time))     model_objective = jax.jit(lambda params: objective(params, predictive_gmm, model))      # parameter adaption and burn-in     warmup = blackjax.window_adaptation(         blackjax.nuts,         model_objective,         progress_bar=True,     )     key, warmup_key, sample_key = jax.random.split(key, 3)     (burned_in_state, nuts_parameters), _ = warmup.run(         warmup_key,         initial_guess,         num_steps=1000,     )      # sampling     nuts_sampler = blackjax.nuts(model_objective, **nuts_parameters)      final_state, (history, info) = run_inference_algorithm(         rng_key=sample_key,         inference_algorithm=nuts_sampler,         num_steps=10000,         initial_state=burned_in_state,         progress_bar=True,     )      # save mcmc chains     mcmc_df = pd.DataFrame(         jnp.stack(list(history.position.values())).T,         columns=list(initial_guess.keys()),     ).rename(         columns={             \"r\": \"Radius Ratio\",             \"u1\": \"$u_1$\",             \"u2\": \"$u_2$\",         }     )     mcmc_chains.append(mcmc_df)  mcmc_chains[0][\"Kernel\"] = \"RBF Kernel\" mcmc_chains[1][\"Kernel\"] = \"Learned Kernel\"  mcmc_df = pd.concat(mcmc_chains).reset_index(drop=True) <pre>Running window adaptation\n</pre>        100.00% [1000/1000 00:00&lt;?]             100.00% [10000/10000 00:00&lt;?]      <pre>Running window adaptation\n</pre>        100.00% [1000/1000 00:00&lt;?]             100.00% [10000/10000 00:00&lt;?]      In\u00a0[29]: Copied! <pre>mcmc_df[mcmc_df[\"Kernel\"] == \"RBF Kernel\"].describe(percentiles=[0.16, 0.5, 0.84, 0.95])\n</pre> mcmc_df[mcmc_df[\"Kernel\"] == \"RBF Kernel\"].describe(percentiles=[0.16, 0.5, 0.84, 0.95]) Out[29]: Radius Ratio $u_1$ $u_2$ count 10000.000000 10000.000000 10000.000000 mean 0.110365 0.160286 0.258915 std 0.008942 0.124070 0.187645 min 0.071264 0.000072 0.000001 16% 0.101603 0.037408 0.068077 50% 0.110423 0.133107 0.222156 84% 0.119281 0.287376 0.457564 95% 0.125036 0.397691 0.612866 max 0.140688 0.678735 0.973787 In\u00a0[30]: Copied! <pre>mcmc_df[mcmc_df[\"Kernel\"] == \"Learned Kernel\"].describe(\n    percentiles=[0.16, 0.5, 0.84, 0.95]\n)\n</pre> mcmc_df[mcmc_df[\"Kernel\"] == \"Learned Kernel\"].describe(     percentiles=[0.16, 0.5, 0.84, 0.95] ) Out[30]: Radius Ratio $u_1$ $u_2$ count 10000.000000 10000.000000 1.000000e+04 mean 0.100085 0.180068 2.957173e-01 std 0.003432 0.135331 1.991710e-01 min 0.087077 0.000001 7.516587e-07 16% 0.096657 0.043620 8.558147e-02 50% 0.100168 0.153437 2.695642e-01 84% 0.103411 0.317645 5.079794e-01 95% 0.105633 0.441386 6.593678e-01 max 0.112025 0.798909 9.955275e-01 In\u00a0[26]: Copied! <pre>corner_plot = sns.pairplot(\n    mcmc_df,\n    corner=True,\n    hue=\"Kernel\",\n    hue_order=[\"RBF Kernel\", \"Learned Kernel\"],\n    diag_kind=\"hist\",\n    palette={\"RBF Kernel\": \"C2\", \"Learned Kernel\": \"C0\"},\n    kind=\"kde\",\n    diag_kws={\"element\": \"step\", \"alpha\": 0.4},\n    plot_kws={\"fill\": True, \"alpha\": 0.6, \"levels\": 4},\n    grid_kws={\"despine\": False},\n    height=5,\n)\n\n# add ground truth\nground_truth = [0.1, 0.1, 0.3]\nfor i in range(3):\n    for j in range(3):\n        if corner_plot.axes[i, j]:\n            corner_plot.axes[i, j].axvline(\n                ground_truth[j],\n                color=\"grey\",\n                alpha=0.7,\n                linewidth=3,\n            )\n            if i != j:\n                corner_plot.axes[i, j].axhline(\n                    ground_truth[i], color=\"grey\", alpha=0.7, linewidth=3\n                )\n                corner_plot.axes[i, j].scatter(\n                    ground_truth[j],\n                    ground_truth[i],\n                    color=\"grey\",\n                    alpha=0.7,\n                    s=60,\n                )\n\nsns.move_legend(\n    corner_plot,\n    \"center right\",\n    frameon=True,\n    bbox_to_anchor=(0.825, 0.6),\n    fontsize=30,\n    title=None,\n)\n\n# increase label padding\nfor ax in corner_plot.axes.flatten():\n    if ax:\n        ax.yaxis.labelpad = 20\n\nif save_figures:\n    plt.savefig(figure_directory / \"comparison_corner_plot.pdf\", bbox_inches=\"tight\")\n</pre> corner_plot = sns.pairplot(     mcmc_df,     corner=True,     hue=\"Kernel\",     hue_order=[\"RBF Kernel\", \"Learned Kernel\"],     diag_kind=\"hist\",     palette={\"RBF Kernel\": \"C2\", \"Learned Kernel\": \"C0\"},     kind=\"kde\",     diag_kws={\"element\": \"step\", \"alpha\": 0.4},     plot_kws={\"fill\": True, \"alpha\": 0.6, \"levels\": 4},     grid_kws={\"despine\": False},     height=5, )  # add ground truth ground_truth = [0.1, 0.1, 0.3] for i in range(3):     for j in range(3):         if corner_plot.axes[i, j]:             corner_plot.axes[i, j].axvline(                 ground_truth[j],                 color=\"grey\",                 alpha=0.7,                 linewidth=3,             )             if i != j:                 corner_plot.axes[i, j].axhline(                     ground_truth[i], color=\"grey\", alpha=0.7, linewidth=3                 )                 corner_plot.axes[i, j].scatter(                     ground_truth[j],                     ground_truth[i],                     color=\"grey\",                     alpha=0.7,                     s=60,                 )  sns.move_legend(     corner_plot,     \"center right\",     frameon=True,     bbox_to_anchor=(0.825, 0.6),     fontsize=30,     title=None, )  # increase label padding for ax in corner_plot.axes.flatten():     if ax:         ax.yaxis.labelpad = 20  if save_figures:     plt.savefig(figure_directory / \"comparison_corner_plot.pdf\", bbox_inches=\"tight\") <p>We can see that the posterior distribution for the planet-to-star radius ratio is much more localised and closer to the true value compared to the simple RBF kernel.</p>"},{"location":"comparison_with_simple_kernel/#comparison-with-simple-kernel","title":"Comparison with simple kernel\u00b6","text":""},{"location":"comparison_with_simple_kernel/#notebook-setup","title":"Notebook setup\u00b6","text":""},{"location":"comparison_with_simple_kernel/#create-toy-data","title":"Create toy data\u00b6","text":""},{"location":"comparison_with_simple_kernel/#fitting-the-gallifrey-model","title":"Fitting the <code>gallifrey</code> model\u00b6","text":""},{"location":"comparison_with_simple_kernel/#fitting-the-rbf-model","title":"Fitting the RBF model\u00b6","text":""},{"location":"comparison_with_simple_kernel/#model-comparison","title":"Model comparison\u00b6","text":""},{"location":"comparison_with_simple_kernel/#sampling-the-transit-parameter-distribution","title":"Sampling the transit parameter distribution\u00b6","text":""},{"location":"quickstart/","title":"Getting started","text":"<p><code>gallifrey</code> is a Python package for Bayesian structure learning, inference, and analysis within Gaussian Process models, focused on time series data. Built on JAX and using Sequential Monte Carlo (SMC) techniques, it enables efficient and flexible modeling of complex time series data. This guide will walk you through the basic steps to get started with <code>gallifrey</code>.</p> <p>Before importing gallifrey, we need to configure JAX to utilize all available CPU cores. This is done using the following code snippet:</p> In\u00a0[1]: Copied! <pre>import multiprocessing\nimport os\n\n# enable jax to recognize all CPU cores\nos.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count={}\".format(\n    multiprocessing.cpu_count()\n)\n</pre> import multiprocessing import os  # enable jax to recognize all CPU cores os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count={}\".format(     multiprocessing.cpu_count() ) In\u00a0[2]: Copied! <pre># import necessary packages and set up plotting\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(\n    context=\"poster\",\n    style=\"ticks\",\n    palette=\"rocket\",\n    font_scale=1,\n    rc={\n        \"figure.figsize\": (16, 7),\n        \"axes.grid\": False,\n        \"font.family\": \"serif\",\n        \"text.usetex\": True,\n        \"lines.linewidth\": 3,\n    },\n)\n</pre> # import necessary packages and set up plotting import jax.numpy as jnp import jax.random as jr import matplotlib.pyplot as plt import seaborn as sns  sns.set_theme(     context=\"poster\",     style=\"ticks\",     palette=\"rocket\",     font_scale=1,     rc={         \"figure.figsize\": (16, 7),         \"axes.grid\": False,         \"font.family\": \"serif\",         \"text.usetex\": True,         \"lines.linewidth\": 3,     }, ) <p>Next, import the core components from <code>gallifrey</code>:</p> In\u00a0[3]: Copied! <pre>from gallifrey import GPConfig, GPModel, LinearSchedule\n</pre> from gallifrey import GPConfig, GPModel, LinearSchedule <pre>gallifrey: Setting flag `JAX_ENABLE_X64` to `True`\ngallifrey: Setting flag `OMP_NUM_THREADS` to `1`\n</pre> <p>For this quickstart, we'll generate some mock data for demonstration. We create a simple data set, and reserve some data for training and testing.</p> In\u00a0[4]: Copied! <pre>rng_key = jr.PRNGKey(0)\n</pre> rng_key = jr.PRNGKey(0) In\u00a0[5]: Copied! <pre># Mock data\nkey, data_key = jr.split(rng_key)\nn = 160\nnoise_var = 9.0\nx = jnp.linspace(0, 15, n)\ny = (x + 0.01) * jnp.sin(x * 3.2) + jnp.sqrt(noise_var) * jr.normal(data_key, (n,))\n\n\n# mask values\nxtrain = x[(x &lt; 10)]\nytrain = y[(x &lt; 10)]\n</pre> # Mock data key, data_key = jr.split(rng_key) n = 160 noise_var = 9.0 x = jnp.linspace(0, 15, n) y = (x + 0.01) * jnp.sin(x * 3.2) + jnp.sqrt(noise_var) * jr.normal(data_key, (n,))   # mask values xtrain = x[(x &lt; 10)] ytrain = y[(x &lt; 10)] <p>Now we can initialize the Gaussian Process model. There are a variety of setting for the details of the model, but for this quickstart we can stick with the default config. Please see the tutorials for more details on the different options. We also have to set the number of particles, which together form an ensemble of Gaussian Processes used to make predictions.</p> In\u00a0[6]: Copied! <pre>config = GPConfig()\n\nkey, model_key = jr.split(key)\ngpmodel = GPModel(\n    model_key,\n    x=xtrain,\n    y=ytrain,\n    num_particles=8,\n    config=config,\n)\n</pre> config = GPConfig()  key, model_key = jr.split(key) gpmodel = GPModel(     model_key,     x=xtrain,     y=ytrain,     num_particles=8,     config=config, ) <p>And now we can fit the GP model to the data. We use a Sequential Monte Carlo sampling with a data annealing schedule.</p> In\u00a0[7]: Copied! <pre>key, smc_key = jr.split(key)\nfinal_smc_state, history = gpmodel.fit_smc(\n    smc_key,\n    annealing_schedule=LinearSchedule().generate(len(xtrain), 10),\n    n_mcmc=75,\n    n_hmc=10,\n    verbosity=1,\n)\ngpmodel = gpmodel.update_state(final_smc_state)\n</pre> key, smc_key = jr.split(key) final_smc_state, history = gpmodel.fit_smc(     smc_key,     annealing_schedule=LinearSchedule().generate(len(xtrain), 10),     n_mcmc=75,     n_hmc=10,     verbosity=1, ) gpmodel = gpmodel.update_state(final_smc_state) <pre>Running SMC round [1/10] with [1/106] data points.\nWeights: [0.05392501 0.10727033 0.23023163 0.07581755 0.09684554 0.17955717\n 0.16175602 0.09459675]\nResampled: False (Normalised ESS: 0.83)\nParticle 1 | Accepted: MCMC[49/75]  HMC[490/490]\nParticle 2 | Accepted: MCMC[45/75]  HMC[450/450]\nParticle 3 | Accepted: MCMC[51/75]  HMC[510/510]\nParticle 4 | Accepted: MCMC[51/75]  HMC[510/510]\nParticle 5 | Accepted: MCMC[48/75]  HMC[480/480]\nParticle 6 | Accepted: MCMC[54/75]  HMC[540/540]\nParticle 7 | Accepted: MCMC[57/75]  HMC[570/570]\nParticle 8 | Accepted: MCMC[62/75]  HMC[620/620]\n==================================================\nRunning SMC round [2/10] with [13/106] data points.\nWeights: [8.05914360e-04 5.47702896e-06 3.93273140e-07 1.17978333e-02\n 6.02643432e-01 6.84271626e-05 3.84672447e-01 6.07536689e-06]\nResampled: True (Normalised ESS: 0.24)\nParticle 1 | Accepted: MCMC[48/75]  HMC[479/480]\nParticle 2 | Accepted: MCMC[36/75]  HMC[360/360]\nParticle 3 | Accepted: MCMC[46/75]  HMC[460/460]\nParticle 4 | Accepted: MCMC[46/75]  HMC[460/460]\nParticle 5 | Accepted: MCMC[42/75]  HMC[420/420]\nParticle 6 | Accepted: MCMC[46/75]  HMC[460/460]\nParticle 7 | Accepted: MCMC[43/75]  HMC[430/430]\nParticle 8 | Accepted: MCMC[41/75]  HMC[410/410]\n==================================================\nRunning SMC round [3/10] with [24/106] data points.\nWeights: [0.01799203 0.38161775 0.31482115 0.00537581 0.0032308  0.01912249\n 0.25110413 0.00673584]\nResampled: True (Normalised ESS: 0.41)\nParticle 1 | Accepted: MCMC[43/75]  HMC[430/430]\nParticle 2 | Accepted: MCMC[47/75]  HMC[470/470]\nParticle 3 | Accepted: MCMC[50/75]  HMC[500/500]\nParticle 4 | Accepted: MCMC[40/75]  HMC[400/400]\nParticle 5 | Accepted: MCMC[48/75]  HMC[480/480]\nParticle 6 | Accepted: MCMC[30/75]  HMC[300/300]\nParticle 7 | Accepted: MCMC[47/75]  HMC[470/470]\nParticle 8 | Accepted: MCMC[44/75]  HMC[440/440]\n==================================================\nRunning SMC round [4/10] with [36/106] data points.\nWeights: [0.03426517 0.0556542  0.11974631 0.09799773 0.45193096 0.17182724\n 0.05309929 0.01547909]\nResampled: True (Normalised ESS: 0.47)\nParticle 1 | Accepted: MCMC[40/75]  HMC[400/400]\nParticle 2 | Accepted: MCMC[44/75]  HMC[440/440]\nParticle 3 | Accepted: MCMC[46/75]  HMC[460/460]\nParticle 4 | Accepted: MCMC[39/75]  HMC[390/390]\nParticle 5 | Accepted: MCMC[47/75]  HMC[470/470]\nParticle 6 | Accepted: MCMC[34/75]  HMC[339/340]\nParticle 7 | Accepted: MCMC[36/75]  HMC[360/360]\nParticle 8 | Accepted: MCMC[46/75]  HMC[460/460]\n==================================================\nRunning SMC round [5/10] with [48/106] data points.\nWeights: [0.021441   0.2299715  0.2355654  0.02415679 0.11430621 0.24059889\n 0.09370489 0.04025532]\nResampled: False (Normalised ESS: 0.66)\nParticle 1 | Accepted: MCMC[42/75]  HMC[419/420]\nParticle 2 | Accepted: MCMC[47/75]  HMC[469/470]\nParticle 3 | Accepted: MCMC[38/75]  HMC[380/380]\nParticle 4 | Accepted: MCMC[42/75]  HMC[419/420]\nParticle 5 | Accepted: MCMC[44/75]  HMC[439/440]\nParticle 6 | Accepted: MCMC[46/75]  HMC[460/460]\nParticle 7 | Accepted: MCMC[41/75]  HMC[410/410]\nParticle 8 | Accepted: MCMC[43/75]  HMC[430/430]\n==================================================\nRunning SMC round [6/10] with [59/106] data points.\nWeights: [0.01346345 0.46789399 0.15199219 0.02610874 0.09903758 0.14641147\n 0.06438393 0.03070865]\nResampled: True (Normalised ESS: 0.45)\nParticle 1 | Accepted: MCMC[34/75]  HMC[340/340]\nParticle 2 | Accepted: MCMC[31/75]  HMC[309/310]\nParticle 3 | Accepted: MCMC[28/75]  HMC[279/280]\nParticle 4 | Accepted: MCMC[42/75]  HMC[420/420]\nParticle 5 | Accepted: MCMC[49/75]  HMC[490/490]\nParticle 6 | Accepted: MCMC[41/75]  HMC[410/410]\nParticle 7 | Accepted: MCMC[44/75]  HMC[440/440]\nParticle 8 | Accepted: MCMC[39/75]  HMC[390/390]\n==================================================\nRunning SMC round [7/10] with [71/106] data points.\nWeights: [0.01389594 0.01428358 0.8226441  0.0120111  0.00830618 0.02457485\n 0.09274809 0.01153616]\nResampled: True (Normalised ESS: 0.18)\nParticle 1 | Accepted: MCMC[14/75]  HMC[140/140]\nParticle 2 | Accepted: MCMC[40/75]  HMC[399/400]\nParticle 3 | Accepted: MCMC[19/75]  HMC[189/190]\nParticle 4 | Accepted: MCMC[23/75]  HMC[230/230]\nParticle 5 | Accepted: MCMC[15/75]  HMC[150/150]\nParticle 6 | Accepted: MCMC[20/75]  HMC[199/200]\nParticle 7 | Accepted: MCMC[17/75]  HMC[168/170]\nParticle 8 | Accepted: MCMC[33/75]  HMC[330/330]\n==================================================\nRunning SMC round [8/10] with [83/106] data points.\nWeights: [1.44958343e-01 4.55231902e-04 4.55909880e-01 1.89346520e-02\n 9.01826052e-02 2.08687957e-01 8.00052122e-02 8.66118823e-04]\nResampled: True (Normalised ESS: 0.44)\nParticle 1 | Accepted: MCMC[19/75]  HMC[190/190]\nParticle 2 | Accepted: MCMC[32/75]  HMC[317/320]\nParticle 3 | Accepted: MCMC[17/75]  HMC[169/170]\nParticle 4 | Accepted: MCMC[24/75]  HMC[238/240]\nParticle 5 | Accepted: MCMC[23/75]  HMC[227/230]\nParticle 6 | Accepted: MCMC[18/75]  HMC[176/180]\nParticle 7 | Accepted: MCMC[24/75]  HMC[237/240]\nParticle 8 | Accepted: MCMC[24/75]  HMC[239/240]\n==================================================\nRunning SMC round [9/10] with [94/106] data points.\nWeights: [0.08233663 0.23588815 0.10630179 0.05492449 0.00296773 0.1164186\n 0.25538018 0.14578241]\nResampled: False (Normalised ESS: 0.71)\nParticle 1 | Accepted: MCMC[24/75]  HMC[235/240]\nParticle 2 | Accepted: MCMC[17/75]  HMC[162/170]\nParticle 3 | Accepted: MCMC[17/75]  HMC[163/170]\nParticle 4 | Accepted: MCMC[18/75]  HMC[176/180]\nParticle 5 | Accepted: MCMC[18/75]  HMC[177/180]\nParticle 6 | Accepted: MCMC[19/75]  HMC[177/190]\nParticle 7 | Accepted: MCMC[25/75]  HMC[235/250]\nParticle 8 | Accepted: MCMC[20/75]  HMC[195/200]\n==================================================\nRunning SMC round [10/10] with [106/106] data points.\nWeights: [0.04970241 0.098085   0.03733657 0.01727045 0.00186775 0.09631474\n 0.64934274 0.05008033]\nResampled: False (Normalised ESS: 0.28)\nParticle 1 | Accepted: MCMC[23/75]  HMC[229/230]\nParticle 2 | Accepted: MCMC[26/75]  HMC[249/260]\nParticle 3 | Accepted: MCMC[22/75]  HMC[203/220]\nParticle 4 | Accepted: MCMC[20/75]  HMC[172/200]\nParticle 5 | Accepted: MCMC[15/75]  HMC[146/150]\nParticle 6 | Accepted: MCMC[17/75]  HMC[151/170]\nParticle 7 | Accepted: MCMC[17/75]  HMC[157/170]\nParticle 8 | Accepted: MCMC[16/75]  HMC[148/160]\n==================================================\n</pre> <p>With the fitted model, we can now make predictions. Since the SMC sampler fits an entire ensemble of Gaussian processes, we can use the mixture of predictive distributions for our forecasting.</p> In\u00a0[8]: Copied! <pre>xtest = gpmodel.x_transform(jnp.linspace(0, 20, 500))\ndist = gpmodel.get_mixture_distribution(xtest)\n\npredictive_mean = dist.mean()\npredictive_std = dist.stddev()\n</pre> xtest = gpmodel.x_transform(jnp.linspace(0, 20, 500)) dist = gpmodel.get_mixture_distribution(xtest)  predictive_mean = dist.mean() predictive_std = dist.stddev() In\u00a0[9]: Copied! <pre>plot = sns.lineplot(x=xtest, y=predictive_mean)\nplot.fill_between(\n    xtest,\n    predictive_mean - predictive_std,\n    predictive_mean + predictive_std,\n    alpha=0.3,\n)\n\nsns.scatterplot(\n    x=gpmodel.x_transformed,\n    y=gpmodel.y_transformed,\n    label=\"Training Data\",\n    ax=plot,\n    zorder=3,\n)\nsns.scatterplot(\n    x=gpmodel.x_transform(x),\n    y=gpmodel.y_transform(y),\n    label=\"Test Data\",\n    ax=plot,\n    zorder=2,\n)\n</pre> plot = sns.lineplot(x=xtest, y=predictive_mean) plot.fill_between(     xtest,     predictive_mean - predictive_std,     predictive_mean + predictive_std,     alpha=0.3, )  sns.scatterplot(     x=gpmodel.x_transformed,     y=gpmodel.y_transformed,     label=\"Training Data\",     ax=plot,     zorder=3, ) sns.scatterplot(     x=gpmodel.x_transform(x),     y=gpmodel.y_transform(y),     label=\"Test Data\",     ax=plot,     zorder=2, ) Out[9]: <pre>&lt;Axes: &gt;</pre> <p>Alternatively, we can look that the prediction from each particle individually.</p> In\u00a0[10]: Copied! <pre>dist = gpmodel.get_predictive_distributions(xtest)\n\nmeans, stds = [], []\nfor d in dist:\n    means.append(d.mean())\n    stds.append(d.stddev())\n\nfig, ax = plt.subplots()\nfor i in range(len(means)):\n    sns.lineplot(x=xtest, y=means[i], label=f\"Particle {i+1}\", ax=ax)\n    ax.fill_between(\n        xtest,\n        means[i] - stds[i],\n        means[i] + stds[i],\n        alpha=0.1,\n    )\n\nsns.scatterplot(\n    x=gpmodel.x_transformed,\n    y=gpmodel.y_transformed,\n    ax=ax,\n    zorder=3,\n    color=\"C0\",\n)\nsns.scatterplot(\n    x=gpmodel.x_transform(x),\n    y=gpmodel.y_transform(y),\n    ax=ax,\n    zorder=2,\n    color=\"C1\",\n)\nax.legend(ncols=4)\n</pre> dist = gpmodel.get_predictive_distributions(xtest)  means, stds = [], [] for d in dist:     means.append(d.mean())     stds.append(d.stddev())  fig, ax = plt.subplots() for i in range(len(means)):     sns.lineplot(x=xtest, y=means[i], label=f\"Particle {i+1}\", ax=ax)     ax.fill_between(         xtest,         means[i] - stds[i],         means[i] + stds[i],         alpha=0.1,     )  sns.scatterplot(     x=gpmodel.x_transformed,     y=gpmodel.y_transformed,     ax=ax,     zorder=3,     color=\"C0\", ) sns.scatterplot(     x=gpmodel.x_transform(x),     y=gpmodel.y_transform(y),     ax=ax,     zorder=2,     color=\"C1\", ) ax.legend(ncols=4) Out[10]: <pre>&lt;matplotlib.legend.Legend at 0x7cc6c02549d0&gt;</pre> <p>And we're done! This quickstart guide provides a minimal example to get you started. For more advanced features, customization options, and detailed explanations, please refer to the tutorials and full documentation.</p>"},{"location":"quickstart/#getting-started","title":"Getting started\u00b6","text":""},{"location":"quickstart/#setup-environment","title":"Setup environment\u00b6","text":""},{"location":"quickstart/#generate-mock-data","title":"Generate mock data\u00b6","text":""},{"location":"quickstart/#initialize-the-gp-model","title":"Initialize the GP Model\u00b6","text":""},{"location":"quickstart/#fit-the-gp-model-using-smc","title":"Fit the GP Model using SMC\u00b6","text":""},{"location":"quickstart/#predictions-using-the-model","title":"Predictions using the model\u00b6","text":""},{"location":"quickstart/#conclusion","title":"Conclusion\u00b6","text":""},{"location":"stellar_variability/","title":"Time series interpolation and forecasting","text":"<p>This tutorial introduces the basic features and settings of <code>gallifrey</code> and how it can be used for interpolation and forecasting of a time series. In particular, we will look at a few days of TESS observations for the target TIC 10863087, a variable M dwarf star that was also presented as a case study in the python exoplanet package.</p> <p>First things first, we need to set up the notebook properly.</p> <p>We will run this notebook on a CPU, and since SMC is an inherently parallel algorithm, we want to use all available cores. By default, JAX recognises a CPU only as a single device. To register the available cores as independent devices, we have to set the XLA_FLAGS before importing JAX and any JAX-using libraries.</p> In\u00a0[1]: Copied! <pre># register all available CPU cores for XLA\nimport multiprocessing\nimport os\n\nos.environ[\"XLA_FLAGS\"] = (\n    f\"--xla_force_host_platform_device_count={multiprocessing.cpu_count()}\"\n)\n</pre> # register all available CPU cores for XLA import multiprocessing import os  os.environ[\"XLA_FLAGS\"] = (     f\"--xla_force_host_platform_device_count={multiprocessing.cpu_count()}\" ) In\u00a0[2]: Copied! <pre># now do imports\nimport pathlib\n\nimport astroquery.mast as mast\nimport jax.numpy as jnp\nimport jax.random as jr\nimport lightkurve as lk\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom gallifrey import GPConfig, GPModel, LinearSchedule\nfrom gallifrey.data import Dataset\n</pre> # now do imports import pathlib  import astroquery.mast as mast import jax.numpy as jnp import jax.random as jr import lightkurve as lk import matplotlib.pyplot as plt import pandas as pd import seaborn as sns  from gallifrey import GPConfig, GPModel, LinearSchedule from gallifrey.data import Dataset <pre>gallifrey: Setting flag `JAX_ENABLE_X64` to `True`\ngallifrey: Setting flag `OMP_NUM_THREADS` to `1`\n</pre> <p>By default, gallifrey enables the 64-bit mode in JAX, this makes for more stable matrix inversion in the GP algorithms. We also set <code>OMP_NUM_THREADS</code> to 1.</p> In\u00a0[3]: Copied! <pre># notebook settings\n\n# making the plots pretty\nsns.set_theme(\n    context=\"poster\",\n    style=\"ticks\",\n    palette=\"rocket\",\n    font_scale=1,\n    rc={\n        \"figure.figsize\": (16, 7),\n        \"axes.grid\": False,\n        \"font.family\": \"serif\",\n        \"text.usetex\": True,\n        \"lines.linewidth\": 5,\n        # \"axes.grid\": True,\n    },\n)\n\n# and setting saving defaults\nsave_figures = True\nload_models = True  # load pre-trained models,\n\n# set saving paths\npath = pathlib.Path.cwd().parent\nfigure_directory = path / \"figures/stellar_variability/\"\nif not figure_directory.exists():\n    figure_directory.mkdir(parents=True)\n\n# set a random key for for this notebook\nrng_key = jr.PRNGKey(12)\n</pre> # notebook settings  # making the plots pretty sns.set_theme(     context=\"poster\",     style=\"ticks\",     palette=\"rocket\",     font_scale=1,     rc={         \"figure.figsize\": (16, 7),         \"axes.grid\": False,         \"font.family\": \"serif\",         \"text.usetex\": True,         \"lines.linewidth\": 5,         # \"axes.grid\": True,     }, )  # and setting saving defaults save_figures = True load_models = True  # load pre-trained models,  # set saving paths path = pathlib.Path.cwd().parent figure_directory = path / \"figures/stellar_variability/\" if not figure_directory.exists():     figure_directory.mkdir(parents=True)  # set a random key for for this notebook rng_key = jr.PRNGKey(12) <p>Let's first gather some information about the target. As mentioned before, we are looking at target 10863087 from the TESS Input Catalogue (TIC). We can use <code>astroquery</code> to obtain some basic information on the target first.</p> In\u00a0[4]: Copied! <pre>target = \"TIC 10863087\"\n\n# get basic stellar properties\nstellar_parameters = (\n    mast.Catalogs.query_object(target, radius=0.01, catalog=\"TIC\")\n    .to_pandas()\n    .filter(\n        [\n            \"objType\",\n            \"ra\",\n            \"dec\",\n            \"mass\",\n            \"e_mass\",\n            \"radius\",\n            \"e_radius\",\n            \"Teff\",\n            \"e_Teff\",\n            \"logg\",\n            \"e_logg\",\n            \"Vmag\",\n            \"e_Vmag\",\n        ]\n    )\n)\nstellar_parameters\n</pre> target = \"TIC 10863087\"  # get basic stellar properties stellar_parameters = (     mast.Catalogs.query_object(target, radius=0.01, catalog=\"TIC\")     .to_pandas()     .filter(         [             \"objType\",             \"ra\",             \"dec\",             \"mass\",             \"e_mass\",             \"radius\",             \"e_radius\",             \"Teff\",             \"e_Teff\",             \"logg\",             \"e_logg\",             \"Vmag\",             \"e_Vmag\",         ]     ) ) stellar_parameters Out[4]: objType ra dec mass e_mass Teff e_Teff logg e_logg Vmag e_Vmag 0 STAR 25.938103 -6.0445 0.397525 0.02021 3296.0 157.0 4.82174 0.003616 12.949 0.099 <p>So we can see that the target is a star with a mass around 0.4 solar masses and a V-band magnitude of 12.95. Now to obtains a section of it's observed light curve, we make use of the <code>lightkurve</code> package. We gather ~7 days of observation data, thin the lightcurve to use only every sixth points, and further remove outliers and NaNs. That leaves us with a time series of 814 data points.</p> In\u00a0[5]: Copied! <pre># get lightcurve\nlcf = (\n    lk.search_lightcurve(\"TIC 10863087\", mission=\"TESS\", author=\"SPOC\", limit=1)\n    .download(quality_bitmask=\"hard\")\n    .remove_nans()\n)\n# we thin the lightcurve to speed up the GP fitting, and remove outliers\nlc = lcf[100:5000:6].remove_outliers(sigma=3)\n\n# convert to pandas dataframe\ndf = (\n    lc.to_pandas()\n    .reset_index()\n    .filter([\"time\", \"flux\", \"flux_err\"])\n    .rename(\n        columns={\n            \"time\": \"Time [Days]\",\n            \"flux\": r\"Flux [$\\frac{\\mathrm{e}^{-}}{\\mathrm{s}}$]\",\n            \"flux_err\": r\"Flux Error [$\\frac{\\mathrm{e}^{-}}{\\mathrm{s}}$]]\",\n        }\n    )\n)\nprint(\"\\nNumber of data points:\", len(df))\n</pre> # get lightcurve lcf = (     lk.search_lightcurve(\"TIC 10863087\", mission=\"TESS\", author=\"SPOC\", limit=1)     .download(quality_bitmask=\"hard\")     .remove_nans() ) # we thin the lightcurve to speed up the GP fitting, and remove outliers lc = lcf[100:5000:6].remove_outliers(sigma=3)  # convert to pandas dataframe df = (     lc.to_pandas()     .reset_index()     .filter([\"time\", \"flux\", \"flux_err\"])     .rename(         columns={             \"time\": \"Time [Days]\",             \"flux\": r\"Flux [$\\frac{\\mathrm{e}^{-}}{\\mathrm{s}}$]\",             \"flux_err\": r\"Flux Error [$\\frac{\\mathrm{e}^{-}}{\\mathrm{s}}$]]\",         }     ) ) print(\"\\nNumber of data points:\", len(df)) <pre>Warning: 33% (6445/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=7407).\n</pre> <pre>\nNumber of data points: 814\n</pre> In\u00a0[6]: Copied! <pre>data_plot = sns.scatterplot(\n    df,\n    x=\"Time [Days]\",\n    y=r\"Flux [$\\frac{\\mathrm{e}^{-}}{\\mathrm{s}}$]\",\n    label=\"Data\",\n    s=100,\n)\nif save_figures:\n    plt.savefig(figure_directory / \"data_plot.pdf\", bbox_inches=\"tight\")\n</pre> data_plot = sns.scatterplot(     df,     x=\"Time [Days]\",     y=r\"Flux [$\\frac{\\mathrm{e}^{-}}{\\mathrm{s}}$]\",     label=\"Data\",     s=100, ) if save_figures:     plt.savefig(figure_directory / \"data_plot.pdf\", bbox_inches=\"tight\") <p>The first thing we can look at is how well <code>gallifrey</code> performs at interpolating a missing piece this time series. We can see a clearly see an oscillatory pattern in this time series, so we better hope <code>gallifrey</code> picks up on that. Lets first select a subset of the data to remove, say between day 1388 and 1390. We use the remainder of the data for training and to condition our Gaussian Process on.</p> In\u00a0[7]: Copied! <pre># first, transform the data to jax arrays\nx = jnp.array(df[\"Time [Days]\"])\ny = jnp.array(df[r\"Flux [$\\frac{\\mathrm{e}^{-}}{\\mathrm{s}}$]\"])\n\n# now remove a window of data points\nmask_interpolation = (x &gt; 1388) &amp; (x &lt; 1390)\nxtrain = x[~mask_interpolation]\nytrain = y[~mask_interpolation]\nxall = x\n</pre> # first, transform the data to jax arrays x = jnp.array(df[\"Time [Days]\"]) y = jnp.array(df[r\"Flux [$\\frac{\\mathrm{e}^{-}}{\\mathrm{s}}$]\"])  # now remove a window of data points mask_interpolation = (x &gt; 1388) &amp; (x &lt; 1390) xtrain = x[~mask_interpolation] ytrain = y[~mask_interpolation] xall = x <p>The next thing we need to do is set up the config for our Gaussian Process model. <code>gallifrey</code> is set up in such a way that it should work out of the box for many different forms of data. But for this example, let's make things more explicit. The <code>GPConfig</code> class hosts most of the settings that can be controlled my the user. The major settings are the following:</p> <p>max_depth: The maximum depth of the binary tree that makes up the GP kernel. Higher values allow for more complex models, but at the expense of higher computation time. The default value is 3, which should suffice for smooth and continous time series.</p> <p>atoms : This is a list of the available base GP kernels (such as Linear, Periodic, etc..). Each kernel in this list can act as a leaf in the binary kernel tree that makes up the overall kernel structure. Most commonly used kernels are implemented in the <code>gallifrey.kernels.atoms</code> module. We use the default list of kernels, which includes a</p> <ul> <li>Linear Kernel</li> <li>Periodic Kernel</li> <li>RBF (Radial Basis Function/Squared Exponential) Kernel</li> </ul> <p>operators : Addition or Multiplication of kernel atoms also yield valid kernels. This is the main way to combine kernels to construct the kernel tree. We implement these operations as <code>Operatior</code>s <code>gallifrey.kernels.atoms</code> module. By default, the <code>SumOperator</code> and <code>ProductOperatur</code> included, so that both sums and products of kernels are considered</p> <p>node_probabilities : Finally, when running the <code>fit_mcmc</code> or <code>fit_smc</code> methods in a gallifrey model, kernel trees are constructed by randomly sampling from the available atoms and operators. The node_probabilities array assigns selection probabilities to each of these options.  This array should have a length equal to the sum of the number of kernels and operators. The first part of the array should contain the probabilities for sampling the kernels (in the order of them being listed in the <code>atoms</code> attribute), and the second part should contain the probabilities for sampling the operators (in the order of them being listed in the <code>operators</code> attribute).By default, the probabilities are set to be equal for all kernels, and half that probability for the operators (to encourage kernels with fewer terms).</p> <p>If want to use the default options, you can just invoke the config like this</p> In\u00a0[8]: Copied! <pre>config = GPConfig()\n</pre> config = GPConfig() <p>But if you want to make changes, you can explicitly pass arguments, e.g.</p> In\u00a0[9]: Copied! <pre>from gallifrey.kernels.atoms import (\n    LinearAtom,\n    PeriodicAtom,\n    ProductOperator,\n    RBFAtom,\n    SumOperator,\n)\n\nconfig = GPConfig(\n    max_depth=3,\n    atoms=[LinearAtom(), PeriodicAtom(), RBFAtom()],\n    operators=[SumOperator(), ProductOperator()],\n    node_probabilities=jnp.array([1.0, 1.0, 1.0, 0.5, 0.5]),\n)\n</pre> from gallifrey.kernels.atoms import (     LinearAtom,     PeriodicAtom,     ProductOperator,     RBFAtom,     SumOperator, )  config = GPConfig(     max_depth=3,     atoms=[LinearAtom(), PeriodicAtom(), RBFAtom()],     operators=[SumOperator(), ProductOperator()],     node_probabilities=jnp.array([1.0, 1.0, 1.0, 0.5, 0.5]), ) <p>Now we create a GPModel object and fit it to the data, to do so, we pass a the training data, config and random key to the <code>GPModel</code> constructor. We also decide on the number of particles in the ensemble</p> <p>The parallelisation over different particles is currently implemented using JAX's <code>pmap</code> function, which limits us to a number of particles that is less or equal to the number of available cores.</p> In\u00a0[10]: Copied! <pre># create GP model instance\nkey, model_key = jr.split(rng_key)\ngpmodel = GPModel(\n    model_key,\n    x=xtrain,\n    y=ytrain,\n    num_particles=6,\n    config=config,\n)\n</pre> # create GP model instance key, model_key = jr.split(rng_key) gpmodel = GPModel(     model_key,     x=xtrain,     y=ytrain,     num_particles=6,     config=config, ) <p>The <code>model_key</code> is used to create the initial ensemble of particles, which are drawn from a prior that constructed using the arguments in the <code>GPconfig</code>. We can visualise the first 5 members of the initial ensemble as follows</p> In\u00a0[11]: Copied! <pre>gpmodel.display(num_particles=3)\n</pre> gpmodel.display(num_particles=3) <pre>==================================================\nParticle 1 | Variance: 1.0740917544050912 \n*\n\u2514\u2500\u2500 Linear: [0.73385188]\n\u2514\u2500\u2500 Periodic: [0.38990454 1.08583796 0.94300419]\n\n==================================================\nParticle 2 | Variance: 1.6983593526390177 \nLinear: [0.47201347]\n\n==================================================\nParticle 3 | Variance: 5.163133476632695 \n*\n\u2514\u2500\u2500 Linear: [1.8165231]\n\u2514\u2500\u2500 RBF: [1.75840451 2.69949143]\n\n</pre> <p>We are ready to fit the model to the data. We will use the Sequential Monte Carlo (SMC) sampler to perform the sampling. The SMC sampler performs the sampling via data annealing, meaning that in each round more data is added. To create an annealing schedule, we use <code>gallifrey.schedule.LinearSchedule</code>. We also need to decide how many MCMC steps are performed over the kernel structures, and how many HMC (Hamiltonian Monte Carlo) steps are performed over the kernel parameters.</p> <p>(Note: if you run the example given here, we use 6 particles with 10 SMC rounds. If you instead load the model checkpoint, it will be 64 particles with 20 SMC rounds.)</p> In\u00a0[12]: Copied! <pre># run the model\n\nif load_models is False:\n    key, smc_key = jr.split(key)\n    final_smc_state, history = gpmodel.fit_smc(\n        smc_key,\n        annealing_schedule=LinearSchedule().generate(len(xtrain), 10),\n        n_mcmc=75,\n        n_hmc=10,\n        verbosity=1,\n    )\nelse:\n    final_smc_state = gpmodel.load_state(\n        str(\n            path / \"model_checkpoints/stellar_variability_interpolation/final_state.pkl\"\n        )\n    )\n    history = gpmodel.load_state(\n        str(path / \"model_checkpoints/stellar_variability_interpolation/history.pkl\")\n    )\n\n# update the model with the new state\ngpmodel = gpmodel.update_state(final_smc_state)\n</pre> # run the model  if load_models is False:     key, smc_key = jr.split(key)     final_smc_state, history = gpmodel.fit_smc(         smc_key,         annealing_schedule=LinearSchedule().generate(len(xtrain), 10),         n_mcmc=75,         n_hmc=10,         verbosity=1,     ) else:     final_smc_state = gpmodel.load_state(         str(             path / \"model_checkpoints/stellar_variability_interpolation/final_state.pkl\"         )     )     history = gpmodel.load_state(         str(path / \"model_checkpoints/stellar_variability_interpolation/history.pkl\")     )  # update the model with the new state gpmodel = gpmodel.update_state(final_smc_state) In\u00a0[13]: Copied! <pre>gpmodel.display(num_particles=2)\n</pre> gpmodel.display(num_particles=2) <pre>==================================================\nParticle 1 | Weight: 0.03 | Variance: 0.009701915947732747 \n*\n\u2514\u2500\u2500 *\n    \u2514\u2500\u2500 +\n        \u2514\u2500\u2500 Periodic: [0.66834122 5.34428713 1.71528721]\n        \u2514\u2500\u2500 RBF: [0.96532584 0.84700439]\n    \u2514\u2500\u2500 *\n        \u2514\u2500\u2500 RBF: [0.98132759 0.17626192]\n        \u2514\u2500\u2500 Periodic: [2.69859038 0.30746604 4.17483938]\n\u2514\u2500\u2500 +\n    \u2514\u2500\u2500 *\n        \u2514\u2500\u2500 Linear: [1.22759597]\n        \u2514\u2500\u2500 RBF: [0.82610079 1.06687129]\n    \u2514\u2500\u2500 *\n        \u2514\u2500\u2500 RBF: [0.55356261 0.54715645]\n        \u2514\u2500\u2500 Periodic: [1.25118302 1.7167952  0.13142572]\n\n==================================================\nParticle 2 | Weight: 0.01 | Variance: 0.009775859653600047 \n*\n\u2514\u2500\u2500 RBF: [2.3458651  0.31270641]\n\u2514\u2500\u2500 +\n    \u2514\u2500\u2500 Linear: [0.36739003]\n    \u2514\u2500\u2500 *\n        \u2514\u2500\u2500 Periodic: [0.40767254 0.40475807 1.79032924]\n        \u2514\u2500\u2500 Periodic: [0.72147762 0.41304572 0.13346414]\n\n</pre> <p>We can see that the model has learned a more intricate kernel structure to describe the data, one for each particles. Let's now check the forecasting capabilities of the learned kernels. We use the training data to condition the GP model and create predictions for the remainder of the time series.  Since each particle in our ensemble is modeled as a Gaussian Process (GP), their predictive distributions take the form of Multivariate Gaussians. Consequently, the predictive distribution for the entire ensemble can be determined by simply combining the predictive distributions of individual particles. The result in a Gaussian Mixture Model (GMM), where the weights of each Gaussian component are determined by the importance weights derived from the Sequential Monte Carlo (SMC) sampling. The <code>gallifrey</code> model contains a simple function obtain this mixture distribution as a <code>tensorflow_probability</code> distribution object.</p> <p>A <code>gallifrey</code> model instance internally transforms the input data using a linear transformation. Both x and y get normalised, so that x is constrained to lie between 0 and 1, and y between -0.5 and 0.5. This results in more stable learning across different datasets, but we need to manually transform the data we want to predict on as well. The <code>GPModel</code> instance provides convenient methods to do these transformations.</p> In\u00a0[14]: Copied! <pre># use all the data for prediction, and transform the data to the normalised space\nxall_norm = gpmodel.x_transform(xall)\n\n# now we can get the predictive distribution\npredictive_gmm = gpmodel.get_mixture_distribution(xall_norm)\n</pre> # use all the data for prediction, and transform the data to the normalised space xall_norm = gpmodel.x_transform(xall)  # now we can get the predictive distribution predictive_gmm = gpmodel.get_mixture_distribution(xall_norm) <p>Now, we can calculate the model prediction. The mean and standard deviation are easily be easily obtained in closed form, while percentiles can be calculated from a sample of the GP.</p> In\u00a0[15]: Copied! <pre># calculate the mean and std of the predictive distribution\npredictive_mean = predictive_gmm.mean()\npredictive_stddev = predictive_gmm.stddev()\n\n# calculate the median and 16th and 84th percentiles from a sample\n# key, sample_key = jr.split(key)\n# samples = predictive_gmm.sample(seed=sample_key, sample_shape=(int(1e4),))\n# sample_percentiles = jnp.percentile(samples, q=jnp.array([16, 50, 84]), axis=0)\n</pre> # calculate the mean and std of the predictive distribution predictive_mean = predictive_gmm.mean() predictive_stddev = predictive_gmm.stddev()  # calculate the median and 16th and 84th percentiles from a sample # key, sample_key = jr.split(key) # samples = predictive_gmm.sample(seed=sample_key, sample_shape=(int(1e4),)) # sample_percentiles = jnp.percentile(samples, q=jnp.array([16, 50, 84]), axis=0) <p>Now let's plot the data and prediction. Since the absolute value of the flux is not too important here, we can stick with the normalised fluxes that the <code>GPModel</code> provides. For the training data, the GP model has an attribute <code>y_transformed</code>, which contains the already transformed training data. For the test data, we need to manually transform the data using the <code>y_transform</code> method.</p> In\u00a0[16]: Copied! <pre># prepare the test data the test data\nx_predict = x[mask_interpolation]\ny_predict = y[mask_interpolation]\ny_predict_norm = gpmodel.y_transform(\n    y_predict  # transform the data to the normalised space!\n)\n</pre> # prepare the test data the test data x_predict = x[mask_interpolation] y_predict = y[mask_interpolation] y_predict_norm = gpmodel.y_transform(     y_predict  # transform the data to the normalised space! ) In\u00a0[17]: Copied! <pre># create a convienience function to plot the intervals in different colors\ndef plot_intervals(ax, x, mean, fill_lower, fill_upper, masks, colors):\n    last_index = 0\n    for i in range(len(masks)):\n        mask = masks[i]\n        color = colors[i]\n        # get indices of mask, add last index from previous mask\n        # for smooth plotting\n        indices = jnp.insert(jnp.where(mask)[0], 0, last_index)\n        xm = x[indices]\n\n        # plot means\n        sns.lineplot(x=xm, y=mean[indices], color=color, ax=ax)\n\n        # plot stddevs\n        ax.fill_between(\n            xm,\n            fill_lower[indices],\n            fill_upper[indices],\n            color=color,\n            alpha=0.3,\n        )\n        sns.lineplot(\n            x=xm, y=fill_lower[indices], color=color, ax=ax, linestyle=\"--\", linewidth=2\n        )\n        sns.lineplot(\n            x=xm, y=fill_upper[indices], color=color, ax=ax, linestyle=\"--\", linewidth=2\n        )\n\n        last_index = indices[-1]  # update last index\n    return ax\n</pre> # create a convienience function to plot the intervals in different colors def plot_intervals(ax, x, mean, fill_lower, fill_upper, masks, colors):     last_index = 0     for i in range(len(masks)):         mask = masks[i]         color = colors[i]         # get indices of mask, add last index from previous mask         # for smooth plotting         indices = jnp.insert(jnp.where(mask)[0], 0, last_index)         xm = x[indices]          # plot means         sns.lineplot(x=xm, y=mean[indices], color=color, ax=ax)          # plot stddevs         ax.fill_between(             xm,             fill_lower[indices],             fill_upper[indices],             color=color,             alpha=0.3,         )         sns.lineplot(             x=xm, y=fill_lower[indices], color=color, ax=ax, linestyle=\"--\", linewidth=2         )         sns.lineplot(             x=xm, y=fill_upper[indices], color=color, ax=ax, linestyle=\"--\", linewidth=2         )          last_index = indices[-1]  # update last index     return ax In\u00a0[18]: Copied! <pre># plot the training data and the prediction\nprediction_plot = sns.scatterplot(\n    x=gpmodel.x,\n    y=gpmodel.y_transformed,\n    label=\"Training\",\n    color=\"C0\",\n    alpha=0.5,\n    s=70,\n)\n\nsns.scatterplot(\n    x=x_predict,\n    y=y_predict_norm,\n    label=\"Prediction\",\n    color=\"C5\",\n    ax=prediction_plot,\n    s=70,\n    alpha=0.7,\n)\n\n# plot mean and standard deviation\nmasks = [\n    ~mask_interpolation &amp; (xall &lt; 1390),\n    mask_interpolation,\n    ~mask_interpolation &amp; (xall &gt; 1390),\n]\ncolors = [\"C1\", \"C5\", \"C1\"]\nprediction_plot = plot_intervals(\n    prediction_plot,\n    xall,\n    predictive_mean,\n    predictive_mean - predictive_stddev,\n    predictive_mean + predictive_stddev,\n    masks,\n    colors,\n)\n\n\nprediction_plot.set_xlabel(\"Time [Days]\")\nprediction_plot.set_ylabel(r\"Normalised Flux\")\nprediction_plot.set_ylim(-0.99, 0.59)\nprediction_plot.legend(\n    ncols=2,\n    loc=\"lower right\",\n    fontsize=30,\n    markerscale=1.7,\n)\n\nif save_figures:\n    plt.savefig(figure_directory / \"interpolation_plot.pdf\", bbox_inches=\"tight\")\n</pre> # plot the training data and the prediction prediction_plot = sns.scatterplot(     x=gpmodel.x,     y=gpmodel.y_transformed,     label=\"Training\",     color=\"C0\",     alpha=0.5,     s=70, )  sns.scatterplot(     x=x_predict,     y=y_predict_norm,     label=\"Prediction\",     color=\"C5\",     ax=prediction_plot,     s=70,     alpha=0.7, )  # plot mean and standard deviation masks = [     ~mask_interpolation &amp; (xall &lt; 1390),     mask_interpolation,     ~mask_interpolation &amp; (xall &gt; 1390), ] colors = [\"C1\", \"C5\", \"C1\"] prediction_plot = plot_intervals(     prediction_plot,     xall,     predictive_mean,     predictive_mean - predictive_stddev,     predictive_mean + predictive_stddev,     masks,     colors, )   prediction_plot.set_xlabel(\"Time [Days]\") prediction_plot.set_ylabel(r\"Normalised Flux\") prediction_plot.set_ylim(-0.99, 0.59) prediction_plot.legend(     ncols=2,     loc=\"lower right\",     fontsize=30,     markerscale=1.7, )  if save_figures:     plt.savefig(figure_directory / \"interpolation_plot.pdf\", bbox_inches=\"tight\") <p>That looks pretty good! The model correctly picked up on the (quite obvious) oscillations, and predicts the masked area quite well.</p> <p>Apart from the mixture distribution of the entire ensemble, we can also look at the predictions of the individual partiles, which are simple Gaussian distributions. To get a list of of the predictive distributions for each particle we can use the <code>get_predictive_distributions</code> method of the <code>GPModel</code>.</p> In\u00a0[19]: Copied! <pre># get list of individual predictive distributions for each particle\nparticle_distributions = gpmodel.get_predictive_distributions(xall_norm)\n</pre> # get list of individual predictive distributions for each particle particle_distributions = gpmodel.get_predictive_distributions(xall_norm) <p>Let's plot a random subsample of particles.</p> In\u00a0[20]: Copied! <pre># get distributions of a random subset of particles\nn_particles = 6\nkey, sample_key = jr.split(key)  # split the key\nsample_indices = jr.choice(  # get indices using the key\n    sample_key,\n    jnp.arange(len(particle_distributions)),\n    (n_particles,),\n    replace=False,\n)\nsample_distributions = [particle_distributions[i] for i in sample_indices]\n\n# calculate the mean and std of the predictive distribution\nmeans = [dist.mean() for dist in sample_distributions]\nstddevs = [dist.stddev() for dist in sample_distributions]\nlower = [mean - stddev for mean, stddev in zip(means, stddevs)]\nupper = [mean + stddev for mean, stddev in zip(means, stddevs)]\n\n# make a dataframe for easy plotting\ndf = pd.DataFrame(\n    {\n        \"Time [Days]\": jnp.tile(xall, n_particles),\n        \"Normalised Flux\": jnp.concatenate(means),\n        \"Lower\": jnp.concatenate(lower),\n        \"Upper\": jnp.concatenate(upper),\n        \"Particle\": jnp.repeat(jnp.arange(n_particles), len(xall)),\n    }\n)\n</pre> # get distributions of a random subset of particles n_particles = 6 key, sample_key = jr.split(key)  # split the key sample_indices = jr.choice(  # get indices using the key     sample_key,     jnp.arange(len(particle_distributions)),     (n_particles,),     replace=False, ) sample_distributions = [particle_distributions[i] for i in sample_indices]  # calculate the mean and std of the predictive distribution means = [dist.mean() for dist in sample_distributions] stddevs = [dist.stddev() for dist in sample_distributions] lower = [mean - stddev for mean, stddev in zip(means, stddevs)] upper = [mean + stddev for mean, stddev in zip(means, stddevs)]  # make a dataframe for easy plotting df = pd.DataFrame(     {         \"Time [Days]\": jnp.tile(xall, n_particles),         \"Normalised Flux\": jnp.concatenate(means),         \"Lower\": jnp.concatenate(lower),         \"Upper\": jnp.concatenate(upper),         \"Particle\": jnp.repeat(jnp.arange(n_particles), len(xall)),     } ) In\u00a0[21]: Copied! <pre># create plot grid\nparticle_plot = sns.FacetGrid(\n    df,\n    col=\"Particle\",\n    col_wrap=3,\n    sharey=True,\n    sharex=True,\n    height=5,\n    aspect=1.5,\n)\n\n# plot the predictive mean and standard deviation\nfor i, ax in enumerate(particle_plot.axes):\n    plot_intervals(\n        ax,\n        xall,\n        means[i],\n        lower[i],\n        upper[i],\n        masks,\n        colors,\n    )\n\n# plot training and test data\nparticle_plot.map(\n    sns.scatterplot,\n    x=gpmodel.x,\n    y=gpmodel.y_transformed,\n    color=\"C0\",\n    label=\"Training\",\n    s=60,\n)\nparticle_plot.map(\n    sns.scatterplot,\n    x=x_predict,\n    y=y_predict_norm,\n    color=\"C5\",\n    label=\"Prediction\",\n    s=60,\n)\n\n# set titles\nfor i, ax in enumerate(particle_plot.axes):\n    ax.set_title(f\"Particle {i+1}\")\n\n# add kernel descriptions\nkernel_descriptions = [\n    gpmodel.get_particles()[i]\n    .kernel._tree_viz(include_parameters=False)\n    .replace(\"*\", r\"$\\times$\")\n    .replace(\"   \", \"      \")[:-1]\n    for i in sample_indices\n]\nfor i, ax in enumerate(particle_plot.axes):\n    ax.text(\n        0.65,\n        0.05,\n        kernel_descriptions[i],\n        horizontalalignment=\"left\",\n        verticalalignment=\"bottom\",\n        transform=ax.transAxes,\n        fontsize=14,\n        usetex=False,\n        bbox=dict(facecolor=\"white\", alpha=0.97, edgecolor=\"black\"),\n    )\n\nparticle_plot.figure.supxlabel(\"Time [Days]\", y=0)\nparticle_plot.figure.supylabel(\"Normalised Flux\", x=0)\nif save_figures:\n    particle_plot.savefig(\n        figure_directory / \"particle_interpolation_plot.pdf\", bbox_inches=\"tight\"\n    )\n</pre> # create plot grid particle_plot = sns.FacetGrid(     df,     col=\"Particle\",     col_wrap=3,     sharey=True,     sharex=True,     height=5,     aspect=1.5, )  # plot the predictive mean and standard deviation for i, ax in enumerate(particle_plot.axes):     plot_intervals(         ax,         xall,         means[i],         lower[i],         upper[i],         masks,         colors,     )  # plot training and test data particle_plot.map(     sns.scatterplot,     x=gpmodel.x,     y=gpmodel.y_transformed,     color=\"C0\",     label=\"Training\",     s=60, ) particle_plot.map(     sns.scatterplot,     x=x_predict,     y=y_predict_norm,     color=\"C5\",     label=\"Prediction\",     s=60, )  # set titles for i, ax in enumerate(particle_plot.axes):     ax.set_title(f\"Particle {i+1}\")  # add kernel descriptions kernel_descriptions = [     gpmodel.get_particles()[i]     .kernel._tree_viz(include_parameters=False)     .replace(\"*\", r\"$\\times$\")     .replace(\"   \", \"      \")[:-1]     for i in sample_indices ] for i, ax in enumerate(particle_plot.axes):     ax.text(         0.65,         0.05,         kernel_descriptions[i],         horizontalalignment=\"left\",         verticalalignment=\"bottom\",         transform=ax.transAxes,         fontsize=14,         usetex=False,         bbox=dict(facecolor=\"white\", alpha=0.97, edgecolor=\"black\"),     )  particle_plot.figure.supxlabel(\"Time [Days]\", y=0) particle_plot.figure.supylabel(\"Normalised Flux\", x=0) if save_figures:     particle_plot.savefig(         figure_directory / \"particle_interpolation_plot.pdf\", bbox_inches=\"tight\"     ) <p>All of the particles pick up on the periodic signal in the data, and do a reasonable job at interpolating. But you can see that they have a tendency to be overconfident in their estimate of the amplitude (this can be seen escpecially clearly for Particle 4 and 6). Keeping an ensemble of particles instead leads to a more robust prediction.</p> <p>Another task we can look at is forecasting the time series. For this exercise, we start again with the same initial GP model (we even use the same initial random key!) and this time mask out everything in the time series beyond day 1390.</p> In\u00a0[18]: Copied! <pre># create training data\nmask_extrapolation = x &lt; 1390\nxtrain = x[mask_extrapolation]\nytrain = y[mask_extrapolation]\n\n# initialise the model\ngpmodel = GPModel(\n    model_key,\n    x=xtrain,\n    y=ytrain,\n    num_particles=6,\n    config=config,\n)\n</pre> # create training data mask_extrapolation = x &lt; 1390 xtrain = x[mask_extrapolation] ytrain = y[mask_extrapolation]  # initialise the model gpmodel = GPModel(     model_key,     x=xtrain,     y=ytrain,     num_particles=6,     config=config, ) In\u00a0[19]: Copied! <pre># run the model\n\nif load_models is False:\n    key, smc_key = jr.split(key)\n    final_smc_state, history = gpmodel.fit_smc(\n        smc_key,\n        annealing_schedule=LinearSchedule().generate(len(xtrain), 20),\n        n_mcmc=75,\n        n_hmc=10,\n        verbosity=1,\n    )\nelse:\n    final_smc_state = gpmodel.load_state(\n        str(\n            path / \"model_checkpoints/stellar_variability_extrapolation/final_state.pkl\"\n        )\n    )\n    history = gpmodel.load_state(\n        str(path / \"model_checkpoints/stellar_variability_extrapolation/history.pkl\")\n    )\n\n\n# update the model with the new state\ngpmodel = gpmodel.update_state(final_smc_state)\n</pre> # run the model  if load_models is False:     key, smc_key = jr.split(key)     final_smc_state, history = gpmodel.fit_smc(         smc_key,         annealing_schedule=LinearSchedule().generate(len(xtrain), 20),         n_mcmc=75,         n_hmc=10,         verbosity=1,     ) else:     final_smc_state = gpmodel.load_state(         str(             path / \"model_checkpoints/stellar_variability_extrapolation/final_state.pkl\"         )     )     history = gpmodel.load_state(         str(path / \"model_checkpoints/stellar_variability_extrapolation/history.pkl\")     )   # update the model with the new state gpmodel = gpmodel.update_state(final_smc_state) <p>We calculate and plot the predictive distribution again:</p> In\u00a0[20]: Copied! <pre># now we can get the predictive distribution\nxall_norm = gpmodel.x_transform(xall)\n\npredictive_gmm = gpmodel.get_mixture_distribution(xall_norm)\npredictive_mean = predictive_gmm.mean()\npredictive_stddev = predictive_gmm.stddev()\n</pre> # now we can get the predictive distribution xall_norm = gpmodel.x_transform(xall)  predictive_gmm = gpmodel.get_mixture_distribution(xall_norm) predictive_mean = predictive_gmm.mean() predictive_stddev = predictive_gmm.stddev() In\u00a0[21]: Copied! <pre># plot the training data\nprediction_plot = sns.scatterplot(\n    x=gpmodel.x,\n    y=gpmodel.y_transformed,\n    color=\"C0\",\n    alpha=0.5,\n    s=70,\n)\n\nsns.scatterplot(\n    x=x[~mask_extrapolation],\n    y=gpmodel.y_transform(y[~mask_extrapolation]),\n    color=\"C5\",\n    ax=prediction_plot,\n    s=70,\n    alpha=0.7,\n)\n\n# plot mean and standard deviation\nmasks = [mask_extrapolation, ~mask_extrapolation]\ncolors = [\"C1\", \"C5\"]\nprediction_plot = plot_intervals(\n    prediction_plot,\n    xall,\n    predictive_mean,\n    predictive_mean - predictive_stddev,\n    predictive_mean + predictive_stddev,\n    masks,\n    colors,\n)\n\n\nprediction_plot.set_xlabel(\"Time [Days]\")\nprediction_plot.set_ylabel(r\"Normalised Flux\")\nprediction_plot.set_ylim(-0.99, 0.59)\nif save_figures:\n    plt.savefig(figure_directory / \"extrapolation_plot.pdf\", bbox_inches=\"tight\")\n</pre> # plot the training data prediction_plot = sns.scatterplot(     x=gpmodel.x,     y=gpmodel.y_transformed,     color=\"C0\",     alpha=0.5,     s=70, )  sns.scatterplot(     x=x[~mask_extrapolation],     y=gpmodel.y_transform(y[~mask_extrapolation]),     color=\"C5\",     ax=prediction_plot,     s=70,     alpha=0.7, )  # plot mean and standard deviation masks = [mask_extrapolation, ~mask_extrapolation] colors = [\"C1\", \"C5\"] prediction_plot = plot_intervals(     prediction_plot,     xall,     predictive_mean,     predictive_mean - predictive_stddev,     predictive_mean + predictive_stddev,     masks,     colors, )   prediction_plot.set_xlabel(\"Time [Days]\") prediction_plot.set_ylabel(r\"Normalised Flux\") prediction_plot.set_ylim(-0.99, 0.59) if save_figures:     plt.savefig(figure_directory / \"extrapolation_plot.pdf\", bbox_inches=\"tight\") <p>The model again does a pretty good job! The mean prediction and actual data are pretty close up to one day after the observations. As the forecast horizon increases, the predictions become more uncertain, and the mean prediction deviates more strongly from the true data. Yet they stay within one standard deviation of the model.</p> <p>We also look at a different feature that comes with the SMC sampler, namely how the prediction changes over time as we add more data. For this, the <code>fit_smc</code> method returns not only the final state of the model, but also checkpoints at every completed SMC round. Both the final state, and the history is stored as a <code>GPState</code> instance. The history contains the batched states after every SMC round. We can unbatch these states using the <code>unbatch_states</code> function</p> In\u00a0[22]: Copied! <pre>from gallifrey.model import unbatch_states\n\nhistory_states = unbatch_states(history)\n</pre> from gallifrey.model import unbatch_states  history_states = unbatch_states(history) <p>Now, we can calculate the predictive distribution after every round of SMC. We have to make sure to only condition the GP on data that was seen up this step in the data annealing schedule. To do this, we manually pass a <code>Dataset</code> object to the <code>get_mixture_distribution</code> attribute, making again sure we pass the normalised data. For plotting purposes, let's only take a look at a subset of states:</p> In\u00a0[23]: Copied! <pre>means = []\nlower = []\nupper = []\ndatapoints = []\nmasks = []\n\nfor state in history_states[2::2]:\n    gpmodel_hist = gpmodel.update_state(state)\n\n    included_datapoints = state.num_data_points\n    data_norm = Dataset(\n        x=gpmodel.x_transform(xtrain[:included_datapoints]),\n        y=gpmodel.y_transform(ytrain[:included_datapoints]),\n    )\n\n    predictive_gmm = gpmodel_hist.get_mixture_distribution(xall_norm, data=data_norm)\n    means.append(predictive_gmm.mean())\n    stddevs = predictive_gmm.stddev()\n    lower.append(predictive_gmm.mean() - stddevs)\n    upper.append(predictive_gmm.mean() + stddevs)\n    datapoints.append(jnp.repeat(included_datapoints, len(xall)))\n    masks.append([xall &lt; xall[included_datapoints], xall &gt;= xall[included_datapoints]])\n\n# make a dataframe for easy plotting\ndf = pd.DataFrame(\n    {\n        \"Time [Days]\": jnp.tile(xall, len(means)),\n        \"Normalised Flux\": jnp.concatenate(means),\n        \"Lower\": jnp.concatenate(lower),\n        \"Upper\": jnp.concatenate(upper),\n        \"Datapoints\": jnp.concatenate(datapoints),\n    }\n)\n</pre> means = [] lower = [] upper = [] datapoints = [] masks = []  for state in history_states[2::2]:     gpmodel_hist = gpmodel.update_state(state)      included_datapoints = state.num_data_points     data_norm = Dataset(         x=gpmodel.x_transform(xtrain[:included_datapoints]),         y=gpmodel.y_transform(ytrain[:included_datapoints]),     )      predictive_gmm = gpmodel_hist.get_mixture_distribution(xall_norm, data=data_norm)     means.append(predictive_gmm.mean())     stddevs = predictive_gmm.stddev()     lower.append(predictive_gmm.mean() - stddevs)     upper.append(predictive_gmm.mean() + stddevs)     datapoints.append(jnp.repeat(included_datapoints, len(xall)))     masks.append([xall &lt; xall[included_datapoints], xall &gt;= xall[included_datapoints]])  # make a dataframe for easy plotting df = pd.DataFrame(     {         \"Time [Days]\": jnp.tile(xall, len(means)),         \"Normalised Flux\": jnp.concatenate(means),         \"Lower\": jnp.concatenate(lower),         \"Upper\": jnp.concatenate(upper),         \"Datapoints\": jnp.concatenate(datapoints),     } ) In\u00a0[53]: Copied! <pre># create plot grid\nparticle_plot = sns.FacetGrid(\n    df,\n    col=\"Datapoints\",\n    col_wrap=3,\n    sharey=False,\n    sharex=True,\n    height=5,\n    aspect=1.5,\n)\n\n# plot the predictive mean and standard deviation\nfor i, ax in enumerate(particle_plot.axes):\n    plot_intervals(\n        ax,\n        xall,\n        means[i],\n        lower[i],\n        upper[i],\n        masks[i],\n        [\"C1\", \"C5\"],\n    )\n    sns.scatterplot(\n        x=xall[: datapoints[i][0]],\n        y=gpmodel.y_transform(y)[: datapoints[i][0]],\n        color=\"C0\",\n        s=50,\n        ax=ax,\n    )\n    sns.scatterplot(\n        x=xall[datapoints[i][0] :],\n        y=gpmodel.y_transform(y[datapoints[i][0] :]),\n        color=\"C5\",\n        s=20,\n        ax=ax,\n        edgecolor=None,\n        alpha=0.08,\n        zorder=0,\n    )\n\n\n# set titles\nfor i, ax in enumerate(particle_plot.axes):\n    ax.set_title(f\"Included data points: {datapoints[i][0]}\")\n\n\nparticle_plot.figure.supxlabel(\"Time [Days]\", y=0)\nparticle_plot.figure.supylabel(\"Normalised Flux\", x=0)\nif save_figures:\n    particle_plot.savefig(\n        figure_directory / \"extrapolation_history_plot.pdf\", bbox_inches=\"tight\"\n    )\n</pre> # create plot grid particle_plot = sns.FacetGrid(     df,     col=\"Datapoints\",     col_wrap=3,     sharey=False,     sharex=True,     height=5,     aspect=1.5, )  # plot the predictive mean and standard deviation for i, ax in enumerate(particle_plot.axes):     plot_intervals(         ax,         xall,         means[i],         lower[i],         upper[i],         masks[i],         [\"C1\", \"C5\"],     )     sns.scatterplot(         x=xall[: datapoints[i][0]],         y=gpmodel.y_transform(y)[: datapoints[i][0]],         color=\"C0\",         s=50,         ax=ax,     )     sns.scatterplot(         x=xall[datapoints[i][0] :],         y=gpmodel.y_transform(y[datapoints[i][0] :]),         color=\"C5\",         s=20,         ax=ax,         edgecolor=None,         alpha=0.08,         zorder=0,     )   # set titles for i, ax in enumerate(particle_plot.axes):     ax.set_title(f\"Included data points: {datapoints[i][0]}\")   particle_plot.figure.supxlabel(\"Time [Days]\", y=0) particle_plot.figure.supylabel(\"Normalised Flux\", x=0) if save_figures:     particle_plot.savefig(         figure_directory / \"extrapolation_history_plot.pdf\", bbox_inches=\"tight\"     )"},{"location":"stellar_variability/#time-series-interpolation-and-forecasting","title":"Time series interpolation and forecasting\u00b6","text":""},{"location":"stellar_variability/#notebook-setup","title":"Notebook setup\u00b6","text":""},{"location":"stellar_variability/#data-preparation-and-exploration","title":"Data preparation and exploration\u00b6","text":""},{"location":"stellar_variability/#interpolation","title":"Interpolation\u00b6","text":""},{"location":"stellar_variability/#extrapolationforecasting","title":"Extrapolation/Forecasting\u00b6","text":""},{"location":"transit/","title":"Transit fitting","text":"<p>This tutorial demonstrates how to use <code>gallifrey</code> for background modelling, with the goal of fitting/sampling transit parameters in exoplanet transit light curves. We will see that we can use the predictive distribution that we obstain from the <code>gallifrey</code> SMC sampler as the objective for the transit parameter fit, which means we do not have to fit the Gaussian Process parameter and transit parameter together, which reduces the computation time we need to sample the transit parameter posterior. To stay within the JAX ecosystem, we will use <code>jaxoplanet</code> to model the transit signal, and <code>blackjax</code> for sampling.</p> <p>First, we again set up the computational environment. We set the <code>XLA_FLAGS</code> to register all available cores as independent devices.</p> In\u00a0[1]: Copied! <pre>import multiprocessing\nimport os\n\nos.environ[\"XLA_FLAGS\"] = (\n    f\"--xla_force_host_platform_device_count={multiprocessing.cpu_count()}\"\n)\n</pre> import multiprocessing import os  os.environ[\"XLA_FLAGS\"] = (     f\"--xla_force_host_platform_device_count={multiprocessing.cpu_count()}\" ) In\u00a0[2]: Copied! <pre># import libraries\nimport pathlib\nfrom collections import OrderedDict\n\nimport blackjax\nimport jax\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom blackjax.util import run_inference_algorithm\nfrom jax import numpy as jnp\nfrom jax import random as jr\nfrom jaxoplanet.light_curves import limb_dark_light_curve\nfrom jaxoplanet.orbits import TransitOrbit\n\nfrom gallifrey import GPConfig, GPModel, LinearSchedule\nfrom gallifrey.kernels import Matern32Atom, ProductOperator, RBFAtom, SumOperator\n</pre> # import libraries import pathlib from collections import OrderedDict  import blackjax import jax import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from blackjax.util import run_inference_algorithm from jax import numpy as jnp from jax import random as jr from jaxoplanet.light_curves import limb_dark_light_curve from jaxoplanet.orbits import TransitOrbit  from gallifrey import GPConfig, GPModel, LinearSchedule from gallifrey.kernels import Matern32Atom, ProductOperator, RBFAtom, SumOperator <pre>gallifrey: Setting flag `JAX_ENABLE_X64` to `True`\ngallifrey: Setting flag `OMP_NUM_THREADS` to `1`\n</pre> In\u00a0[3]: Copied! <pre># notebook settings\n\n# making the plots pretty\nsns.set_theme(\n    context=\"poster\",\n    style=\"ticks\",\n    palette=\"rocket\",\n    font_scale=1,\n    rc={\n        \"figure.figsize\": (16, 7),\n        \"axes.grid\": False,\n        \"font.family\": \"serif\",\n        \"text.usetex\": True,\n        \"lines.linewidth\": 5,\n        # \"axes.grid\": True,\n    },\n)\n\n# setting saving defaults\nsave_figures = True\nload_models = True  # load pre-trained models,\n\n# set saving paths\npath = pathlib.Path.cwd().parent\nfigure_directory = path / \"figures/transit_fitting/\"\nif not figure_directory.exists():\n    figure_directory.mkdir(parents=True)\n\n# set a random key for for this notebook\nrng_key = jr.PRNGKey(7)\n</pre> # notebook settings  # making the plots pretty sns.set_theme(     context=\"poster\",     style=\"ticks\",     palette=\"rocket\",     font_scale=1,     rc={         \"figure.figsize\": (16, 7),         \"axes.grid\": False,         \"font.family\": \"serif\",         \"text.usetex\": True,         \"lines.linewidth\": 5,         # \"axes.grid\": True,     }, )  # setting saving defaults save_figures = True load_models = True  # load pre-trained models,  # set saving paths path = pathlib.Path.cwd().parent figure_directory = path / \"figures/transit_fitting/\" if not figure_directory.exists():     figure_directory.mkdir(parents=True)  # set a random key for for this notebook rng_key = jr.PRNGKey(7) <p>We begin by loading the light curve data with a transit signal. We will use a light curve provided by the PLATO Solar-like Light-curve Simulator (arXiv link). The data contains a simulated PLATO observation of 16 Cyg B (KIC 12069449), a main sequence star comparable in mass and radius to the Sun.</p> In\u00a0[4]: Copied! <pre>lightcurve = pd.read_csv(path / \"data/PSLS/0012069449_with_transit.csv\").rename(\n    columns={\"time [d]\": \"Time [Days]\", \"flux [ppm]\": \"Flux [ppm]\"}\n)\n</pre> lightcurve = pd.read_csv(path / \"data/PSLS/0012069449_with_transit.csv\").rename(     columns={\"time [d]\": \"Time [Days]\", \"flux [ppm]\": \"Flux [ppm]\"} ) <p>We select a segment of the light curve containing a single transit and create masks to identify the transit region.</p> In\u00a0[5]: Copied! <pre># select a segment with a single transit\nsingle_transit_mask = (lightcurve[\"Time [Days]\"] &gt; 49.425) &amp; (\n    lightcurve[\"Time [Days]\"] &lt; 49.58\n)\n\n# identify the transit region\ntransit_mask = (lightcurve[\"Time [Days]\"] &gt; 49.49) &amp; (lightcurve[\"Time [Days]\"] &lt; 49.51)\n\nsingle_transit_lc = lightcurve[single_transit_mask]\nsingle_transit_without_transit = lightcurve[single_transit_mask &amp; ~transit_mask]\n\n# convert to jax arrays for training\nxtrain = jnp.array(single_transit_without_transit[\"Time [Days]\"])\nytrain = jnp.array(single_transit_without_transit[\"Flux [ppm]\"])\n</pre> # select a segment with a single transit single_transit_mask = (lightcurve[\"Time [Days]\"] &gt; 49.425) &amp; (     lightcurve[\"Time [Days]\"] &lt; 49.58 )  # identify the transit region transit_mask = (lightcurve[\"Time [Days]\"] &gt; 49.49) &amp; (lightcurve[\"Time [Days]\"] &lt; 49.51)  single_transit_lc = lightcurve[single_transit_mask] single_transit_without_transit = lightcurve[single_transit_mask &amp; ~transit_mask]  # convert to jax arrays for training xtrain = jnp.array(single_transit_without_transit[\"Time [Days]\"]) ytrain = jnp.array(single_transit_without_transit[\"Flux [ppm]\"]) <p>Let's visualize the full light curve, and our selected transit.</p> In\u00a0[6]: Copied! <pre>lightcurve_plot = sns.lineplot(\n    data=lightcurve,\n    x=\"Time [Days]\",\n    y=\"Flux [ppm]\",\n    linewidth=1,\n    color=\"lightgrey\",\n    label=\"Raw\",\n)\n# add smoothed lightcurve\nsns.lineplot(\n    data=lightcurve.rolling(window=100).mean(),\n    x=\"Time [Days]\",\n    y=\"Flux [ppm]\",\n    linewidth=4,\n    label=\"Smoothed\",\n)\n\nlegend = lightcurve_plot.legend(\n    ncols=2,\n    loc=\"lower right\",\n    fontsize=30,\n)\nfor line in legend.get_lines():\n    line.set_linewidth(4)\n\n\nif save_figures:\n    plt.savefig(figure_directory / \"lightcurve_plot.pdf\", bbox_inches=\"tight\")\n</pre> lightcurve_plot = sns.lineplot(     data=lightcurve,     x=\"Time [Days]\",     y=\"Flux [ppm]\",     linewidth=1,     color=\"lightgrey\",     label=\"Raw\", ) # add smoothed lightcurve sns.lineplot(     data=lightcurve.rolling(window=100).mean(),     x=\"Time [Days]\",     y=\"Flux [ppm]\",     linewidth=4,     label=\"Smoothed\", )  legend = lightcurve_plot.legend(     ncols=2,     loc=\"lower right\",     fontsize=30, ) for line in legend.get_lines():     line.set_linewidth(4)   if save_figures:     plt.savefig(figure_directory / \"lightcurve_plot.pdf\", bbox_inches=\"tight\") In\u00a0[\u00a0]: Copied! <pre># visualize the transit\ndata_plot = sns.scatterplot(\n    x=single_transit_lc[\"Time [Days]\"][~transit_mask],\n    y=single_transit_lc[\"Flux [ppm]\"][~transit_mask],\n    label=\"Training Data\",\n    color=\"C0\",\n)\nsns.scatterplot(\n    x=single_transit_lc[\"Time [Days]\"][transit_mask],\n    y=single_transit_lc[\"Flux [ppm]\"][transit_mask],\n    label=\"Masked Transit\",\n    color=\"C5\",\n    ax=data_plot,\n)\ndata_plot.legend(\n    loc=\"lower right\",\n    fontsize=30,\n    markerscale=1.2,\n)\n# if save_figures:\n#     plt.savefig(figure_directory / \"lightcurve_segment_plot.pdf\", bbox_inches=\"tight\")\n</pre> # visualize the transit data_plot = sns.scatterplot(     x=single_transit_lc[\"Time [Days]\"][~transit_mask],     y=single_transit_lc[\"Flux [ppm]\"][~transit_mask],     label=\"Training Data\",     color=\"C0\", ) sns.scatterplot(     x=single_transit_lc[\"Time [Days]\"][transit_mask],     y=single_transit_lc[\"Flux [ppm]\"][transit_mask],     label=\"Masked Transit\",     color=\"C5\",     ax=data_plot, ) data_plot.legend(     loc=\"lower right\",     fontsize=30,     markerscale=1.2, ) # if save_figures: #     plt.savefig(figure_directory / \"lightcurve_segment_plot.pdf\", bbox_inches=\"tight\") <p>To model the stellar background (the underlying trend apart from the transit), we'll use our Gaussian Process. Looking at the data, we can see that the light curve is mostly smooth, and changes gradually. To model this, we will include a RBF kernel as part of atomic kernels for the <code>gallifrey</code> model. To account for more jagged features on smaller time scales, we also add a Mat\u00e9rn 3/2 kernel to the library.</p> In\u00a0[13]: Copied! <pre>config = GPConfig(\n    max_depth=2,\n    atoms=[RBFAtom(), Matern32Atom()],\n    operators=[SumOperator(), ProductOperator()],\n    node_probabilities=jnp.array([1.0, 1.0, 0.5, 0.5]),\n)\n</pre> config = GPConfig(     max_depth=2,     atoms=[RBFAtom(), Matern32Atom()],     operators=[SumOperator(), ProductOperator()],     node_probabilities=jnp.array([1.0, 1.0, 0.5, 0.5]), ) <p>Now we create our GP model instance:</p> In\u00a0[14]: Copied! <pre># create GP model instance\nkey, model_key = jr.split(rng_key)\ngpmodel = GPModel(\n    model_key,\n    x=xtrain,\n    y=ytrain,\n    num_particles=16,\n    config=config,\n)\n</pre> # create GP model instance key, model_key = jr.split(rng_key) gpmodel = GPModel(     model_key,     x=xtrain,     y=ytrain,     num_particles=16,     config=config, ) <p>Next, we'll fit the model using the SMC sampler.</p> In\u00a0[15]: Copied! <pre># run the model\n\nif load_models is False:\n    key, smc_key = jr.split(key)\n    final_smc_state, history = gpmodel.fit_smc(\n        smc_key,\n        annealing_schedule=LinearSchedule().generate(len(xtrain), 20),\n        n_mcmc=75,\n        n_hmc=10,\n        verbosity=1,\n    )\nelse:\n    final_smc_state = gpmodel.load_state(\n        str(path / \"model_checkpoints/psls_deep/final_state.pkl\")\n    )\n    history = gpmodel.load_state(str(path / \"model_checkpoints/psls_deep/history.pkl\"))\n\n# update the model with the new state\ngpmodel = gpmodel.update_state(final_smc_state)\n</pre> # run the model  if load_models is False:     key, smc_key = jr.split(key)     final_smc_state, history = gpmodel.fit_smc(         smc_key,         annealing_schedule=LinearSchedule().generate(len(xtrain), 20),         n_mcmc=75,         n_hmc=10,         verbosity=1,     ) else:     final_smc_state = gpmodel.load_state(         str(path / \"model_checkpoints/psls_deep/final_state.pkl\")     )     history = gpmodel.load_state(str(path / \"model_checkpoints/psls_deep/history.pkl\"))  # update the model with the new state gpmodel = gpmodel.update_state(final_smc_state) <p>Now that our GP model is trained, we can use it to predict the underlying stellar variability across the light curve segment:</p> In\u00a0[16]: Copied! <pre># predict values for all data in the segement (including the transit), and transform the data to the normalised space\nxall_norm = gpmodel.x_transform(single_transit_lc[\"Time [Days]\"].values)\n</pre> # predict values for all data in the segement (including the transit), and transform the data to the normalised space xall_norm = gpmodel.x_transform(single_transit_lc[\"Time [Days]\"].values) <p>We can use the predictive distribution that we obtain from the model as the objective for the transit fitting. In this way, we include the learned characteristics of the time series, including correlations, into the fitting process. Since the computational demand (both in time and memory) for calculating the log probability increases with the number of particles in the sample, we will use only a subset of particles for this. The <code>get_mixture_distribution</code> method contains a convenience parameter to sample a subset of particles from the full ensemble and construct the mixture distribution using this subset. In this case, we have to pass a value for <code>num_particles</code> and a random <code>key</code> for the sampling.</p> In\u00a0[17]: Copied! <pre>key, mixture_key = jr.split(rng_key)\n\n# now we can get the predictive distribution, using subset of the particles\npredictive_gmm = gpmodel.get_mixture_distribution(\n    xall_norm,\n    num_particles=8,\n    key=mixture_key,\n)\n</pre> key, mixture_key = jr.split(rng_key)  # now we can get the predictive distribution, using subset of the particles predictive_gmm = gpmodel.get_mixture_distribution(     xall_norm,     num_particles=8,     key=mixture_key, ) <p>Let's visualize the GP model fit:</p> In\u00a0[18]: Copied! <pre># calculate the mean and std of the predictive distribution\npredictive_mean = predictive_gmm.mean()\npredictive_stddev = predictive_gmm.stddev()\n</pre> # calculate the mean and std of the predictive distribution predictive_mean = predictive_gmm.mean() predictive_stddev = predictive_gmm.stddev() In\u00a0[34]: Copied! <pre># plot the data\nfigure, prediction_plot = plt.subplots()\n\nfor mask, color, label in zip(\n    [~transit_mask, transit_mask],\n    [\"C0\", \"C5\"],\n    [\"Training Data\", \"Masked Transit\"],\n):\n    sns.scatterplot(\n        x=single_transit_lc[\"Time [Days]\"][mask],\n        y=single_transit_lc[\"Flux [ppm]\"][mask],\n        label=label,\n        color=color,\n        zorder=10,\n        alpha=0.8,\n        linewidths=1,\n        s=200,\n        ax=prediction_plot,\n    )\n\n# plot predictive mean and std dev\nfor offset, lw, ls, flag in [\n    (0, 4, None, 1),\n    (predictive_stddev, 1, \"--\", 1),\n    (-predictive_stddev, 1, \"--\", 0),\n    (2 * predictive_stddev, 1, \"--\", 1),\n    (-2 * predictive_stddev, 1, \"--\", 0),\n    (3 * predictive_stddev, 1, \"--\", 1),\n    (-3 * predictive_stddev, 1, \"--\", 0),\n]:\n    sns.lineplot(\n        x=single_transit_lc[\"Time [Days]\"],\n        y=gpmodel.y_transform.unapply(predictive_mean + offset),\n        color=\"C0\",\n        ax=prediction_plot,\n        linewidth=lw,\n        linestyle=ls,\n        alpha=0.9,\n        zorder=100 if ls is None else None,\n    )\n    if flag:\n        prediction_plot.fill_between(\n            single_transit_lc[\"Time [Days]\"],\n            gpmodel.y_transform.unapply(predictive_mean + offset),\n            gpmodel.y_transform.unapply(predictive_mean - offset),\n            alpha=0.15,\n            color=\"C0\",\n        )\n\nprediction_plot.legend(\n    loc=\"lower right\",\n    fontsize=30,\n)\n\nif save_figures:\n    plt.savefig(figure_directory / \"lc_prediction_plot.pdf\", bbox_inches=\"tight\")\n</pre> # plot the data figure, prediction_plot = plt.subplots()  for mask, color, label in zip(     [~transit_mask, transit_mask],     [\"C0\", \"C5\"],     [\"Training Data\", \"Masked Transit\"], ):     sns.scatterplot(         x=single_transit_lc[\"Time [Days]\"][mask],         y=single_transit_lc[\"Flux [ppm]\"][mask],         label=label,         color=color,         zorder=10,         alpha=0.8,         linewidths=1,         s=200,         ax=prediction_plot,     )  # plot predictive mean and std dev for offset, lw, ls, flag in [     (0, 4, None, 1),     (predictive_stddev, 1, \"--\", 1),     (-predictive_stddev, 1, \"--\", 0),     (2 * predictive_stddev, 1, \"--\", 1),     (-2 * predictive_stddev, 1, \"--\", 0),     (3 * predictive_stddev, 1, \"--\", 1),     (-3 * predictive_stddev, 1, \"--\", 0), ]:     sns.lineplot(         x=single_transit_lc[\"Time [Days]\"],         y=gpmodel.y_transform.unapply(predictive_mean + offset),         color=\"C0\",         ax=prediction_plot,         linewidth=lw,         linestyle=ls,         alpha=0.9,         zorder=100 if ls is None else None,     )     if flag:         prediction_plot.fill_between(             single_transit_lc[\"Time [Days]\"],             gpmodel.y_transform.unapply(predictive_mean + offset),             gpmodel.y_transform.unapply(predictive_mean - offset),             alpha=0.15,             color=\"C0\",         )  prediction_plot.legend(     loc=\"lower right\",     fontsize=30, )  if save_figures:     plt.savefig(figure_directory / \"lc_prediction_plot.pdf\", bbox_inches=\"tight\") <p>With our GP model capturing the stellar background, we can now create a transit model using the <code>jaxoplanet</code> package. We will fix the period, impact parameter and limb darkening to their true values, and only fit the transit duration, transit time and transit depth.</p> In\u00a0[21]: Copied! <pre>def transit_model(params, period=10.0, impact_param=0.0, limb_dark=[0.25, 0.75]):\n    \"\"\"\n    Creates a transit light curve model with given parameters.\n\n    Parameters:\n    ----------\n    params : dict\n        Dictionary containing the transit parameters:\n        - duration: Transit duration\n        - t0: Time of transit center\n        - r: Planet-to-star radius ratio\n    period : float, optional\n        Orbital period in days (default: 10.0)\n    impact_param : float, optional\n        Impact parameter (default: 0.0)\n    limb_dark : list, optional\n        Limb darkening coefficients (default: [0.25, 0.75])\n\n    Returns:\n    -------\n    jnp.ndarray: Model light curve\n    \"\"\"\n    orbit = TransitOrbit(\n        period=jnp.array(period),\n        duration=params[\"duration\"],\n        time_transit=params[\"t0\"],\n        impact_param=jnp.array(impact_param),\n        radius_ratio=params[\"r\"],\n    )\n    return limb_dark_light_curve(orbit, jnp.array(limb_dark))(xall_norm)\n</pre> def transit_model(params, period=10.0, impact_param=0.0, limb_dark=[0.25, 0.75]):     \"\"\"     Creates a transit light curve model with given parameters.      Parameters:     ----------     params : dict         Dictionary containing the transit parameters:         - duration: Transit duration         - t0: Time of transit center         - r: Planet-to-star radius ratio     period : float, optional         Orbital period in days (default: 10.0)     impact_param : float, optional         Impact parameter (default: 0.0)     limb_dark : list, optional         Limb darkening coefficients (default: [0.25, 0.75])      Returns:     -------     jnp.ndarray: Model light curve     \"\"\"     orbit = TransitOrbit(         period=jnp.array(period),         duration=params[\"duration\"],         time_transit=params[\"t0\"],         impact_param=jnp.array(impact_param),         radius_ratio=params[\"r\"],     )     return limb_dark_light_curve(orbit, jnp.array(limb_dark))(xall_norm) <p>Now combine our GP background model with the transit model. We make the following assumption: If we can determine the correct transit parameter and substract the transit light curve from the data, the remaining residuals should be well described the the GP background model. Since we took care to fit the GP model with a masked transit, we do not have to worry about having overfit the transit either. We can therefore use the predictive probability distribution the we obtained from the SMC sampling as our objective for the residuals.</p> <p>Note that this is different from most transit fitting recipes in the literature, where one would fit the transit parameter and GP parameters together to avoid overfitting on the GP parameter. For us, the SMC yields an estimate of the GP parameter (and structure) posterior, so we don't have to worry about overfitting the Gaussian Process. This reduces the calculation to simply calculating the transit light curve and evaluating the predictive log probability, which is much less computational demanding that fitting the GP parameters at the same time.</p> In\u00a0[22]: Copied! <pre>@jax.jit\ndef objective(params):\n    \"\"\"\n    Objective function for transit fitting.\n\n    Calculates the log probability of the residuals (data - transit model)\n    under the GP model.\n\n    Parameters:\n    ----------\n    params : dict\n        Transit parameters\n\n    Returns:\n    -------\n    float: Log probability\n    \"\"\"\n    transit_light_curve = transit_model(params) * 1e6  # in ppm\n\n    residuals = single_transit_lc[\"Flux [ppm]\"].values - transit_light_curve\n    log_prob = predictive_gmm.log_prob(gpmodel.y_transform(residuals))\n    return log_prob\n</pre> @jax.jit def objective(params):     \"\"\"     Objective function for transit fitting.      Calculates the log probability of the residuals (data - transit model)     under the GP model.      Parameters:     ----------     params : dict         Transit parameters      Returns:     -------     float: Log probability     \"\"\"     transit_light_curve = transit_model(params) * 1e6  # in ppm      residuals = single_transit_lc[\"Flux [ppm]\"].values - transit_light_curve     log_prob = predictive_gmm.log_prob(gpmodel.y_transform(residuals))     return log_prob <p>For simplicity, we will assume flat priors over the transit parameters. This makes our objective identical to the posterior distribution that we want to sample from.  We'll use MCMC (specifically, the NUTS sampler from <code>blackjax</code>) to sample the transit parameter posterior distribution. We initialize the sampler with reasonable initial guesses. We will make these guesses in the transformed space of the <code>GPModel</code> instance, rather than the original space, since we're passing the transformed time values <code>xall_norm</code> to the <code>transit_model</code>, rather than the original data.</p> In\u00a0[23]: Copied! <pre>initial_guess = OrderedDict(\n    {\n        \"r\": jnp.array(0.06),\n        \"t0\": gpmodel.x_transform(jnp.array(49.502)),\n        \"duration\": jnp.array(0.02)\n        * gpmodel.x_transform.slope,  # scale but don't shift\n    }\n)\n</pre> initial_guess = OrderedDict(     {         \"r\": jnp.array(0.06),         \"t0\": gpmodel.x_transform(jnp.array(49.502)),         \"duration\": jnp.array(0.02)         * gpmodel.x_transform.slope,  # scale but don't shift     } ) <p>Next, we can do the sampling. We will use <code>blackjax</code>'s window adaption feature to set the NUTS hyperparameter and perform the the MCMC burn-in. Then we run the sampling</p> In\u00a0[24]: Copied! <pre># parameter adaption and burn-in\nwarmup = blackjax.window_adaptation(blackjax.nuts, objective, progress_bar=True)\nkey, warmup_key, sample_key = jax.random.split(key, 3)\n(burned_in_state, nuts_parameters), _ = warmup.run(\n    warmup_key, initial_guess, num_steps=1000\n)\n\n# sampling\nnuts_sampler = blackjax.nuts(objective, **nuts_parameters)\n\nfinal_state, (history, info) = run_inference_algorithm(\n    rng_key=sample_key,\n    inference_algorithm=nuts_sampler,\n    num_steps=int(1e4),\n    initial_state=burned_in_state,\n    progress_bar=True,\n)\n</pre> # parameter adaption and burn-in warmup = blackjax.window_adaptation(blackjax.nuts, objective, progress_bar=True) key, warmup_key, sample_key = jax.random.split(key, 3) (burned_in_state, nuts_parameters), _ = warmup.run(     warmup_key, initial_guess, num_steps=1000 )  # sampling nuts_sampler = blackjax.nuts(objective, **nuts_parameters)  final_state, (history, info) = run_inference_algorithm(     rng_key=sample_key,     inference_algorithm=nuts_sampler,     num_steps=int(1e4),     initial_state=burned_in_state,     progress_bar=True, ) <pre>Running window adaptation\n</pre>        100.00% [1000/1000 00:00&lt;?]             100.00% [10000/10000 00:00&lt;?]      <p>We convert the MCMC chain to a dataframe for easier analysis and plotting:</p> In\u00a0[25]: Copied! <pre>mcmc_df = pd.DataFrame(\n    jnp.stack(list(history.position.values())).T, columns=list(initial_guess.keys())\n).rename(\n    columns={\n        \"r\": \"Radius Ratio\",\n        \"t0\": \"Time of Transit Center [Days]\",\n        \"duration\": \"Transit Duration [Days]\",\n    }\n)\n\n# convert the time values back to the original space\nmcmc_df[\"Time of Transit Center [Days]\"] = gpmodel.x_transform.unapply(\n    mcmc_df[\"Time of Transit Center [Days]\"].values\n)\nmcmc_df[\"Transit Duration [Days]\"] = (\n    mcmc_df[\"Transit Duration [Days]\"] / gpmodel.x_transform.slope\n)\n\n\nmcmc_df.head()\n</pre> mcmc_df = pd.DataFrame(     jnp.stack(list(history.position.values())).T, columns=list(initial_guess.keys()) ).rename(     columns={         \"r\": \"Radius Ratio\",         \"t0\": \"Time of Transit Center [Days]\",         \"duration\": \"Transit Duration [Days]\",     } )  # convert the time values back to the original space mcmc_df[\"Time of Transit Center [Days]\"] = gpmodel.x_transform.unapply(     mcmc_df[\"Time of Transit Center [Days]\"].values ) mcmc_df[\"Transit Duration [Days]\"] = (     mcmc_df[\"Transit Duration [Days]\"] / gpmodel.x_transform.slope )   mcmc_df.head() Out[25]: Radius Ratio Time of Transit Center [Days] Transit Duration [Days] 0 0.046613 49.499464 0.017472 1 0.046589 49.499458 0.017591 2 0.046472 49.499429 0.017539 3 0.046740 49.499498 0.017468 4 0.045629 49.499517 0.017095 <p>Let's look at the statistics first:</p> In\u00a0[52]: Copied! <pre>mcmc_df.describe(percentiles=[0.16, 0.5, 0.84])\n</pre> mcmc_df.describe(percentiles=[0.16, 0.5, 0.84]) Out[52]: Radius Ratio Time of Transit Center [Days] Transit Duration [Days] count 10000.000000 10000.000000 10000.000000 mean 0.046484 49.499463 0.017298 std 0.000591 0.000067 0.000146 min 0.044223 49.499213 0.016752 16% 0.045897 49.499396 0.017154 50% 0.046480 49.499464 0.017297 84% 0.047072 49.499528 0.017442 max 0.049249 49.499751 0.017833 <p>And finally make a corner plot of the MCMC samples:</p> In\u00a0[36]: Copied! <pre>corner_plot = sns.pairplot(\n    mcmc_df,\n    corner=True,\n    diag_kind=\"hist\",\n    kind=\"kde\",\n    diag_kws={\"element\": \"step\", \"alpha\": 0.4},\n    plot_kws={\"fill\": True, \"alpha\": 0.6, \"levels\": 4},\n    grid_kws={\"despine\": False},\n    height=5,\n)\n# disable scientific notation\nfor ax in corner_plot.axes.flatten():\n    if ax is not None:\n        ax.ticklabel_format(useOffset=False, style=\"plain\")\n\n# add ground truth\nground_truth = {\n    \"Radius Ratio\": 0.046013,\n    \"Time of Transit Center [Days]\": 49.499602,\n    \"Transit Duration [Days]\": 0.01728,\n}\nfor i in range(3):\n    for j in range(3):\n        if corner_plot.axes[i, j]:\n            corner_plot.axes[i, j].axvline(\n                list(ground_truth.values())[j],\n                color=\"grey\",\n                alpha=0.7,\n                linewidth=3,\n            )\n            if i != j:\n                corner_plot.axes[i, j].axhline(\n                    list(ground_truth.values())[i],\n                    color=\"grey\",\n                    alpha=0.7,\n                    linewidth=3,\n                )\n                corner_plot.axes[i, j].scatter(\n                    list(ground_truth.values())[j],\n                    list(ground_truth.values())[i],\n                    color=\"grey\",\n                    alpha=0.7,\n                    s=60,\n                )\n\n\nif save_figures:\n    plt.savefig(figure_directory / \"corner_plot.pdf\", bbox_inches=\"tight\")\n</pre> corner_plot = sns.pairplot(     mcmc_df,     corner=True,     diag_kind=\"hist\",     kind=\"kde\",     diag_kws={\"element\": \"step\", \"alpha\": 0.4},     plot_kws={\"fill\": True, \"alpha\": 0.6, \"levels\": 4},     grid_kws={\"despine\": False},     height=5, ) # disable scientific notation for ax in corner_plot.axes.flatten():     if ax is not None:         ax.ticklabel_format(useOffset=False, style=\"plain\")  # add ground truth ground_truth = {     \"Radius Ratio\": 0.046013,     \"Time of Transit Center [Days]\": 49.499602,     \"Transit Duration [Days]\": 0.01728, } for i in range(3):     for j in range(3):         if corner_plot.axes[i, j]:             corner_plot.axes[i, j].axvline(                 list(ground_truth.values())[j],                 color=\"grey\",                 alpha=0.7,                 linewidth=3,             )             if i != j:                 corner_plot.axes[i, j].axhline(                     list(ground_truth.values())[i],                     color=\"grey\",                     alpha=0.7,                     linewidth=3,                 )                 corner_plot.axes[i, j].scatter(                     list(ground_truth.values())[j],                     list(ground_truth.values())[i],                     color=\"grey\",                     alpha=0.7,                     s=60,                 )   if save_figures:     plt.savefig(figure_directory / \"corner_plot.pdf\", bbox_inches=\"tight\")"},{"location":"transit/#transit-fitting","title":"Transit fitting\u00b6","text":""},{"location":"transit/#notebook-setup","title":"Notebook setup\u00b6","text":""},{"location":"transit/#data-preparation","title":"Data preparation\u00b6","text":""},{"location":"transit/#gallifrey-gaussian-process-model-setup","title":"<code>gallifrey</code> Gaussian Process model setup\u00b6","text":""},{"location":"transit/#model-training-with-sequential-monte-carlo","title":"Model training with Sequential Monte Carlo\u00b6","text":""},{"location":"transit/#gp-predictive-distribution","title":"GP predictive distribution\u00b6","text":""},{"location":"transit/#transit-model-with-jaxoplanet","title":"Transit model with <code>jaxoplanet</code>\u00b6","text":""},{"location":"transit/#combining-gp-background-and-transit-model","title":"Combining GP background and transit model\u00b6","text":""},{"location":"transit/#sampling-the-posterior-using-blackjax","title":"Sampling the posterior using <code>blackjax</code>\u00b6","text":""},{"location":"transmission_spectrum/","title":"Transmission spectrum of HATS-46 b","text":"<p>In this notebook, we perform end-to-end analysis of a transmission spectrum for HATS-46 b using <code>gallifrey</code> for stellar background modelling. We will show how use <code>gallifrey</code> models for multiple different datasets, how we can use the resulting predictive distributions to sample all 82 transit parameter together in one go! As in previous tutorials, we will use <code>jaxoplanet</code> to model the transit signal, and <code>blackjax</code> for sampling.</p> In\u00a0[1]: Copied! <pre>import multiprocessing\nimport os\n\nos.environ[\"XLA_FLAGS\"] = (\n    f\"--xla_force_host_platform_device_count={multiprocessing.cpu_count()}\"\n)\n</pre> import multiprocessing import os  os.environ[\"XLA_FLAGS\"] = (     f\"--xla_force_host_platform_device_count={multiprocessing.cpu_count()}\" ) In\u00a0[2]: Copied! <pre># import libraries\nimport pathlib\nimport pickle\nfrom collections import OrderedDict\n\nimport blackjax\nimport jax\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom astropy.table import Table\nfrom blackjax.util import run_inference_algorithm\nfrom jax import numpy as jnp\nfrom jax import random as jr\nfrom jaxoplanet.light_curves import limb_dark_light_curve\nfrom jaxoplanet.orbits.keplerian import Body, Central, System\nfrom tqdm import tqdm\n\nfrom gallifrey import GPConfig, GPModel, LinearSchedule\nfrom gallifrey.kernels import (\n    LinearAtom,\n    Matern12Atom,\n    Matern32Atom,\n    Matern52Atom,\n    ProductOperator,\n    RBFAtom,\n    SumOperator,\n)\n</pre> # import libraries import pathlib import pickle from collections import OrderedDict  import blackjax import jax import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from astropy.table import Table from blackjax.util import run_inference_algorithm from jax import numpy as jnp from jax import random as jr from jaxoplanet.light_curves import limb_dark_light_curve from jaxoplanet.orbits.keplerian import Body, Central, System from tqdm import tqdm  from gallifrey import GPConfig, GPModel, LinearSchedule from gallifrey.kernels import (     LinearAtom,     Matern12Atom,     Matern32Atom,     Matern52Atom,     ProductOperator,     RBFAtom,     SumOperator, ) <pre>gallifrey: Setting flag `JAX_ENABLE_X64` to `True`\ngallifrey: Setting flag `OMP_NUM_THREADS` to `1`\n</pre> In\u00a0[3]: Copied! <pre># notebook settings\n\n# making the plots pretty\nsns.set_theme(\n    context=\"poster\",\n    style=\"ticks\",\n    palette=\"rocket\",\n    font_scale=1,\n    rc={\n        \"figure.figsize\": (16, 7),\n        \"axes.grid\": False,\n        \"font.family\": \"serif\",\n        \"text.usetex\": True,\n        \"lines.linewidth\": 5,\n        # \"axes.grid\": True,\n    },\n)\n\n# setting saving defaults\nsave_figures = True\nload_models = True  # load pre-trained models\nsave_models = False  # save trained models, only works if load_models is False\n\n# set saving paths\npath = pathlib.Path.cwd().parent\nfigure_directory = path / \"figures/HATS46b/\"\nif not figure_directory.exists():\n    figure_directory.mkdir(parents=True)\n\n# set a random key for for this notebook\nrng_key = jr.PRNGKey(7)\n</pre> # notebook settings  # making the plots pretty sns.set_theme(     context=\"poster\",     style=\"ticks\",     palette=\"rocket\",     font_scale=1,     rc={         \"figure.figsize\": (16, 7),         \"axes.grid\": False,         \"font.family\": \"serif\",         \"text.usetex\": True,         \"lines.linewidth\": 5,         # \"axes.grid\": True,     }, )  # setting saving defaults save_figures = True load_models = True  # load pre-trained models save_models = False  # save trained models, only works if load_models is False  # set saving paths path = pathlib.Path.cwd().parent figure_directory = path / \"figures/HATS46b/\" if not figure_directory.exists():     figure_directory.mkdir(parents=True)  # set a random key for for this notebook rng_key = jr.PRNGKey(7) <p>For this tutorial, we will use the pre-processed transmission light curves, which can be found here. For more information, please see the original publication by Ahrer+2023 (ADS link). We will fix the period of the planet to the value given in the paper.</p> In\u00a0[4]: Copied! <pre># load data\ndata = (\n    Table.read(path / \"data/HATS46b/HATS_46b.fits\")\n    .to_pandas()\n    .drop(columns=[\"FWB20\", \"e_FWB20\"])  # not used in paper\n    .rename(columns={\"Time\": \"Time [MJD]\"})\n)\n\ntime = jnp.array(data[\"Time [MJD]\"].values)\ntime_zero = time[0]\ntime -= time_zero\n\n# spectroscopic and white light curves, initial entry is white lc\nflux = jnp.array(data.iloc[:, 1::2].values).T\ne_flux = jnp.array(data.iloc[:, 2::2].values).T  # uncertainties\n\n# mask out transit\ntransit_mask = jnp.where(\n    (time &gt; time[6]) &amp; (time &lt; time[41]),\n    True,\n    False,\n)\n\n\n# reference parameter from arXiv:2303.07381, first entry is white lc\n# from arXiv:2303.07381\nreference = pd.read_csv(path / \"data/HATS46b/HATS_46b_reference.csv\").set_index(\n    data.columns[1::2]\n)\nPLANET_PERIOD = 4.7423749  # in days, reference from arXiv:2303.07381\n\nnum_datasets = len(flux)\ndata.head()\n</pre> # load data data = (     Table.read(path / \"data/HATS46b/HATS_46b.fits\")     .to_pandas()     .drop(columns=[\"FWB20\", \"e_FWB20\"])  # not used in paper     .rename(columns={\"Time\": \"Time [MJD]\"}) )  time = jnp.array(data[\"Time [MJD]\"].values) time_zero = time[0] time -= time_zero  # spectroscopic and white light curves, initial entry is white lc flux = jnp.array(data.iloc[:, 1::2].values).T e_flux = jnp.array(data.iloc[:, 2::2].values).T  # uncertainties  # mask out transit transit_mask = jnp.where(     (time &gt; time[6]) &amp; (time &lt; time[41]),     True,     False, )   # reference parameter from arXiv:2303.07381, first entry is white lc # from arXiv:2303.07381 reference = pd.read_csv(path / \"data/HATS46b/HATS_46b_reference.csv\").set_index(     data.columns[1::2] ) PLANET_PERIOD = 4.7423749  # in days, reference from arXiv:2303.07381  num_datasets = len(flux) data.head() Out[4]: Time [MJD] FWL e_FWL FWB01 e_FWB01 FWB02 e_FWB02 FWB03 e_FWB03 FWB04 ... FWB22 e_FWB22 FWB23 e_FWB23 FWB24 e_FWB24 FWB25 e_FWB25 FWB26 e_FWB26 0 57983.131980 1.001562 0.000208 0.998636 0.001034 1.000043 0.000969 1.000023 0.000808 0.999887 ... 1.000623 0.001502 0.997391 0.001697 1.002182 0.001811 1.002301 0.001960 1.001256 0.002255 1 57983.135012 1.000602 0.000207 0.997627 0.001031 0.999776 0.000967 0.999022 0.000806 1.001258 ... 1.000310 0.001491 0.998996 0.001691 1.000000 0.001792 0.999717 0.001968 0.997278 0.002181 2 57983.138045 1.000448 0.000207 0.997814 0.001027 0.998698 0.000964 0.999182 0.000805 1.000254 ... 1.000477 0.001490 1.002012 0.001692 0.999031 0.001790 0.988152 0.001936 1.001592 0.002178 3 57983.141078 1.000778 0.000207 0.999512 0.001024 0.999664 0.000962 0.999969 0.000803 1.000919 ... 0.994235 0.001483 1.005648 0.001720 0.997607 0.001758 0.997192 0.001962 0.999897 0.002174 4 57983.144109 1.001500 0.000207 0.998298 0.001021 1.000616 0.000961 1.002659 0.000804 1.000844 ... 0.997525 0.001487 1.001105 0.001693 0.997700 0.001789 1.000176 0.001971 0.997816 0.002174 <p>5 rows \u00d7 53 columns</p> <p>Let's again plot the light curve and our selected transit, to see if everything went right so far. We will only plot the white light curve for now:</p> In\u00a0[5]: Copied! <pre>white_lightcurve_plot = sns.scatterplot(\n    x=time[~transit_mask] + time_zero,\n    y=flux[0, ~transit_mask],\n    label=\"Training Data\",\n    color=\"C0\",\n)\nwhite_lightcurve_plot = sns.scatterplot(\n    x=time[transit_mask] + time_zero,\n    y=flux[0, transit_mask],\n    label=\"Masked Transit\",\n    color=\"C5\",\n)\nwhite_lightcurve_plot.set_xlabel(\"Time [MJD]\")\nwhite_lightcurve_plot.set_ylabel(\"Normalized Flux\")\n\nwhite_lightcurve_plot.legend(\n    loc=\"lower right\",\n    fontsize=30,\n    markerscale=1.2,\n)\n\n# if save_figures:\n#     plt.savefig(figure_directory / \"white_lightcurve_plot.pdf\", bbox_inches=\"tight\")\n</pre> white_lightcurve_plot = sns.scatterplot(     x=time[~transit_mask] + time_zero,     y=flux[0, ~transit_mask],     label=\"Training Data\",     color=\"C0\", ) white_lightcurve_plot = sns.scatterplot(     x=time[transit_mask] + time_zero,     y=flux[0, transit_mask],     label=\"Masked Transit\",     color=\"C5\", ) white_lightcurve_plot.set_xlabel(\"Time [MJD]\") white_lightcurve_plot.set_ylabel(\"Normalized Flux\")  white_lightcurve_plot.legend(     loc=\"lower right\",     fontsize=30,     markerscale=1.2, )  # if save_figures: #     plt.savefig(figure_directory / \"white_lightcurve_plot.pdf\", bbox_inches=\"tight\") Out[5]: <pre>&lt;matplotlib.legend.Legend at 0x751607b0b460&gt;</pre> <p>We will use a seperate GP model for every light curve, but use the same config for all of them.</p> In\u00a0[6]: Copied! <pre>config = GPConfig(\n    max_depth=2,\n    atoms=[LinearAtom(), RBFAtom(), Matern12Atom(), Matern32Atom(), Matern52Atom()],\n    operators=[SumOperator(), ProductOperator()],\n    node_probabilities=jnp.array([1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5]),\n)\n</pre> config = GPConfig(     max_depth=2,     atoms=[LinearAtom(), RBFAtom(), Matern12Atom(), Matern32Atom(), Matern52Atom()],     operators=[SumOperator(), ProductOperator()],     node_probabilities=jnp.array([1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5]), ) In\u00a0[7]: Copied! <pre># create GP models\ngpmodels = []\nkey = rng_key\nfor i in range(num_datasets):\n    key, model_key = jr.split(key)\n    gpmodel = GPModel(\n        model_key,\n        x=time[~transit_mask],\n        y=flux[i, ~transit_mask],\n        num_particles=8,\n        config=config,\n    )\n    gpmodels.append(gpmodel)\n</pre> # create GP models gpmodels = [] key = rng_key for i in range(num_datasets):     key, model_key = jr.split(key)     gpmodel = GPModel(         model_key,         x=time[~transit_mask],         y=flux[i, ~transit_mask],         num_particles=8,         config=config,     )     gpmodels.append(gpmodel) <p>And now we fit the models using the SMC sampler, one after another:</p> In\u00a0[8]: Copied! <pre># run the models\nmodel_names = data.columns[1::2]\n\nfor i in tqdm(range(num_datasets)):\n    gpmodel = gpmodels[i]\n    if load_models is False:\n        key, smc_key = jr.split(key)\n        final_smc_state, history = gpmodel.fit_smc(\n            smc_key,\n            annealing_schedule=LinearSchedule().generate(len(gpmodel.x), 10),\n            n_mcmc=75,\n            n_hmc=10,\n            verbosity=0,\n        )\n        if save_models:\n            gpmodel.save_state(\n                str(\n                    path / f\"model_checkpoints/HATS46b/final_state_{model_names[i]}.pkl\"\n                ),\n                final_smc_state,\n            )\n            gpmodel.save_state(\n                str(path / f\"model_checkpoints/HATS46b/history_{model_names[i]}.pkl\"),\n                history,\n            )\n    else:\n        final_smc_state = gpmodel.load_state(\n            str(path / f\"model_checkpoints/HATS46b/final_state_{model_names[i]}.pkl\")\n        )\n\n    # update the model with the new state\n    gpmodels[i] = gpmodel.update_state(final_smc_state)\n</pre> # run the models model_names = data.columns[1::2]  for i in tqdm(range(num_datasets)):     gpmodel = gpmodels[i]     if load_models is False:         key, smc_key = jr.split(key)         final_smc_state, history = gpmodel.fit_smc(             smc_key,             annealing_schedule=LinearSchedule().generate(len(gpmodel.x), 10),             n_mcmc=75,             n_hmc=10,             verbosity=0,         )         if save_models:             gpmodel.save_state(                 str(                     path / f\"model_checkpoints/HATS46b/final_state_{model_names[i]}.pkl\"                 ),                 final_smc_state,             )             gpmodel.save_state(                 str(path / f\"model_checkpoints/HATS46b/history_{model_names[i]}.pkl\"),                 history,             )     else:         final_smc_state = gpmodel.load_state(             str(path / f\"model_checkpoints/HATS46b/final_state_{model_names[i]}.pkl\")         )      # update the model with the new state     gpmodels[i] = gpmodel.update_state(final_smc_state) <pre>  0%|          | 0/26 [00:00&lt;?, ?it/s]</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26/26 [00:00&lt;00:00, 40.99it/s]\n</pre> <p>We have the option to either fit the parameter for each light curve independently, or all within the same model. A common approach is to first fit the shared parameter for the white light curve, and then use these fixed parameter to fit the transit depth (and limb darkening coefficients) for the spectroscopic light curves. This is needed to keep the transmission spectrum consistent (otherwise you might fit different stellar radii for the spectrosopic light curves, which will result in shifted transit depths). But this might lead to overconfident results, since we do not take uncertainties in the shared parameters into account.</p> <p>Instead, we will use a different approach and sample all parameters together. In this way, we can keep the shared parameters consistent between the different lightcurves, but sample those parameters at the same time. This is typically not done, since one would fit the GP parameters together with the transit parameter leading to large parameter spaces, and very expensive likelihood evaluations. But since we use the GP predictive distributions from <code>gallifrey</code> as the background model, we don't have to sample the GP parameters, which makes shared transit parameter sampling feasible.</p> In\u00a0[9]: Copied! <pre>def transit_model(params, shared_params, time, period):\n    # define planetary system model with shared and individual parameters\n    central_body = Central(\n        mass=shared_params[\"central_mass\"],\n        radius=shared_params[\"central_radius\"],\n    )\n\n    orbiting_planet = Body(\n        period=period,\n        radius=params[\"radius_ratio\"] * shared_params[\"central_radius\"],\n        inclination=shared_params[\"inclination\"],\n        time_transit=shared_params[\"t0\"],\n    )\n\n    stellar_system = System(central_body, bodies=[orbiting_planet])\n\n    # return the light curve\n    return limb_dark_light_curve(\n        stellar_system, jnp.array([params[\"u1\"], params[\"u2\"]])\n    )(time).squeeze()\n\n\n# vectorize the transit model for all datasets\ntransit_model = jax.vmap(transit_model, in_axes=(0, None, None, None))\n</pre> def transit_model(params, shared_params, time, period):     # define planetary system model with shared and individual parameters     central_body = Central(         mass=shared_params[\"central_mass\"],         radius=shared_params[\"central_radius\"],     )      orbiting_planet = Body(         period=period,         radius=params[\"radius_ratio\"] * shared_params[\"central_radius\"],         inclination=shared_params[\"inclination\"],         time_transit=shared_params[\"t0\"],     )      stellar_system = System(central_body, bodies=[orbiting_planet])      # return the light curve     return limb_dark_light_curve(         stellar_system, jnp.array([params[\"u1\"], params[\"u2\"]])     )(time).squeeze()   # vectorize the transit model for all datasets transit_model = jax.vmap(transit_model, in_axes=(0, None, None, None)) <p>We define a single objective over all light curves together. We also define simple flat priors over parameter, similar to the example in the Comparison with simple kernel tutorial. This time however, we use bijectors to enforce the parameters to lie between the minimum and maximum allowed values. Bijectors are accomplish this by using parameter transformations. For example, if we want to enforce that a parameter is positive, we can use the <code>exp</code> function, since it guarantees the resulting value is larger than zero. Bijectors are used extensively in the internal workings of <code>gallifrey</code> to ensure stable sampling.</p> <p>We use the bijectors that are implemented in<code>tensorflow_probability</code>. Specifically,the <code>Sigmoid</code> bijector can be used to ensure that a value is restricted to a specific interval, and the <code>Softplus</code> bijector guarantees a parameter to be positive. We will use the <code>Sigmoid</code> bijector ro restrict values to lie within plausible ranges given by the original paper.</p> <p>Finally, unlike in the other tutorials, we won't calculate the probabilities for the entire light curve, but only within the transit region itself. That reduces the number of data points by a good fraction, which should speed up the sampling</p> In\u00a0[10]: Copied! <pre>from tensorflow_probability.substrates.jax.bijectors import Sigmoid\n\n# define bijectors for the parameters\nbijectors = {\n    \"radius_ratio\": Sigmoid(low=0.0, high=1.0),\n    \"u1\": Sigmoid(low=0.3, high=1.0),\n    \"u2\": Sigmoid(low=-0.1, high=0.2),\n    \"central_mass\": Sigmoid(low=0.8, high=0.9),\n    \"central_radius\": Sigmoid(low=0.85, high=0.95),\n    \"inclination\": Sigmoid(low=jnp.deg2rad(80.0), high=jnp.deg2rad(90.0)),\n    \"t0\": Sigmoid(low=0.05, high=0.09),\n}\n\n\n# define a function to transform the parameters using the bijectors\ndef transform_params(params, bijectors, direction=\"forward\"):\n    shared_params_dict = params[\"shared\"]\n    individual_params_dict = params[\"individual\"]\n\n    if direction == \"forward\":\n        bijection_funcs = {key: bijectors[key].forward for key in bijectors.keys()}\n    elif direction == \"inverse\":\n        bijection_funcs = {key: bijectors[key].inverse for key in bijectors.keys()}\n\n    shared_params = OrderedDict(\n        {key: bijection_funcs[key](value) for key, value in shared_params_dict.items()}\n    )\n    individual_params = OrderedDict(\n        {\n            key: bijection_funcs[key](value)\n            for key, value in individual_params_dict.items()\n        }\n    )\n    return OrderedDict({\"shared\": shared_params, \"individual\": individual_params})\n</pre> from tensorflow_probability.substrates.jax.bijectors import Sigmoid  # define bijectors for the parameters bijectors = {     \"radius_ratio\": Sigmoid(low=0.0, high=1.0),     \"u1\": Sigmoid(low=0.3, high=1.0),     \"u2\": Sigmoid(low=-0.1, high=0.2),     \"central_mass\": Sigmoid(low=0.8, high=0.9),     \"central_radius\": Sigmoid(low=0.85, high=0.95),     \"inclination\": Sigmoid(low=jnp.deg2rad(80.0), high=jnp.deg2rad(90.0)),     \"t0\": Sigmoid(low=0.05, high=0.09), }   # define a function to transform the parameters using the bijectors def transform_params(params, bijectors, direction=\"forward\"):     shared_params_dict = params[\"shared\"]     individual_params_dict = params[\"individual\"]      if direction == \"forward\":         bijection_funcs = {key: bijectors[key].forward for key in bijectors.keys()}     elif direction == \"inverse\":         bijection_funcs = {key: bijectors[key].inverse for key in bijectors.keys()}      shared_params = OrderedDict(         {key: bijection_funcs[key](value) for key, value in shared_params_dict.items()}     )     individual_params = OrderedDict(         {             key: bijection_funcs[key](value)             for key, value in individual_params_dict.items()         }     )     return OrderedDict({\"shared\": shared_params, \"individual\": individual_params}) In\u00a0[11]: Copied! <pre>time_norm = gpmodels[0].x_transform(time)  # same for all models\ntime_transit = time_norm[transit_mask]\n\n# create predictive distributions for each dataset (only for the transit region)\npredictive_gmms = [\n    gpmodel.get_mixture_distribution(time_transit) for gpmodel in gpmodels\n]\n</pre> time_norm = gpmodels[0].x_transform(time)  # same for all models time_transit = time_norm[transit_mask]  # create predictive distributions for each dataset (only for the transit region) predictive_gmms = [     gpmodel.get_mixture_distribution(time_transit) for gpmodel in gpmodels ] In\u00a0[12]: Copied! <pre>def objective(params, time, flux, predictive_gmms, gpmodels, bijectors):\n\n    # transform the parameters to the original space\n    constrained_params = transform_params(params, bijectors, direction=\"forward\")\n\n    # calculate the transit light curves and residuals\n    transit_light_curves = transit_model(\n        constrained_params[\"individual\"],\n        constrained_params[\"shared\"],\n        time,\n        PLANET_PERIOD,\n    )\n\n    residuals = flux - transit_light_curves\n\n    # calculate the log probability for each light curve\n    log_probs = jnp.array(\n        [\n            predictive_gmm.log_prob(gpmodel.y_transform(residual))\n            for residual, gpmodel, predictive_gmm in zip(\n                residuals, gpmodels, predictive_gmms\n            )\n        ]\n    )\n    # sum the log probabilities to get the total log probability\n    log_prob = jnp.sum(log_probs)\n    return log_prob\n</pre> def objective(params, time, flux, predictive_gmms, gpmodels, bijectors):      # transform the parameters to the original space     constrained_params = transform_params(params, bijectors, direction=\"forward\")      # calculate the transit light curves and residuals     transit_light_curves = transit_model(         constrained_params[\"individual\"],         constrained_params[\"shared\"],         time,         PLANET_PERIOD,     )      residuals = flux - transit_light_curves      # calculate the log probability for each light curve     log_probs = jnp.array(         [             predictive_gmm.log_prob(gpmodel.y_transform(residual))             for residual, gpmodel, predictive_gmm in zip(                 residuals, gpmodels, predictive_gmms             )         ]     )     # sum the log probabilities to get the total log probability     log_prob = jnp.sum(log_probs)     return log_prob <p>We use the just-in-time compilation feature of jax to speed up the objective evaluations, and only pass time and flux values for the transit region to the objective.</p> In\u00a0[13]: Copied! <pre>jitted_objective = jax.jit(\n    lambda params: objective(\n        params,\n        time[transit_mask],\n        flux[:, transit_mask],\n        predictive_gmms,\n        gpmodels,\n        bijectors,\n    )\n)\n</pre> jitted_objective = jax.jit(     lambda params: objective(         params,         time[transit_mask],         flux[:, transit_mask],         predictive_gmms,         gpmodels,         bijectors,     ) ) <p>And define an initial guess for the transit parameters, we'll use values from the original paper.</p> In\u00a0[14]: Copied! <pre>initial_shared_params = OrderedDict(\n    central_mass=jnp.array(0.869),\n    central_radius=jnp.array(0.894),\n    inclination=jnp.deg2rad(86.97),\n    t0=jnp.array(0.07527),\n)\n\ninitial_individual_params = OrderedDict(\n    radius_ratio=jnp.full(num_datasets, 0.1125),\n    u1=jnp.full(num_datasets, 0.547),\n    u2=jnp.full(num_datasets, 0.1171),\n)\n\ninitial_params = OrderedDict(\n    shared=initial_shared_params,\n    individual=initial_individual_params,\n)\n\n# transform parameters to unconstrained space\ninitial_params_uncontrained = transform_params(\n    initial_params, bijectors, direction=\"inverse\"\n)\n</pre> initial_shared_params = OrderedDict(     central_mass=jnp.array(0.869),     central_radius=jnp.array(0.894),     inclination=jnp.deg2rad(86.97),     t0=jnp.array(0.07527), )  initial_individual_params = OrderedDict(     radius_ratio=jnp.full(num_datasets, 0.1125),     u1=jnp.full(num_datasets, 0.547),     u2=jnp.full(num_datasets, 0.1171), )  initial_params = OrderedDict(     shared=initial_shared_params,     individual=initial_individual_params, )  # transform parameters to unconstrained space initial_params_uncontrained = transform_params(     initial_params, bijectors, direction=\"inverse\" ) <p>Finally, we can sample the posterior using the <code>NUTS</code> sampler from <code>blackjax</code>. This will be quite computationally expensive (we're sampling 82 parameters after all!), so we use <code>jax.pmap</code> to parallelise the inference algorithm function, and sample 8 chains at once.</p> In\u00a0[15]: Copied! <pre>if load_models is False:\n    # parameter adaption and burn-in\n    warmup = blackjax.window_adaptation(\n        blackjax.nuts, jitted_objective, progress_bar=True\n    )\n    key, warmup_key, sample_key = jax.random.split(key, 3)\n    (burned_in_state, nuts_parameters), _ = warmup.run(\n        warmup_key,\n        initial_params_uncontrained,\n        num_steps=1000,\n    )\n\n    # sampling\n    nuts_sampler = blackjax.nuts(jitted_objective, **nuts_parameters)\n    inference_algorithm = lambda rng_key: run_inference_algorithm(\n        rng_key=rng_key,\n        inference_algorithm=nuts_sampler,\n        num_steps=int(3e3),\n        initial_state=burned_in_state,\n        progress_bar=True,\n    )\n    final_state, (history, info) = jax.pmap(inference_algorithm)(\n        jr.split(sample_key, 8)\n    )\n\n    # transform the parameters back to the original space\n    mcmc_chains = transform_params(history.position, bijectors, direction=\"forward\")\n\n    if save_models:\n        with open(path / \"data/interim/mcmc_chains/HATS46b.pkl\", \"wb\") as file:\n            pickle.dump(mcmc_chains, file)\nelse:\n    with open(path / \"data/interim/mcmc_chains/HATS46b.pkl\", \"rb\") as file:\n        mcmc_chains = pickle.load(file)\n</pre> if load_models is False:     # parameter adaption and burn-in     warmup = blackjax.window_adaptation(         blackjax.nuts, jitted_objective, progress_bar=True     )     key, warmup_key, sample_key = jax.random.split(key, 3)     (burned_in_state, nuts_parameters), _ = warmup.run(         warmup_key,         initial_params_uncontrained,         num_steps=1000,     )      # sampling     nuts_sampler = blackjax.nuts(jitted_objective, **nuts_parameters)     inference_algorithm = lambda rng_key: run_inference_algorithm(         rng_key=rng_key,         inference_algorithm=nuts_sampler,         num_steps=int(3e3),         initial_state=burned_in_state,         progress_bar=True,     )     final_state, (history, info) = jax.pmap(inference_algorithm)(         jr.split(sample_key, 8)     )      # transform the parameters back to the original space     mcmc_chains = transform_params(history.position, bijectors, direction=\"forward\")      if save_models:         with open(path / \"data/interim/mcmc_chains/HATS46b.pkl\", \"wb\") as file:             pickle.dump(mcmc_chains, file) else:     with open(path / \"data/interim/mcmc_chains/HATS46b.pkl\", \"rb\") as file:         mcmc_chains = pickle.load(file) <p>We can flatten the 8 seperate MCMC chains into single chains:</p> In\u00a0[16]: Copied! <pre>mcmc_chains[\"shared\"] = {\n    key: value.reshape(-1) for key, value in mcmc_chains[\"shared\"].items()\n}\nmcmc_chains[\"individual\"] = {\n    key: value.reshape(-1, num_datasets)\n    for key, value in mcmc_chains[\"individual\"].items()\n}\n</pre> mcmc_chains[\"shared\"] = {     key: value.reshape(-1) for key, value in mcmc_chains[\"shared\"].items() } mcmc_chains[\"individual\"] = {     key: value.reshape(-1, num_datasets)     for key, value in mcmc_chains[\"individual\"].items() } <p>With the chains we obtained, we can finally plot the transmission spectrum. Let's also plot the reference spectrum from the original paper for comparison.</p> In\u00a0[17]: Copied! <pre># get the chains for the radius ratio, and flatten them for the different walkers\nradius_ratio = mcmc_chains[\"individual\"][\"radius_ratio\"]\n\n# calculate the percentiles\nr_median = jnp.median(radius_ratio, axis=0)\nr_low = jnp.percentile(radius_ratio, 16, axis=0)\nr_high = jnp.percentile(radius_ratio, 84, axis=0)\n</pre> # get the chains for the radius ratio, and flatten them for the different walkers radius_ratio = mcmc_chains[\"individual\"][\"radius_ratio\"]  # calculate the percentiles r_median = jnp.median(radius_ratio, axis=0) r_low = jnp.percentile(radius_ratio, 16, axis=0) r_high = jnp.percentile(radius_ratio, 84, axis=0) In\u00a0[29]: Copied! <pre>fig, ax = plt.subplots()\n\n# we skip the first entry as it is the white light curve\nfor data, errors, label, color in zip(\n    [reference[\"Rp\"][1:], r_median[1:]],\n    [\n        reference[[\"e_Rp_lower\", \"e_Rp_upper\"]][1:],\n        jnp.stack([r_median[1:] - r_low[1:], r_high[1:] - r_median[1:]]).T,\n    ],\n    [\"Ahrer+2023 (Linear in FWHM)\", \"GP with Learned Kernel\"],\n    [\"C2\", \"C0\"],\n):\n\n    ax.errorbar(\n        reference[\"wavelength\"].iloc[1:],\n        data,\n        yerr=errors.T,\n        fmt=\".\",\n        capsize=6,\n        capthick=2,\n        elinewidth=3,\n        color=color,\n        label=label,\n    )\n    sns.scatterplot(\n        x=reference[\"wavelength\"].iloc[1:],\n        y=data,\n        color=color,\n        zorder=10,\n        ax=ax,\n    )\n\nax.set_xlabel(r\"Wavelength [$\\mathrm{\\mathring{A}}$]\")\nax.set_ylabel(\"Radius Ratio\", labelpad=10)\n\nax.set_ylim(0.1035, 0.13)\nax.legend(loc=\"upper left\", fontsize=30)\n\nif save_figures:\n    plt.savefig(figure_directory / \"transmission_spectrum.pdf\", bbox_inches=\"tight\")\n</pre> fig, ax = plt.subplots()  # we skip the first entry as it is the white light curve for data, errors, label, color in zip(     [reference[\"Rp\"][1:], r_median[1:]],     [         reference[[\"e_Rp_lower\", \"e_Rp_upper\"]][1:],         jnp.stack([r_median[1:] - r_low[1:], r_high[1:] - r_median[1:]]).T,     ],     [\"Ahrer+2023 (Linear in FWHM)\", \"GP with Learned Kernel\"],     [\"C2\", \"C0\"], ):      ax.errorbar(         reference[\"wavelength\"].iloc[1:],         data,         yerr=errors.T,         fmt=\".\",         capsize=6,         capthick=2,         elinewidth=3,         color=color,         label=label,     )     sns.scatterplot(         x=reference[\"wavelength\"].iloc[1:],         y=data,         color=color,         zorder=10,         ax=ax,     )  ax.set_xlabel(r\"Wavelength [$\\mathrm{\\mathring{A}}$]\") ax.set_ylabel(\"Radius Ratio\", labelpad=10)  ax.set_ylim(0.1035, 0.13) ax.legend(loc=\"upper left\", fontsize=30)  if save_figures:     plt.savefig(figure_directory / \"transmission_spectrum.pdf\", bbox_inches=\"tight\") <p>We can also plot the light curves themselves, to see if our model yields a good representation of the data. To stay fully in the Bayesian framework, we should sample light curves from the joint distribution of the Gaussian Processes and transit light curves. For this notebook, we'll make our lives a little easier and simply use the Gaussian process means, and median transit parameters. (In particular that means we only take the Gaussian Process variance into account here, not the transit parameter uncertainties.)</p> In\u00a0[18]: Copied! <pre># get GP means and stddevs from predictive distributions (this time for the whole light curve)\ngp_gmms = [gpmodel.get_mixture_distribution(time_norm) for gpmodel in gpmodels]\ngp_means = jnp.array([gp_gmm.mean() for gp_gmm in gp_gmms])\ngp_stds = jnp.array([gp_gmm.stddev() for gp_gmm in gp_gmms])\n\n# get mean transit parameters\nshared_transit_parameter_means = {\n    key: jnp.median(value) for key, value in mcmc_chains[\"shared\"].items()\n}\nindividual_transit_parameter_means = {\n    key: jnp.median(value, axis=0) for key, value in mcmc_chains[\"individual\"].items()\n}\ntransit_parameter_means = {\n    \"shared\": shared_transit_parameter_means,\n    \"individual\": individual_transit_parameter_means,\n}\n\n# calculate mean transit light curves\ntransit_light_curves = transit_model(\n    transit_parameter_means[\"individual\"],\n    transit_parameter_means[\"shared\"],\n    time,\n    PLANET_PERIOD,\n)\n</pre> # get GP means and stddevs from predictive distributions (this time for the whole light curve) gp_gmms = [gpmodel.get_mixture_distribution(time_norm) for gpmodel in gpmodels] gp_means = jnp.array([gp_gmm.mean() for gp_gmm in gp_gmms]) gp_stds = jnp.array([gp_gmm.stddev() for gp_gmm in gp_gmms])  # get mean transit parameters shared_transit_parameter_means = {     key: jnp.median(value) for key, value in mcmc_chains[\"shared\"].items() } individual_transit_parameter_means = {     key: jnp.median(value, axis=0) for key, value in mcmc_chains[\"individual\"].items() } transit_parameter_means = {     \"shared\": shared_transit_parameter_means,     \"individual\": individual_transit_parameter_means, }  # calculate mean transit light curves transit_light_curves = transit_model(     transit_parameter_means[\"individual\"],     transit_parameter_means[\"shared\"],     time,     PLANET_PERIOD, ) <p>Now we can calculate the full model light curves (GP + transit) and the residuals. We'll put them all in a dataframe for easier plotting.</p> In\u00a0[19]: Copied! <pre>light_curve_models = []\n\nfor i in range(1, num_datasets):  # skip the white light curve\n    gp_mean = gpmodels[i].y_transform.unapply(gp_means[i])\n    light_curve = gp_mean + transit_light_curves[i]\n\n    df_data = {\"Time [MJD]\": time + time_zero, \"Flux\": light_curve}\n\n    # calculate 1, 2, and 3 sigma\n    for sig in [1, 2, 3]:\n        gp_lower = gpmodels[i].y_transform.unapply(gp_means[i] - sig * gp_stds[i])\n        gp_upper = gpmodels[i].y_transform.unapply(gp_means[i] + sig * gp_stds[i])\n\n        df_data[f\"lower_{sig}sig\"] = gp_lower + transit_light_curves[i]\n        df_data[f\"upper_{sig}sig\"] = gp_upper + transit_light_curves[i]\n\n    df_data[r\"$\\lambda$\"] = (\n        str(reference[\"wavelength\"].iloc[i]) + r\"$\\mathrm{\\mathring{A}}$\"\n    )\n\n    df = pd.DataFrame(df_data)\n\n    light_curve_models.append(df)\n\nlight_curve_models = pd.concat(light_curve_models)\n</pre> light_curve_models = []  for i in range(1, num_datasets):  # skip the white light curve     gp_mean = gpmodels[i].y_transform.unapply(gp_means[i])     light_curve = gp_mean + transit_light_curves[i]      df_data = {\"Time [MJD]\": time + time_zero, \"Flux\": light_curve}      # calculate 1, 2, and 3 sigma     for sig in [1, 2, 3]:         gp_lower = gpmodels[i].y_transform.unapply(gp_means[i] - sig * gp_stds[i])         gp_upper = gpmodels[i].y_transform.unapply(gp_means[i] + sig * gp_stds[i])          df_data[f\"lower_{sig}sig\"] = gp_lower + transit_light_curves[i]         df_data[f\"upper_{sig}sig\"] = gp_upper + transit_light_curves[i]      df_data[r\"$\\lambda$\"] = (         str(reference[\"wavelength\"].iloc[i]) + r\"$\\mathrm{\\mathring{A}}$\"     )      df = pd.DataFrame(df_data)      light_curve_models.append(df)  light_curve_models = pd.concat(light_curve_models) <p>And finally we plot all the different light curves:</p> In\u00a0[20]: Copied! <pre>light_curve_plot = sns.FacetGrid(\n    light_curve_models,\n    col=r\"$\\lambda$\",\n    col_wrap=3,\n    sharex=True,\n    sharey=False,\n    height=3.5,\n    aspect=3,\n)\n\nlight_curve_plot.map(sns.lineplot, \"Time [MJD]\", \"Flux\", color=\"C0\")\n\nfor sig in [1, 2, 3]:\n    light_curve_plot.map(\n        plt.fill_between,\n        \"Time [MJD]\",\n        f\"lower_{sig}sig\",\n        f\"upper_{sig}sig\",\n        alpha=0.15,\n        color=\"C0\",\n    )\n\nfor i, ax in enumerate(light_curve_plot.axes):\n    ax.errorbar(\n        time + time_zero,\n        flux[i + 1],\n        yerr=e_flux[i + 1],\n        fmt=\".\",\n        capsize=6,\n        capthick=2,\n        elinewidth=3,\n        color=\"C1\",\n    )\n\nlight_curve_plot.set_axis_labels(\"\", \"\")\n\nlight_curve_plot.figure.supxlabel(\"Time [MJD]\", fontsize=40)\nlight_curve_plot.figure.supylabel(\"Normalized Flux\", fontsize=40)\nlight_curve_plot.figure.tight_layout()\n\nif save_figures:\n    plt.savefig(figure_directory / \"light_curve_models.pdf\", bbox_inches=\"tight\")\n</pre> light_curve_plot = sns.FacetGrid(     light_curve_models,     col=r\"$\\lambda$\",     col_wrap=3,     sharex=True,     sharey=False,     height=3.5,     aspect=3, )  light_curve_plot.map(sns.lineplot, \"Time [MJD]\", \"Flux\", color=\"C0\")  for sig in [1, 2, 3]:     light_curve_plot.map(         plt.fill_between,         \"Time [MJD]\",         f\"lower_{sig}sig\",         f\"upper_{sig}sig\",         alpha=0.15,         color=\"C0\",     )  for i, ax in enumerate(light_curve_plot.axes):     ax.errorbar(         time + time_zero,         flux[i + 1],         yerr=e_flux[i + 1],         fmt=\".\",         capsize=6,         capthick=2,         elinewidth=3,         color=\"C1\",     )  light_curve_plot.set_axis_labels(\"\", \"\")  light_curve_plot.figure.supxlabel(\"Time [MJD]\", fontsize=40) light_curve_plot.figure.supylabel(\"Normalized Flux\", fontsize=40) light_curve_plot.figure.tight_layout()  if save_figures:     plt.savefig(figure_directory / \"light_curve_models.pdf\", bbox_inches=\"tight\")"},{"location":"transmission_spectrum/#transmission-spectrum-of-hats-46-b","title":"Transmission spectrum of HATS-46 b\u00b6","text":""},{"location":"transmission_spectrum/#notebook-setup","title":"Notebook setup\u00b6","text":""},{"location":"transmission_spectrum/#data-preparation","title":"Data preparation\u00b6","text":""},{"location":"transmission_spectrum/#gallifrey-gp-models","title":"<code>gallifrey</code> GP models\u00b6","text":""},{"location":"transmission_spectrum/#creating-the-transit-model","title":"Creating the transit model\u00b6","text":""},{"location":"transmission_spectrum/#sampling-the-transit-parameter-posterior","title":"Sampling the transit parameter posterior\u00b6","text":""},{"location":"transmission_spectrum/#plotting-the-transmission-spectrum","title":"Plotting the transmission spectrum\u00b6","text":""},{"location":"transmission_spectrum/#plotting-the-light-curves","title":"Plotting the light curves\u00b6","text":""},{"location":"autoapi/summary/","title":"Summary","text":"<ul> <li>gallifrey<ul> <li>data</li> <li>gpconfig</li> <li>inference<ul> <li>hmc</li> <li>parameter_rejuvenation</li> <li>rejuvenation</li> <li>smc</li> <li>state</li> <li>transforms</li> </ul> </li> <li>kernels<ul> <li>atoms</li> <li>library</li> <li>prior</li> <li>tree</li> </ul> </li> <li>model</li> <li>moves<ul> <li>acceptance_probability</li> <li>detach_attach</li> <li>noise_proposal</li> <li>particle_move</li> <li>subtree_replace</li> </ul> </li> <li>parameter</li> <li>particles</li> <li>schedule</li> <li>utils<ul> <li>probability_calculations</li> <li>tree_helper</li> <li>typing</li> </ul> </li> </ul> </li> </ul>"},{"location":"autoapi/gallifrey/","title":"gallifrey","text":""},{"location":"autoapi/gallifrey/#gallifrey.GPConfig","title":"<code>GPConfig</code>","text":"<p>Config for the GP model.</p> <p>Attributes:</p> Name Type Description <code>max_depth</code> <code>int</code> <p>Maximum depth of the kernel tree. By default, 3.</p> <code>atoms</code> <code>list[AbstractAtom]</code> <p>List of atomic kernels to consider in the kernel tree. By default, the following kernels are included: - Linear - Periodic - RBF</p> <code>operators</code> <code>list[AbstractOperator]</code> <p>List of kernel operators to consider in the kernel tree. The operators are used to combine the atomic kernels (i.e., functions) in the kernel structure. By default, the following operators are included: - SumOperator - ProductOperator</p> <code>node_probabilities</code> <code>Float[ndarray, ' D']</code> <p>Probabilities for sampling the kernels and operators. This array should have a length equal to the sum of the number of kernels and operators. The first part of the array should contain the probabilities for sampling the kernels (in the order of them being listed in the <code>atoms</code> attribute), and the second part should contain the probabilities for sampling the operators (in the order of them being listed in the <code>operators</code> attribute). By default, the probabilities are set to be equal for all kernels, and half that probability for the operators (to encourage kernels with fewer terms)</p> <code>prior_transforms</code> <code>dict[str, Transformation]</code> <p>Dictionary containing the bijectors for transforming the kernel parameters to the prior distribution. Originally, the parameters are sampled from a standard normal distribution, and these bijectors transform the samples to the desired prior distribution. The keys are inherited from GPJax, and describe the domain of the kernel parameters. The values are the bijectors that transform the standard normal samples to the desired prior distribution. The bijection transformation should be implemented via TensorFlow Probability bijectors. By default, the following bijectors are included: - \"real\": Log-normal transform (normal -&gt; log-normal) - \"positive\": Log-normal transform (normal -&gt; log-normal) - \"sigmoid\": Logit-normal transform (normal -&gt; logit-normal) NOTE: The \"none\" key is reserved for internal use and should not be used.</p> <code>hmc_config</code> <code>dict[str, float]</code> <p>Configuration for the HMC sampler. The dictionary should contain the following keys: - \"step_size\": The step size of the HMC sampler. By default, 0.02. - \"inv_mass_matrix_scaling\": The scaling factor for the inverse mass matrix.     By default, 1.0. - \"num_integration_steps\": The number of integration steps for the HMC sampler.     By default, 10.</p> <code>noise_prior</code> <code>Distribution</code> <p>An instance of tensorflow_probability.substrates.jax.distributions.Distribution that samples the noise variance from the prior distribution. Must have methods \"sample\" and \"log_prob\". The default is an InverseGamma(1,1) distribution. NOTE: The noise variance prior is currently fixed to the InverseGamma(1,1) distribution, and cannot be changed. This is because the InverseGamma(1,1) distribution is needed to caluculate the Monte Carlo acceptance probabilities analytically. Other distributions are not currently supported.</p> <code>mean_function</code> <code>AbstractMeanFunction</code> <p>The mean function of the GP model. By default, a zero mean function is used. The constant is explicitly set to 0.0, (and is not a ParticleParameter from the gallifrey.parameter class, so it is not trainable). NOTE: Non-zero mean functions are not currently implemented, do not change this attribute.</p> Source code in <code>gallifrey/gpconfig.py</code> <pre><code>@struct.dataclass\nclass GPConfig:\n    \"\"\"\n    Config for the GP model.\n\n    Attributes\n    ----------\n\n    max_depth : int\n        Maximum depth of the kernel tree. By default, 3.\n\n    atoms : list[AbstractAtom]\n        List of atomic kernels to consider in the kernel tree.\n        By default, the following kernels are included:\n        - Linear\n        - Periodic\n        - RBF\n\n    operators : list[AbstractOperator]\n        List of kernel operators to consider in the kernel tree.\n        The operators are used to combine the atomic kernels (i.e., functions)\n        in the kernel structure. By default, the following operators are included:\n        - SumOperator\n        - ProductOperator\n\n    node_probabilities :  Float[jnp.ndarray, \" D\"]\n        Probabilities for sampling the kernels and operators. This array\n        should have a length equal to the sum of the number of kernels\n        and operators. The first part of the array should contain the\n        probabilities for sampling the kernels (in the order of them being listed\n        in the `atoms` attribute), and the second part should contain the\n        probabilities for sampling the operators (in the order of them being listed\n        in the `operators` attribute).\n        By default, the probabilities are set to be equal for all kernels, and\n        half that probability for the operators (to encourage kernels with\n        fewer terms)\n\n    prior_transforms : dict[str, Transformation]\n        Dictionary containing the bijectors for transforming the kernel parameters\n        to the prior distribution. Originally, the parameters are sampled from a\n        standard normal distribution, and these bijectors transform the samples to\n        the desired prior distribution. The keys are inherited from GPJax, and describe\n        the domain of the kernel parameters. The values are the bijectors that\n        transform the standard normal samples to the desired prior distribution. The\n        bijection transformation should be implemented via TensorFlow Probability\n        bijectors.\n        By default, the following bijectors are included:\n        - \"real\": Log-normal transform (normal -&gt; log-normal)\n        - \"positive\": Log-normal transform (normal -&gt; log-normal)\n        - \"sigmoid\": Logit-normal transform (normal -&gt; logit-normal)\n        NOTE: The \"none\" key is reserved for internal use and should not be used.\n\n    hmc_config : dict[str, float]\n        Configuration for the HMC sampler. The dictionary should contain the following\n        keys:\n        - \"step_size\": The step size of the HMC sampler. By default, 0.02.\n        - \"inv_mass_matrix_scaling\": The scaling factor for the inverse mass matrix.\n            By default, 1.0.\n        - \"num_integration_steps\": The number of integration steps for the HMC sampler.\n            By default, 10.\n\n    noise_prior : Distribution\n        An instance of tensorflow_probability.substrates.jax.distributions.Distribution\n        that samples the noise variance from the prior distribution. Must have methods\n        \"sample\" and \"log_prob\". The default is an InverseGamma(1,1) distribution.\n        NOTE: The noise variance prior is currently fixed to the InverseGamma(1,1)\n        distribution, and cannot be changed. This is because the InverseGamma(1,1)\n        distribution is needed to caluculate the Monte Carlo acceptance probabilities\n        analytically. Other distributions are not currently supported.\n\n    mean_function : AbstractMeanFunction\n        The mean function of the GP model. By default, a zero mean function is used.\n        The constant is explicitly set to 0.0, (and is not a ParticleParameter from\n        the gallifrey.parameter class, so it is not trainable).\n        NOTE: Non-zero mean functions are not currently implemented, do not change\n        this attribute.\n\n    \"\"\"\n\n    max_depth: int = 3\n\n    atoms: list[AbstractAtom] = field(\n        default_factory=lambda: [\n            # ConstantAtom(),\n            LinearAtom(),\n            # LinearWithShiftAtom(),\n            PeriodicAtom(),\n            # Matern12Atom(),\n            # Matern32Atom(),\n            # Matern52Atom(),\n            RBFAtom(),\n            # PoweredExponentialAtom(),\n            # RationalQuadraticAtom(),\n            # WhiteAtom(),\n        ]\n    )\n\n    operators: list[AbstractOperator] = field(\n        default_factory=lambda: [\n            SumOperator(),\n            ProductOperator(),\n        ]\n    )\n\n    node_probabilities: Float[jnp.ndarray, \" D\"] = field(\n        default_factory=lambda: jnp.array(\n            [\n                # 1.0,  # Constant\n                1.0,  # Linear\n                # 1.0, # LinearWithShift\n                1.0,  # Periodic\n                # 1.0, # Matern12\n                # 1.0, # Matern32\n                # 1.0, # Matern52\n                1.0,  # RBF\n                # 1.0, # PoweredExponential\n                # 1.0, # RationalQuadratic\n                # 1.0, # White\n                0.5,  # SumOperator\n                0.5,  # ProductOperator\n            ]\n        )\n    )\n\n    prior_transforms: dict[str, tfb.Bijector] = field(\n        default_factory=lambda: dict(\n            {\n                # this is the transformation y = exp(mu + sigma * z),\n                # with mu = 0 and sigma = 1,\n                # if z ~ normal(0, 1) then y ~ log-normal(mu, sigma)\n                \"real\": tfb.Chain(\n                    [\n                        tfb.Exp(),\n                        tfb.Shift(jnp.array(0.0)),\n                        tfb.Scale(jnp.array(1.0)),\n                    ]\n                ),\n                \"positive\": tfb.Chain(\n                    [\n                        tfb.Exp(),\n                        tfb.Shift(jnp.array(0.0)),\n                        tfb.Scale(jnp.array(1.0)),\n                    ]\n                ),\n                # this is the transformation y = 1/(1 + exp(-(mu + sigma * z))),\n                # with mu = 0 and sigma = 1,\n                # if z ~ normal(0, 1) then y ~ logit-normal(mu, sigma)\n                \"sigmoid\": tfb.Chain(\n                    [\n                        tfb.Sigmoid(\n                            low=jnp.array(0.0),\n                            high=jnp.array(\n                                0.95  # slightly below 1 to avoid numerical issues\n                            ),\n                        ),\n                        tfb.Shift(jnp.array(0.0)),\n                        tfb.Scale(jnp.array(1.0)),\n                    ]\n                ),\n            }\n        )\n    )\n\n    hmc_config: dict[str, float] = field(\n        default_factory=lambda: {\n            \"step_size\": 0.02,\n            \"inv_mass_matrix_scaling\": 1.0,\n            \"num_integration_steps\": 10,\n        }\n    )\n\n    @property\n    def noise_prior(self) -&gt; Distribution:\n        \"\"\"\n        The distribution of the noise variance prior,\n        specifically an InverseGamma(1,1) distribution.\n\n        Returns\n        -------\n        Distribution\n            The noise variance prior distribution.\n        \"\"\"\n        return InverseGamma(jnp.array(1.0), jnp.array(1.0))\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.GPConfig.noise_prior","title":"<code>noise_prior</code>  <code>property</code>","text":"<p>The distribution of the noise variance prior, specifically an InverseGamma(1,1) distribution.</p> <p>Returns:</p> Type Description <code>Distribution</code> <p>The noise variance prior distribution.</p>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel","title":"<code>GPModel</code>","text":"<p>The GP model class.</p> <p>Attributes:</p> Name Type Description <code>num_particles</code> <code>ScalarInt</code> <p>Number of particles in the model.</p> <code>config</code> <code>GPConfig</code> <p>The config instance for the GP model (see gallifrey.config.GPConfig for details).</p> <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior object, containing various useful methods and attributes for the kernel sampling. Attributes set through config. (See gallifrey.kernels.prior.KernelPrior for details.)</p> <code>noise_prior</code> <code>Distribution</code> <p>The prior distribution for the noise variance, inherited from the config. (See gallifrey.config.GPConfig for details.)</p> <code>kernel_library</code> <code>KernelLibrary</code> <p>The kernel library, containing the atomic kernels, operators, and prior transforms. (See gallifrey.kernels.library.KernelLibrary for details.)</p> <code>x</code> <code>Float[ndarray, ' D']</code> <p>The input x data.</p> <code>y</code> <code>Float[ndarray, ' D']</code> <p>The input y data.</p> <code>noise_variance</code> <p>The initial noise variance if fixed, otherwise None.</p> <code>fix_noise</code> <code>bool</code> <p>Flag indicating whether the noise variance is fixed or learned. This is set to True if the noise variance is provided as an input parameter.</p> <code>x_transform</code> <code>Optional[Callable]</code> <p>A (optional) transformation applied to the input x data.</p> <code>y_transform</code> <code>Optional[Callable]</code> <p>A (optional) transformation applied to the input y data.</p> <code>x_transformed</code> <code>Float[ndarray, ' D']</code> <p>The transformed input x data.</p> <code>y_transformed</code> <code>Float[ndarray, ' D']</code> <p>The transformed noise variance if fixed, otherwise None.</p> <code>noise_variance_transformed</code> <code>Float[ndarray, ' D']</code> <p>The transformed noise variance, if noise variance is provided.</p> <code>dataset</code> <code>Dataset</code> <p>A dataset instance containing the transformed x and y data, processed for the sampler.</p> <code>particle_graphdef</code> <code>GraphDef</code> <p>Graph definition for the Particle object, shared across all particles. This can be used to reconstruct the Particle object from a particle state.</p> <code>state</code> <code>GPState</code> <p>The state of the GP model, containing the particle states and other relevant information. (See gallifrey.inference.state.GPState for details.)</p> <code>hmc_sampler_factory</code> <code>Callable</code> <p>Factory function to create HMC samplers (using blackjax), configured based on <code>config.hmc_config</code></p> <p>Methods:</p> Name Description <code>update_state</code> <p>Update the internal gpstate of the model using a new GPState object.</p> <code>save_state</code> <p>Save a GP state to file. Note that currently only the state is saved and not the model itself. That means if the model is loaded with a different configuration (e.g. different kernel library), the state might not be consistent with the model.</p> <code>load_state</code> <p>Load a GP state from file. Note that currently only the state is loaded and not the model itself. That means if the model is loaded with a different configuration (e.g. different kernel library), the state might not be consistent with the model.</p> <code>fit_mcmc</code> <p>Fit the GP model using MCMC.</p> <code>fit_smc</code> <p>Fit the GP model using SMC.</p> <code>get_particles</code> <p>Get a list of Particle instances from a GP state. If no GP state is provided, the current GP state of the model is used.</p> <code>get_predictive_distributions</code> <p>Calculate the predictive distributions for the individual particles in the GP state. The distributions are calculated at the points <code>x_predict</code> and conditioned on the training data (which was supplied to construct the model instance).</p> <code>get_mixture_distribution</code> <p>Get the mixture distribution of an SMC state (A weighted sum of the predictive distributions of the individual particles). The model should be created using the <code>fit_smc</code> method.</p> <code>display</code> <p>Print a summary of the GP model, including particle kernels and noise variances.</p> Source code in <code>gallifrey/model.py</code> <pre><code>class GPModel:\n    \"\"\"\n    The GP model class.\n\n    Attributes\n    ----------\n\n    num_particles : ScalarInt\n        Number of particles in the model.\n    config : GPConfig\n        The config instance for the GP model (see\n        gallifrey.config.GPConfig for details).\n    kernel_prior : KernelPrior\n        The kernel prior object, containing various\n        useful methods and attributes for the kernel\n        sampling. Attributes set through config. (See\n        gallifrey.kernels.prior.KernelPrior for details.)\n    noise_prior : Distribution\n        The prior distribution for the noise variance, inherited\n        from the config. (See gallifrey.config.GPConfig for details.)\n    kernel_library : KernelLibrary\n        The kernel library, containing the atomic kernels, operators,\n        and prior transforms. (See gallifrey.kernels.library.KernelLibrary\n        for details.)\n    x :  Float[jnp.ndarray, \" D\"]\n        The input x data.\n    y :  Float[jnp.ndarray, \" D\"]\n        The input y data.\n    noise_variance:\n        The initial noise variance if fixed, otherwise None.\n    fix_noise : bool\n        Flag indicating whether the noise variance is fixed or learned.\n        This is set to True if the noise variance is provided\n        as an input parameter.\n    x_transform : Optional[Callable]\n        A (optional) transformation applied to the input x data.\n    y_transform : Optional[Callable]\n        A (optional) transformation applied to the input y data.\n    x_transformed :  Float[jnp.ndarray, \" D\"]\n        The transformed input x data.\n    y_transformed :  Float[jnp.ndarray, \" D\"]\n        The transformed noise variance if fixed, otherwise None.\n    noise_variance_transformed :  Float[jnp.ndarray, \" D\"]\n        The transformed noise variance, if noise variance is provided.\n    dataset : Dataset\n        A dataset instance containing the transformed x and y data, processed\n        for the sampler.\n    particle_graphdef : nnx.GraphDef\n        Graph definition for the Particle object, shared across all particles.\n        This can be used to reconstruct the Particle object from a particle state.\n    state : GPState\n        The state of the GP model, containing the particle states and\n        other relevant information. (See gallifrey.inference.state.GPState\n        for details.)\n    hmc_sampler_factory : Callable\n        Factory function to create HMC samplers (using blackjax), configured based on\n        `config.hmc_config`\n\n    Methods\n    -------\n    update_state\n        Update the internal gpstate of the model using a new GPState object.\n    save_state\n        Save a GP state to file. Note that currently only the state is saved\n        and not the model itself. That means if the model is loaded with a different\n        configuration (e.g. different kernel library), the state might not be\n        consistent with the model.\n    load_state\n        Load a GP state from file. Note that currently only the state is loaded\n        and not the model itself. That means if the model is loaded with a different\n        configuration (e.g. different kernel library), the state might not be\n        consistent with the model.\n    fit_mcmc\n        Fit the GP model using MCMC.\n    fit_smc\n        Fit the GP model using SMC.\n    get_particles\n        Get a list of Particle instances from a GP state. If no GP state\n        is provided, the current GP state of the model is used.\n    get_predictive_distributions\n        Calculate the predictive distributions for the individual particles in the\n        GP state. The distributions are calculated at the points `x_predict` and\n        conditioned on the training data (which was supplied to construct the model\n        instance).\n    get_mixture_distribution\n        Get the mixture distribution of an SMC state (A weighted sum of the predictive\n        distributions of the individual particles). The model should be created using\n        the `fit_smc` method.\n    display\n        Print a summary of the GP model, including particle kernels and noise variances.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        key: PRNGKeyArray,\n        x: Float[Array, \" D\"],\n        y: Float[Array, \" D\"],\n        num_particles: ScalarInt,\n        noise_variance: tp.Optional[ScalarFloat] = None,\n        x_transform: tp.Type[Transform] = LinearTransform,\n        y_transform: tp.Type[Transform] = LinearTransform,\n        config: tp.Optional[GPConfig] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the GP model.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            The random key for the initial sampling.\n        x : Float[Array, \" D\"]\n            Input data, array of shape (D,).\n        y : Float[Array, \" D\"]\n            Target data, array of shape (D,).\n        num_particles : ScalarInt\n            The number of particles in the model.\n        noise_variance : tp.Optional[ScalarFloat], optional\n            The variance of the observation noise. If\n            None, the noise variance is sampled and treated\n            as a trainable parameter. By default None.\n            NOTE: Currently heteroscadastic noise is not\n            supported.\n        x_transform : Transform, optional\n            A transformation applied to the input x data, must\n            be an instance of gallifrey.inference.transforms.Transform\n            class. By default LinearTransform. (Used to normalise\n            data for easier training.)\n        y_transform : Optional[Callable], optional\n            A transformation applied to the input y data, must\n            be an instance of gallifrey.inference.transforms.Transform\n            class. By default LinearTransform. (Used to normalise\n            data for easier training.)\n        config : tp.Optional[GPConfig], optional\n            The configuration object for the GP model. Contains\n            information of the kernel and parameter priors, the\n            mean function, the max depth of the tree kernel, etc.\n            By default None, in which case the default configuration\n            is used (see gallifrey.config.GPConfig for details).\n        \"\"\"\n        if len(x) != len(y):\n            raise ValueError(\n                f\"Input data x and y must have the same length, \"\n                \"but got len(x)={len(x)} and len(y)={len(y)}.\"\n            )\n\n        # set basic attributes\n        self.config = config if config is not None else GPConfig()\n        self.fix_noise = True if noise_variance is not None else False\n\n        # create kernel prior\n        kernel_library = KernelLibrary(\n            atoms=deepcopy(self.config.atoms),\n            operators=deepcopy(self.config.operators),\n            prior_transforms=deepcopy(self.config.prior_transforms),\n        )\n\n        self.kernel_prior = KernelPrior(\n            kernel_library,\n            max_depth=deepcopy(self.config.max_depth),\n            num_datapoints=len(x),\n            probs=deepcopy(self.config.node_probabilities),\n        )\n\n        # preprocess data (apply transformations, and set attributes)\n        self._preprocess_data(\n            x,\n            y,\n            x_transform,\n            y_transform,\n            noise_variance,\n        )\n\n        # create a particle_graphdef attributes (using a randomly\n        # initilized particle state). The graphdef attribute,\n        # is used to create particle instances from the particle states\n        self.particle_graphdef, particle_state = initialize_particle_state(\n            self.kernel_prior.sample(jr.PRNGKey(0)),\n            self.kernel_prior,\n            self._sample_noise_variance(jr.PRNGKey(0)),\n            self.fix_noise,\n        )\n\n        # sample initial particles and create initial GP state\n        key, particle_key = jr.split(key)\n        self._initialize_state(particle_key, num_particles)\n\n        # create HMC sampler factory for rejuvenation\n        self.hmc_sampler_factory = create_hmc_sampler_factory(\n            self.config.hmc_config,\n            int(self.kernel_prior.max_kernel_parameter + (not self.fix_noise)),\n        )\n\n    def _preprocess_data(\n        self,\n        x: Float[Array, \" D\"],\n        y: Float[Array, \" D\"],\n        x_transform: tp.Type[Transform],\n        y_transform: tp.Type[Transform],\n        noise_variance: tp.Optional[ScalarFloat],\n    ) -&gt; None:\n        \"\"\"\n        Preprocesses the input data by applying transformations\n        and creating a Dataset object.\n\n        Applies the specified transformations (`x_transform` and `y_transform`)\n        to the input data `x` and `y`. If a fixed `noise_variance` is provided,\n        it is also transformed using `y_transform`.\n        Finally, it creates a `Dataset` object with the transformed\n        data, which is used for GPJax computations.\n\n\n        Parameters\n        ----------\n        x :  Float[jnp.ndarray, \" D\"]\n            The input x data.\n        y :  Float[jnp.ndarray, \" D\"]\n            The input y data.\n        x_transform : tp.Type[Transform]\n            A transformation applied to the input x data.\n        y_transform : tp.Type[Transform]\n            A transformation applied to the input y data.\n        noise_variance : tp.Optional[ScalarFloat]\n            The variance of the observation noise.\n        \"\"\"\n        # set attributes\n        self.x = jnp.asarray(x)\n        self.y = jnp.asarray(y)\n\n        # create transformation from data and apply to x and y\n        self.x_transform = x_transform.from_data_range(self.x, 0, 1)\n        self.y_transform = y_transform.from_data_width(self.y, 1)\n\n        self.x_transformed = self.x_transform.apply(self.x)\n        self.y_transformed = self.y_transform.apply(self.y)\n\n        # transform noise variance if provided\n        if noise_variance is not None:\n            self.noise_variance: ScalarFloat | None = jnp.asarray(noise_variance)\n            self.noise_variance_transformed: ScalarFloat | None = jnp.asarray(\n                self.y_transform.apply_var(noise_variance)\n            )\n        else:\n            self.noise_variance = None\n            self.noise_variance_transformed = None\n\n        # ensure that x_transformed and y_transformed are still 1D arrays and of\n        # the same length\n        if (not self.x_transformed.ndim == 1) or (not self.y_transformed.ndim == 1):\n            raise ValueError(\n                \"x_transformed and y_transformed must be 1D arrays. \"\n                \"Please check the transformation functions.\"\n            )\n        if len(self.x_transformed) != len(self.y_transformed):\n            raise ValueError(\n                \"x_transformed and y_transformed must have the same length.\"\n            )\n\n        self.data = Dataset(\n            self.x_transformed,\n            self.y_transformed,\n        )\n\n    def _sample_noise_variance(\n        self,\n        key: PRNGKeyArray,\n    ) -&gt; ScalarFloat:\n        \"\"\"\n        Sample the noise variance from the prior distribution, if no\n        noise variance is provided. If a noise variance is provided, it\n        is returned as is.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for sampling.\n\n        Returns\n        -------\n         Float[jnp.ndarray, \" D\"]\n            The sampled noise variance, one for each particle. (If noise_variance\n            is provided, the same value is returned for all particles.)\n        \"\"\"\n\n        if self.noise_variance is None:\n            key, noise_key = jr.split(key)\n            variance: ScalarFloat = jnp.array(\n                self.config.noise_prior.sample(\n                    seed=noise_key,\n                ),\n                dtype=self.x.dtype,\n            )\n        else:\n            assert self.noise_variance_transformed is not None\n            variance = self.noise_variance_transformed\n        return variance\n\n    def _initialize_state(\n        self,\n        key: PRNGKeyArray,\n        num_particles: ScalarInt,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the state of the GP model. This primarily\n        entails creating the initial particle states by sampling\n        the kernel and noise variance (if not fixed).\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for sampling.\n        num_particles : ScalarInt\n            Number of particles in the model.\n\n        Returns\n        -------\n        GPState\n            A GPState object representing the state of the GP model (see\n            gallifrey.inference.state.GPState for details).\n        \"\"\"\n\n        def make_particle_state(key: PRNGKeyArray) -&gt; nnx.State:\n            \"\"\"\n            Create partice by sampling the kernel and noise variance\n            (if not fixed), and initializing the particle state.\n            \"\"\"\n            key, kernel_key, variance_key = jr.split(key, 3)\n            kernel_state = self.kernel_prior.sample(kernel_key)\n            noise_variance = self._sample_noise_variance(variance_key)\n\n            _, particle_state = initialize_particle_state(\n                kernel_state,\n                self.kernel_prior,\n                noise_variance,\n                self.fix_noise,\n            )\n\n            return particle_state\n\n        # create particle states by looping over the number of particles,\n        # and batch into a single state object\n        num_particles = int(num_particles)\n        particle_states = batch_states(\n            [make_particle_state(key) for key in jr.split(key, num_particles)]\n        )\n\n        self.state = GPState(\n            particle_states=particle_states,\n            num_particles=num_particles,\n            num_data_points=self.data.n,\n            mcmc_accepted=jnp.zeros(num_particles),\n            hmc_accepted=jnp.zeros(num_particles),\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the GPModel.\n\n        Returns\n        -------\n        str\n            A string representation of the GPModel.\n        \"\"\"\n        return f\"GPModel(num_particles={self.num_particles})\"\n\n    @property\n    def kernel_library(self) -&gt; KernelLibrary:\n        \"\"\"\n        Get the kernel library.\n\n        Returns\n        -------\n        KernelLibrary\n            The kernel library.\n        \"\"\"\n        return self.kernel_prior.kernel_library\n\n    @property\n    def noise_prior(self) -&gt; Distribution:\n        \"\"\"\n        Get the noise prior distribution.\n\n        Returns\n        -------\n        InverseGamma\n            The noise prior distribution.\n        \"\"\"\n        return self.config.noise_prior\n\n    @property\n    def num_particles(self) -&gt; ScalarInt:\n        \"\"\"\n        Get the number of particles in the model.\n\n        Returns\n        -------\n        ScalarInt\n            The number of particles.\n        \"\"\"\n        return self.state.num_particles\n\n    def update_state(self, gpstate: GPState) -&gt; GPModel:\n        \"\"\"\n        Update the GP state. Returns a new GPModel instance (no in-place\n        update). If the number of particles in the new state is different\n        from the current state, the num_particles attribute is updated.\n\n        Note that no other attributes are updated. If the particle states\n        were created using a different configuration, the model will not be\n        consistent.\n\n        Parameters\n        ----------\n        gpstate : GPState\n            The new GP state to update the model with.\n\n        Returns\n        -------\n        GPModel\n            A new GPModel instance with updated state.\n\n        \"\"\"\n\n        new_gpmodel = deepcopy(self)\n        new_gpmodel.state = gpstate\n\n        return new_gpmodel\n\n    def save_state(\n        self,\n        path: str | PosixPath,\n        gpstate: tp.Optional[GPState] = None,\n    ) -&gt; None:\n        \"\"\"\n        Save a GP state to file.\n\n        Note that only the state is saved and not the model itself.\n        That means if the model is loaded with a different configuration\n        (e.g. different kernel library), the state might not be consistent\n        with the model.\n\n        TODO: Implement saving the model configuration as well.\n\n        Parameters\n        ----------\n        path : str | PosixPath\n            The path where to save the GP state, must\n            be an absolute path.\n        gpstate : tp.Optional[GPState], optional\n            The GP state to save, If None, the current state\n            of the model is used. By default None.\n\n        \"\"\"\n        if gpstate is None:\n            gpstate = self.state\n\n        with open(path, \"wb\") as file:\n            pickle.dump(gpstate, file)\n\n        return None\n\n    @classmethod\n    def load_state(\n        cls,\n        path: str | PosixPath,\n    ) -&gt; GPState:\n        \"\"\"\n        Load a GP state from file.\n\n        Note that only the state is loaded. It is assumed that the model\n        configuration is consistent with how the state was saved. If the\n        model configuration is different, the loaded state might not be\n        consistent with the model.\n\n        The model does not get update with the loaded state. To update the\n        model, use the `update_gpstate` method with the loaded state.\n\n        TODO: Implement loading the model configuration as well.\n\n        Parameters\n        ----------\n        path : str | PosixPath\n            The path where to load the GP state from.\n\n        Returns\n        -------\n        GPState\n            An instance of the GPState object, containing the loaded state.\n\n        \"\"\"\n        with open(path, \"rb\") as file:\n            gpstate = pickle.load(file)\n        return gpstate\n\n    def fit_mcmc(\n        self,\n        key: PRNGKeyArray,\n        n_mcmc: ScalarInt,\n        n_hmc: ScalarInt,\n        verbosity: ScalarInt = 0,\n    ) -&gt; tuple[GPState, GPState]:\n        \"\"\"\n        Fits the GP model using MCMC.\n        It perfoms n_mcmc iterations of the structure, and for each\n        accepted structure move performs n_hmc iterations of the HMC\n        sampler over the parameters.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for MCMC sampling.\n        n_mcmc : ScalarInt\n            Number of MCMC iterations over kernel structure.\n        n_hmc : ScalarInt\n            Number of HMC steps for continuous parameters. Only used if\n            the structure move is accepted.\n        verbosity : ScalarInt, optional\n            The verbosity level, by default 0. Debugging information\n            is printed if `verbosity &gt; 1`.\n\n        Returns\n        -------\n        GPState\n            The final state of the model, wrapped in an GPState object (see\n            gallifrey.inference.state.GPState for details).\n        GPState\n            The history over all MCMC iterations, wrapped in an GPState object.\n\n        \"\"\"\n        if not isinstance(n_mcmc, int):\n            raise TypeError(\n                f\"Expected `n_mcmc` to be an integer, but got {type(n_mcmc)}.\"\n            )\n        if not isinstance(n_hmc, int):\n            raise TypeError(\n                f\"Expected `n_hmc` to be an integer, but got {type(n_hmc)}.\"\n            )\n        if n_mcmc &lt;= 0:\n            raise ValueError(\n                f\"Expected `n_mcmc` to be a positive integer, but got {n_mcmc}.\"\n            )\n        if n_hmc &lt;= 0:\n            raise ValueError(\n                f\"Expected `n_hmc` to be a positive integer, but got {n_hmc}.\"\n            )\n\n        def wrapper(\n            key: PRNGKeyArray,\n            state: nnx.State,\n        ) -&gt; tuple[nnx.State, nnx.State, ScalarInt, ScalarInt]:\n            \"\"\"Wrapper around the rejuvenate_particle function using\n            GPmodel attributes.\"\"\"\n            return rejuvenate_particle(\n                key,\n                state,\n                self.data,\n                self.kernel_prior,\n                self.noise_prior,\n                n_mcmc=n_mcmc,\n                n_hmc=n_hmc,\n                fix_noise=self.fix_noise,\n                hmc_sampler_factory=self.hmc_sampler_factory,\n                verbosity=verbosity,\n            )\n\n        final_state, history, accepted_mcmc, accepted_hmc = pmap(\n            jit(wrapper), in_axes=0\n        )(\n            jr.split(key, int(self.num_particles)),\n            self.state.particle_states,  # use states batched over 0th axis\n        )\n\n        # print information\n        if verbosity &gt; 0:\n            for i, acc_mcmc, acc_hmc in zip(\n                range(self.num_particles), accepted_mcmc, accepted_hmc\n            ):\n                print(\n                    f\"Particle {i+1} | Accepted: MCMC[{acc_mcmc}/{n_mcmc}] \"\n                    f\" HMC[{acc_hmc}/{acc_mcmc*n_hmc}]\"\n                )\n\n        # wrap final state and history in GPState objects, for consistency\n        # with the SMC algorithm\n        final_state_wrapped = GPState(\n            particle_states=final_state,\n            num_particles=self.num_particles,\n            num_data_points=self.data.n,\n            mcmc_accepted=accepted_mcmc,\n            hmc_accepted=accepted_hmc,\n        )\n\n        history_wrapped = GPState(\n            particle_states=history,\n            num_particles=self.num_particles,\n            num_data_points=self.data.n,\n            mcmc_accepted=accepted_mcmc,\n            hmc_accepted=accepted_hmc,\n        )\n\n        return final_state_wrapped, history_wrapped\n\n    def fit_smc(\n        self,\n        key: PRNGKeyArray,\n        annealing_schedule: tuple[int, ...],\n        n_mcmc: ScalarInt,\n        n_hmc: ScalarInt,\n        verbosity: int = 0,\n    ) -&gt; tuple[GPState, GPState]:\n        \"\"\"\n        Fits the GP model using SMC.\n\n        For a detailed description of the SMC algorithm, see the\n        'gallofrey.inference.smc.smc_loop' function.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            The random key for the SMC sampling.\n        annealing_schedule : tuple[int, ...]\n            The data annealing schedule for the SMC algorithm,\n            number of data points to consider at each step.\n            NOTE: Must be given in form of a tuple of integers, for\n            jax compatibility. Easily generated using the `generate`\n            method of the `Schedule` in `gallifrey.schedule`.\n        n_mcmc : ScalarInt\n            Number of MCMC iterations over kernel structure per\n            SMC step.\n        n_hmc : ScalarInt\n            Number of HMC steps for continuous parameters per\n            SMC step. Only used if the structure move is accepted.\n        verbosity : int, optional\n            The verbosity level, by default 0. Debugging information\n            is printed if `verbosity &gt; 1`.\n\n        Returns\n        -------\n        GPState\n            The final SMC state. Contains the final particle states\n            and the final weights (among other things, see\n            'gallifrey.inference.state.GPState' for details).\n        GPState\n            The history of the SMC algorithm. Contains the particle\n            states and weights at each step of the algorithm.\n\n        \"\"\"\n\n        final_smc_state, history = smc_loop(\n            key,\n            self.state.particle_states,\n            annealing_schedule,\n            int(self.num_particles),\n            self.data,\n            self.kernel_prior,\n            self.noise_prior,\n            self.fix_noise,\n            self.hmc_sampler_factory,\n            n_mcmc,\n            n_hmc,\n            verbosity=verbosity,\n        )\n\n        batched_history: GPState = batch_states(history)\n\n        return final_smc_state, batched_history\n\n    def get_particles(\n        self,\n        gpstate: tp.Optional[GPState] = None,\n    ) -&gt; list[Particle]:\n        \"\"\"\n        Get a list of Particle instances from a GP state.\n\n        If no state is provided, the current state of the model\n        is used.\n\n        Parameters\n        ----------\n        gpstate : tp.Optional[GPState], optional\n            The GP state to extract the particles from. If None,\n            the current state of the model is used. By default None.\n\n        Returns\n        -------\n        list[Particle]\n            A list of Particle instances, corresponding to the\n            individual states.\n        \"\"\"\n        if gpstate is None:\n            gpstate = self.state\n\n        unbatched_particle_states = unbatch_states(gpstate.particle_states)\n        return [\n            nnx.merge(self.particle_graphdef, state)\n            for state in unbatched_particle_states\n        ]\n\n    def get_predictive_distributions(\n        self,\n        xpredict: Float[Array, \" D\"],\n        data: tp.Optional[Dataset] = None,\n        gpstate: tp.Optional[GPState] = None,\n        latent: bool = False,\n    ) -&gt; list[Distribution]:\n        \"\"\"\n        Calculate the predictive distributions for the individual particles\n        in the GP state. The distributions are calculated at the points\n        `x_predict` and conditioned on the training data (which was supplied\n        to construct the model instance).\n\n        The distributions are returned as a list of tensorflow probability\n        distribution objects, see\n        https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalFullCovariance\n\n        If no state is provided, the current state of the model\n        is used.\n\n        If `latent` is True, the predictive distribution is that of the latent\n        function, i.e. the distribution of the function values without the\n        observational noise. If False, the predictive distribution of the full\n        data-generating model is returned, which includes the observational noise.\n\n        Parameters\n        ----------\n        xpredict : Float[Array, \" D\"]\n            The points to predict, as a 1D array.\n        data : tp.Optional[Dataset], optional\n            The data to condition the predictive distribution on. If None,\n            the training data of the model is used. By default None.\n        gpstate : tp.Optional[GPState], optional\n            The GP state object, containing the particle states. If None,\n            the current state of the model is used. By default None.\n        latent : bool, optional\n            Whether to return the predictive distribution of the latent\n            functions only (without observational noise), by default\n            False.\n\n        Returns\n        -------\n        list[Distribution]\n            A list of tensorflow probability distribution objects\n            representing the predictive distributions of the Gaussian\n            processes. (Specifically, a MultivariateNormalFullCovariance\n            distribution from `tensorflow_probability.substrates.jax.distributions`).\n\n        \"\"\"\n        gpstate = self.state if gpstate is None else gpstate\n        data = self.data if data is None else data\n\n        particles = self.get_particles(gpstate)\n        distributions = [\n            particle.predictive_distribution(\n                jnp.atleast_1d(xpredict).squeeze(),\n                data,\n                latent,\n            )\n            for particle in particles\n        ]\n\n        return distributions\n\n    def get_mixture_distribution(\n        self,\n        xpredict: Float[Array, \" D\"],\n        gpstate: tp.Optional[GPState] = None,\n        data: tp.Optional[Dataset] = None,\n        log_weights: tp.Optional[Float[Array, \" N\"]] = None,\n        num_particles: tp.Optional[ScalarInt] = None,\n        key: tp.Optional[PRNGKeyArray] = None,\n        latent: bool = False,\n    ) -&gt; Distribution:\n        \"\"\"\n        Get the mixture distribution of an SMC state.\n\n        The predictive distributions for an SMC ensemble are\n        the individual predictive (Gaussion) distributions of the\n        particles, weighted by the particle weights. The resulting\n        distribution is a Gaussian mixture model, implemented as\n        a `MixtureSameFamily` distribution from `tensorflow_probability`, see\n        https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MixtureSameFamily\n\n        The mixture distribution is calculated at the points `xpredict`\n        and conditioned on the training data (which was supplied to construct\n        the model instance).\n\n        The input should be an GPState object as returned by the `fit_smc` method.\n        Alternatively, the log weights can be provided explicitly using the\n        `log_weights` argument.\n\n\n        Parameters\n        ----------\n        xpredict : Float[Array, \" D\"]\n            The points to predict, as a 1D array.\n        gpstate : tp.Optional[GPState], optional\n            The GP state object, containing the particle states and log weights for\n            the mixture distribution. If None, the current state of the model is used.\n            By default None.\n        data : tp.Optional[Dataset], optional\n            The data to condition the predictive distribution on. If None,\n            the training data of the model is used. By default None.\n        log_weights : Float[Array, \" N\"], optional\n            The log weights of the particles. If None, the log weights from the\n            GP state are used. An error is raised if the log weights are not provided\n            and the GP state does not contain log weights. By default None.\n        num_particles : tp.Optional[ScalarInt], optional\n            Number of particles to include in the mixture distribution. If None,\n            all particles in the state are included. If provided, a random sample\n            of particles is chosen based on the weights. By default None.\n        key : tp.Optional[PRNGKeyArray], optional\n            Random key for sampling the particles. Required if `num_particles` is\n            provided. By default None.\n        latent : bool, optional\n            Whether to return the predictive distribution of the latent\n            functions only (without observational noise), by default\n            False.\n\n        Returns\n        -------\n        Distribution\n            A tensorflow probability distribution object representing\n            the Gaussian mixture distribution of the Gaussian processes\n            (a MixtureSameFamily distribution).\n\n        Raises\n        ------\n        ValueError\n            If the GPState object contains no log weights and `log_weights` is None.\n        ValueError\n            If the number of particles and log weights are inconsistent.\n\n        \"\"\"\n        gpstate = self.state if gpstate is None else gpstate\n        data = self.data if data is None else data\n\n        if (gpstate.log_weights is None) and (log_weights is None):\n            raise ValueError(\n                \"The GPState object contains no log weights. This might be \"\n                \"because the state was produced with the MCMC sampler or the \"\n                \"initial state of the GPModel was used. Please either run \"\n                \"`fit_smc` or provide the log weights explicitly. (Note: If \"\n                \"you already ran `fit_smc` and this error occurs, make sure to \"\n                \"passed the the output of `fit_smc` to this method, or run the \"\n                \"`update_state` method with the output of `fit_smc` as input.)\"\n            )\n\n        log_weights = log_weights if log_weights is not None else gpstate.log_weights\n\n        assert log_weights is not None\n        if len(log_weights) != self.num_particles:\n            raise ValueError(\n                f\"Inconsistent number of particles and log weights, \"\n                f\"expected {self.num_particles} but got {len(log_weights)}.\"\n            )\n\n        individual_distributions = self.get_predictive_distributions(\n            xpredict,\n            data,\n            gpstate,\n            latent,\n        )\n\n        if num_particles is not None:\n            if key is None:\n                raise ValueError(\n                    \"If `num_particles` is provided, `key` must also be provided. \"\n                    \"This is the random key for sampling from the particles.\"\n                )\n            if num_particles &gt; self.num_particles:\n                raise ValueError(\n                    f\"Number of particles to sample ({num_particles}) \"\n                    f\"exceeds the total number of particles ({self.num_particles}).\"\n                )\n            # choose random sample of particles based on weights\n            weights = jnp.exp(log_weights)\n            particle_indices = jr.choice(\n                key,\n                jnp.arange(len(weights)),\n                p=weights,\n                shape=(int(num_particles),),\n                replace=False,\n            )\n            # normalize weights\n            log_weights = log_weights[particle_indices] - logsumexp(\n                log_weights[particle_indices]\n            )\n            # select particles\n            individual_distributions = [\n                individual_distributions[idx] for idx in particle_indices\n            ]\n\n        batched_distributions = batch_states(individual_distributions)\n\n        mixture_model = MixtureSameFamily(\n            mixture_distribution=Categorical(logits=log_weights),\n            components_distribution=batched_distributions,\n        )\n\n        return mixture_model\n\n    def display(\n        self,\n        gpstate: tp.Optional[GPState] = None,\n        num_particles: tp.Optional[ScalarInt] = None,\n    ) -&gt; None:\n        \"\"\"\n        Prints a summary of the GP model, including particle kernels\n        and noise variances.\n\n        If no particle states are provided, the current batched state of the model\n        is used.\n\n        Iterates through each particle, merges its graph definition and\n        state, and prints the particle index, noise variance, and the kernel structure.\n        Useful for inspecting the current state of the particle ensemble.\n\n        Parameters\n        ----------\n        gpstate : tp.Optional[GPState], optional\n            The GP state object, containing the particle states. If None, the current\n            state of the model is used. By default None.\n        num_particles : tp.Optional[ScalarInt], optional\n            Number of particles to display. If None, all particles are displayed.\n            By default None.\n\n        \"\"\"\n        gpstate = gpstate if gpstate is not None else self.state\n        num_particles = self.num_particles if num_particles is None else num_particles\n\n        particles = self.get_particles(gpstate)\n        for i in range(num_particles):\n            print(\"=\" * 50)\n            if gpstate.log_weights is not None:\n                print(\n                    f\"Particle {i+1} \"\n                    f\"| Weight: {jnp.exp(gpstate.log_weights[i]):.2f} \"\n                    f\"| Variance: {particles[i].noise_variance.value} \"\n                )\n            else:\n                print(\n                    f\"Particle {i+1} \"\n                    f\"| Variance: {particles[i].noise_variance.value} \"\n                )\n            print(f\"{particles[i].kernel}\")\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel.kernel_library","title":"<code>kernel_library</code>  <code>property</code>","text":"<p>Get the kernel library.</p> <p>Returns:</p> Type Description <code>KernelLibrary</code> <p>The kernel library.</p>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel.noise_prior","title":"<code>noise_prior</code>  <code>property</code>","text":"<p>Get the noise prior distribution.</p> <p>Returns:</p> Type Description <code>InverseGamma</code> <p>The noise prior distribution.</p>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel.num_particles","title":"<code>num_particles</code>  <code>property</code>","text":"<p>Get the number of particles in the model.</p> <p>Returns:</p> Type Description <code>ScalarInt</code> <p>The number of particles.</p>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel.__init__","title":"<code>__init__(key, x, y, num_particles, noise_variance=None, x_transform=LinearTransform, y_transform=LinearTransform, config=None)</code>","text":"<p>Initialize the GP model.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key for the initial sampling.</p> required <code>x</code> <code>Float[Array, ' D']</code> <p>Input data, array of shape (D,).</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>Target data, array of shape (D,).</p> required <code>num_particles</code> <code>ScalarInt</code> <p>The number of particles in the model.</p> required <code>noise_variance</code> <code>Optional[ScalarFloat]</code> <p>The variance of the observation noise. If None, the noise variance is sampled and treated as a trainable parameter. By default None. NOTE: Currently heteroscadastic noise is not supported.</p> <code>None</code> <code>x_transform</code> <code>Transform</code> <p>A transformation applied to the input x data, must be an instance of gallifrey.inference.transforms.Transform class. By default LinearTransform. (Used to normalise data for easier training.)</p> <code>LinearTransform</code> <code>y_transform</code> <code>Optional[Callable]</code> <p>A transformation applied to the input y data, must be an instance of gallifrey.inference.transforms.Transform class. By default LinearTransform. (Used to normalise data for easier training.)</p> <code>LinearTransform</code> <code>config</code> <code>Optional[GPConfig]</code> <p>The configuration object for the GP model. Contains information of the kernel and parameter priors, the mean function, the max depth of the tree kernel, etc. By default None, in which case the default configuration is used (see gallifrey.config.GPConfig for details).</p> <code>None</code> Source code in <code>gallifrey/model.py</code> <pre><code>def __init__(\n    self,\n    key: PRNGKeyArray,\n    x: Float[Array, \" D\"],\n    y: Float[Array, \" D\"],\n    num_particles: ScalarInt,\n    noise_variance: tp.Optional[ScalarFloat] = None,\n    x_transform: tp.Type[Transform] = LinearTransform,\n    y_transform: tp.Type[Transform] = LinearTransform,\n    config: tp.Optional[GPConfig] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the GP model.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key for the initial sampling.\n    x : Float[Array, \" D\"]\n        Input data, array of shape (D,).\n    y : Float[Array, \" D\"]\n        Target data, array of shape (D,).\n    num_particles : ScalarInt\n        The number of particles in the model.\n    noise_variance : tp.Optional[ScalarFloat], optional\n        The variance of the observation noise. If\n        None, the noise variance is sampled and treated\n        as a trainable parameter. By default None.\n        NOTE: Currently heteroscadastic noise is not\n        supported.\n    x_transform : Transform, optional\n        A transformation applied to the input x data, must\n        be an instance of gallifrey.inference.transforms.Transform\n        class. By default LinearTransform. (Used to normalise\n        data for easier training.)\n    y_transform : Optional[Callable], optional\n        A transformation applied to the input y data, must\n        be an instance of gallifrey.inference.transforms.Transform\n        class. By default LinearTransform. (Used to normalise\n        data for easier training.)\n    config : tp.Optional[GPConfig], optional\n        The configuration object for the GP model. Contains\n        information of the kernel and parameter priors, the\n        mean function, the max depth of the tree kernel, etc.\n        By default None, in which case the default configuration\n        is used (see gallifrey.config.GPConfig for details).\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\n            f\"Input data x and y must have the same length, \"\n            \"but got len(x)={len(x)} and len(y)={len(y)}.\"\n        )\n\n    # set basic attributes\n    self.config = config if config is not None else GPConfig()\n    self.fix_noise = True if noise_variance is not None else False\n\n    # create kernel prior\n    kernel_library = KernelLibrary(\n        atoms=deepcopy(self.config.atoms),\n        operators=deepcopy(self.config.operators),\n        prior_transforms=deepcopy(self.config.prior_transforms),\n    )\n\n    self.kernel_prior = KernelPrior(\n        kernel_library,\n        max_depth=deepcopy(self.config.max_depth),\n        num_datapoints=len(x),\n        probs=deepcopy(self.config.node_probabilities),\n    )\n\n    # preprocess data (apply transformations, and set attributes)\n    self._preprocess_data(\n        x,\n        y,\n        x_transform,\n        y_transform,\n        noise_variance,\n    )\n\n    # create a particle_graphdef attributes (using a randomly\n    # initilized particle state). The graphdef attribute,\n    # is used to create particle instances from the particle states\n    self.particle_graphdef, particle_state = initialize_particle_state(\n        self.kernel_prior.sample(jr.PRNGKey(0)),\n        self.kernel_prior,\n        self._sample_noise_variance(jr.PRNGKey(0)),\n        self.fix_noise,\n    )\n\n    # sample initial particles and create initial GP state\n    key, particle_key = jr.split(key)\n    self._initialize_state(particle_key, num_particles)\n\n    # create HMC sampler factory for rejuvenation\n    self.hmc_sampler_factory = create_hmc_sampler_factory(\n        self.config.hmc_config,\n        int(self.kernel_prior.max_kernel_parameter + (not self.fix_noise)),\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the GPModel.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the GPModel.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the GPModel.\n\n    Returns\n    -------\n    str\n        A string representation of the GPModel.\n    \"\"\"\n    return f\"GPModel(num_particles={self.num_particles})\"\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel.display","title":"<code>display(gpstate=None, num_particles=None)</code>","text":"<p>Prints a summary of the GP model, including particle kernels and noise variances.</p> <p>If no particle states are provided, the current batched state of the model is used.</p> <p>Iterates through each particle, merges its graph definition and state, and prints the particle index, noise variance, and the kernel structure. Useful for inspecting the current state of the particle ensemble.</p> <p>Parameters:</p> Name Type Description Default <code>gpstate</code> <code>Optional[GPState]</code> <p>The GP state object, containing the particle states. If None, the current state of the model is used. By default None.</p> <code>None</code> <code>num_particles</code> <code>Optional[ScalarInt]</code> <p>Number of particles to display. If None, all particles are displayed. By default None.</p> <code>None</code> Source code in <code>gallifrey/model.py</code> <pre><code>def display(\n    self,\n    gpstate: tp.Optional[GPState] = None,\n    num_particles: tp.Optional[ScalarInt] = None,\n) -&gt; None:\n    \"\"\"\n    Prints a summary of the GP model, including particle kernels\n    and noise variances.\n\n    If no particle states are provided, the current batched state of the model\n    is used.\n\n    Iterates through each particle, merges its graph definition and\n    state, and prints the particle index, noise variance, and the kernel structure.\n    Useful for inspecting the current state of the particle ensemble.\n\n    Parameters\n    ----------\n    gpstate : tp.Optional[GPState], optional\n        The GP state object, containing the particle states. If None, the current\n        state of the model is used. By default None.\n    num_particles : tp.Optional[ScalarInt], optional\n        Number of particles to display. If None, all particles are displayed.\n        By default None.\n\n    \"\"\"\n    gpstate = gpstate if gpstate is not None else self.state\n    num_particles = self.num_particles if num_particles is None else num_particles\n\n    particles = self.get_particles(gpstate)\n    for i in range(num_particles):\n        print(\"=\" * 50)\n        if gpstate.log_weights is not None:\n            print(\n                f\"Particle {i+1} \"\n                f\"| Weight: {jnp.exp(gpstate.log_weights[i]):.2f} \"\n                f\"| Variance: {particles[i].noise_variance.value} \"\n            )\n        else:\n            print(\n                f\"Particle {i+1} \"\n                f\"| Variance: {particles[i].noise_variance.value} \"\n            )\n        print(f\"{particles[i].kernel}\")\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel.fit_mcmc","title":"<code>fit_mcmc(key, n_mcmc, n_hmc, verbosity=0)</code>","text":"<p>Fits the GP model using MCMC. It perfoms n_mcmc iterations of the structure, and for each accepted structure move performs n_hmc iterations of the HMC sampler over the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for MCMC sampling.</p> required <code>n_mcmc</code> <code>ScalarInt</code> <p>Number of MCMC iterations over kernel structure.</p> required <code>n_hmc</code> <code>ScalarInt</code> <p>Number of HMC steps for continuous parameters. Only used if the structure move is accepted.</p> required <code>verbosity</code> <code>ScalarInt</code> <p>The verbosity level, by default 0. Debugging information is printed if <code>verbosity &gt; 1</code>.</p> <code>0</code> <p>Returns:</p> Type Description <code>GPState</code> <p>The final state of the model, wrapped in an GPState object (see gallifrey.inference.state.GPState for details).</p> <code>GPState</code> <p>The history over all MCMC iterations, wrapped in an GPState object.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def fit_mcmc(\n    self,\n    key: PRNGKeyArray,\n    n_mcmc: ScalarInt,\n    n_hmc: ScalarInt,\n    verbosity: ScalarInt = 0,\n) -&gt; tuple[GPState, GPState]:\n    \"\"\"\n    Fits the GP model using MCMC.\n    It perfoms n_mcmc iterations of the structure, and for each\n    accepted structure move performs n_hmc iterations of the HMC\n    sampler over the parameters.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for MCMC sampling.\n    n_mcmc : ScalarInt\n        Number of MCMC iterations over kernel structure.\n    n_hmc : ScalarInt\n        Number of HMC steps for continuous parameters. Only used if\n        the structure move is accepted.\n    verbosity : ScalarInt, optional\n        The verbosity level, by default 0. Debugging information\n        is printed if `verbosity &gt; 1`.\n\n    Returns\n    -------\n    GPState\n        The final state of the model, wrapped in an GPState object (see\n        gallifrey.inference.state.GPState for details).\n    GPState\n        The history over all MCMC iterations, wrapped in an GPState object.\n\n    \"\"\"\n    if not isinstance(n_mcmc, int):\n        raise TypeError(\n            f\"Expected `n_mcmc` to be an integer, but got {type(n_mcmc)}.\"\n        )\n    if not isinstance(n_hmc, int):\n        raise TypeError(\n            f\"Expected `n_hmc` to be an integer, but got {type(n_hmc)}.\"\n        )\n    if n_mcmc &lt;= 0:\n        raise ValueError(\n            f\"Expected `n_mcmc` to be a positive integer, but got {n_mcmc}.\"\n        )\n    if n_hmc &lt;= 0:\n        raise ValueError(\n            f\"Expected `n_hmc` to be a positive integer, but got {n_hmc}.\"\n        )\n\n    def wrapper(\n        key: PRNGKeyArray,\n        state: nnx.State,\n    ) -&gt; tuple[nnx.State, nnx.State, ScalarInt, ScalarInt]:\n        \"\"\"Wrapper around the rejuvenate_particle function using\n        GPmodel attributes.\"\"\"\n        return rejuvenate_particle(\n            key,\n            state,\n            self.data,\n            self.kernel_prior,\n            self.noise_prior,\n            n_mcmc=n_mcmc,\n            n_hmc=n_hmc,\n            fix_noise=self.fix_noise,\n            hmc_sampler_factory=self.hmc_sampler_factory,\n            verbosity=verbosity,\n        )\n\n    final_state, history, accepted_mcmc, accepted_hmc = pmap(\n        jit(wrapper), in_axes=0\n    )(\n        jr.split(key, int(self.num_particles)),\n        self.state.particle_states,  # use states batched over 0th axis\n    )\n\n    # print information\n    if verbosity &gt; 0:\n        for i, acc_mcmc, acc_hmc in zip(\n            range(self.num_particles), accepted_mcmc, accepted_hmc\n        ):\n            print(\n                f\"Particle {i+1} | Accepted: MCMC[{acc_mcmc}/{n_mcmc}] \"\n                f\" HMC[{acc_hmc}/{acc_mcmc*n_hmc}]\"\n            )\n\n    # wrap final state and history in GPState objects, for consistency\n    # with the SMC algorithm\n    final_state_wrapped = GPState(\n        particle_states=final_state,\n        num_particles=self.num_particles,\n        num_data_points=self.data.n,\n        mcmc_accepted=accepted_mcmc,\n        hmc_accepted=accepted_hmc,\n    )\n\n    history_wrapped = GPState(\n        particle_states=history,\n        num_particles=self.num_particles,\n        num_data_points=self.data.n,\n        mcmc_accepted=accepted_mcmc,\n        hmc_accepted=accepted_hmc,\n    )\n\n    return final_state_wrapped, history_wrapped\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel.fit_smc","title":"<code>fit_smc(key, annealing_schedule, n_mcmc, n_hmc, verbosity=0)</code>","text":"<p>Fits the GP model using SMC.</p> <p>For a detailed description of the SMC algorithm, see the 'gallofrey.inference.smc.smc_loop' function.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key for the SMC sampling.</p> required <code>annealing_schedule</code> <code>tuple[int, ...]</code> <p>The data annealing schedule for the SMC algorithm, number of data points to consider at each step. NOTE: Must be given in form of a tuple of integers, for jax compatibility. Easily generated using the <code>generate</code> method of the <code>Schedule</code> in <code>gallifrey.schedule</code>.</p> required <code>n_mcmc</code> <code>ScalarInt</code> <p>Number of MCMC iterations over kernel structure per SMC step.</p> required <code>n_hmc</code> <code>ScalarInt</code> <p>Number of HMC steps for continuous parameters per SMC step. Only used if the structure move is accepted.</p> required <code>verbosity</code> <code>int</code> <p>The verbosity level, by default 0. Debugging information is printed if <code>verbosity &gt; 1</code>.</p> <code>0</code> <p>Returns:</p> Type Description <code>GPState</code> <p>The final SMC state. Contains the final particle states and the final weights (among other things, see 'gallifrey.inference.state.GPState' for details).</p> <code>GPState</code> <p>The history of the SMC algorithm. Contains the particle states and weights at each step of the algorithm.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def fit_smc(\n    self,\n    key: PRNGKeyArray,\n    annealing_schedule: tuple[int, ...],\n    n_mcmc: ScalarInt,\n    n_hmc: ScalarInt,\n    verbosity: int = 0,\n) -&gt; tuple[GPState, GPState]:\n    \"\"\"\n    Fits the GP model using SMC.\n\n    For a detailed description of the SMC algorithm, see the\n    'gallofrey.inference.smc.smc_loop' function.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key for the SMC sampling.\n    annealing_schedule : tuple[int, ...]\n        The data annealing schedule for the SMC algorithm,\n        number of data points to consider at each step.\n        NOTE: Must be given in form of a tuple of integers, for\n        jax compatibility. Easily generated using the `generate`\n        method of the `Schedule` in `gallifrey.schedule`.\n    n_mcmc : ScalarInt\n        Number of MCMC iterations over kernel structure per\n        SMC step.\n    n_hmc : ScalarInt\n        Number of HMC steps for continuous parameters per\n        SMC step. Only used if the structure move is accepted.\n    verbosity : int, optional\n        The verbosity level, by default 0. Debugging information\n        is printed if `verbosity &gt; 1`.\n\n    Returns\n    -------\n    GPState\n        The final SMC state. Contains the final particle states\n        and the final weights (among other things, see\n        'gallifrey.inference.state.GPState' for details).\n    GPState\n        The history of the SMC algorithm. Contains the particle\n        states and weights at each step of the algorithm.\n\n    \"\"\"\n\n    final_smc_state, history = smc_loop(\n        key,\n        self.state.particle_states,\n        annealing_schedule,\n        int(self.num_particles),\n        self.data,\n        self.kernel_prior,\n        self.noise_prior,\n        self.fix_noise,\n        self.hmc_sampler_factory,\n        n_mcmc,\n        n_hmc,\n        verbosity=verbosity,\n    )\n\n    batched_history: GPState = batch_states(history)\n\n    return final_smc_state, batched_history\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel.get_mixture_distribution","title":"<code>get_mixture_distribution(xpredict, gpstate=None, data=None, log_weights=None, num_particles=None, key=None, latent=False)</code>","text":"<p>Get the mixture distribution of an SMC state.</p> <p>The predictive distributions for an SMC ensemble are the individual predictive (Gaussion) distributions of the particles, weighted by the particle weights. The resulting distribution is a Gaussian mixture model, implemented as a <code>MixtureSameFamily</code> distribution from <code>tensorflow_probability</code>, see https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MixtureSameFamily</p> <p>The mixture distribution is calculated at the points <code>xpredict</code> and conditioned on the training data (which was supplied to construct the model instance).</p> <p>The input should be an GPState object as returned by the <code>fit_smc</code> method. Alternatively, the log weights can be provided explicitly using the <code>log_weights</code> argument.</p> <p>Parameters:</p> Name Type Description Default <code>xpredict</code> <code>Float[Array, ' D']</code> <p>The points to predict, as a 1D array.</p> required <code>gpstate</code> <code>Optional[GPState]</code> <p>The GP state object, containing the particle states and log weights for the mixture distribution. If None, the current state of the model is used. By default None.</p> <code>None</code> <code>data</code> <code>Optional[Dataset]</code> <p>The data to condition the predictive distribution on. If None, the training data of the model is used. By default None.</p> <code>None</code> <code>log_weights</code> <code>Float[Array, ' N']</code> <p>The log weights of the particles. If None, the log weights from the GP state are used. An error is raised if the log weights are not provided and the GP state does not contain log weights. By default None.</p> <code>None</code> <code>num_particles</code> <code>Optional[ScalarInt]</code> <p>Number of particles to include in the mixture distribution. If None, all particles in the state are included. If provided, a random sample of particles is chosen based on the weights. By default None.</p> <code>None</code> <code>key</code> <code>Optional[PRNGKeyArray]</code> <p>Random key for sampling the particles. Required if <code>num_particles</code> is provided. By default None.</p> <code>None</code> <code>latent</code> <code>bool</code> <p>Whether to return the predictive distribution of the latent functions only (without observational noise), by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Distribution</code> <p>A tensorflow probability distribution object representing the Gaussian mixture distribution of the Gaussian processes (a MixtureSameFamily distribution).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the GPState object contains no log weights and <code>log_weights</code> is None.</p> <code>ValueError</code> <p>If the number of particles and log weights are inconsistent.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def get_mixture_distribution(\n    self,\n    xpredict: Float[Array, \" D\"],\n    gpstate: tp.Optional[GPState] = None,\n    data: tp.Optional[Dataset] = None,\n    log_weights: tp.Optional[Float[Array, \" N\"]] = None,\n    num_particles: tp.Optional[ScalarInt] = None,\n    key: tp.Optional[PRNGKeyArray] = None,\n    latent: bool = False,\n) -&gt; Distribution:\n    \"\"\"\n    Get the mixture distribution of an SMC state.\n\n    The predictive distributions for an SMC ensemble are\n    the individual predictive (Gaussion) distributions of the\n    particles, weighted by the particle weights. The resulting\n    distribution is a Gaussian mixture model, implemented as\n    a `MixtureSameFamily` distribution from `tensorflow_probability`, see\n    https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MixtureSameFamily\n\n    The mixture distribution is calculated at the points `xpredict`\n    and conditioned on the training data (which was supplied to construct\n    the model instance).\n\n    The input should be an GPState object as returned by the `fit_smc` method.\n    Alternatively, the log weights can be provided explicitly using the\n    `log_weights` argument.\n\n\n    Parameters\n    ----------\n    xpredict : Float[Array, \" D\"]\n        The points to predict, as a 1D array.\n    gpstate : tp.Optional[GPState], optional\n        The GP state object, containing the particle states and log weights for\n        the mixture distribution. If None, the current state of the model is used.\n        By default None.\n    data : tp.Optional[Dataset], optional\n        The data to condition the predictive distribution on. If None,\n        the training data of the model is used. By default None.\n    log_weights : Float[Array, \" N\"], optional\n        The log weights of the particles. If None, the log weights from the\n        GP state are used. An error is raised if the log weights are not provided\n        and the GP state does not contain log weights. By default None.\n    num_particles : tp.Optional[ScalarInt], optional\n        Number of particles to include in the mixture distribution. If None,\n        all particles in the state are included. If provided, a random sample\n        of particles is chosen based on the weights. By default None.\n    key : tp.Optional[PRNGKeyArray], optional\n        Random key for sampling the particles. Required if `num_particles` is\n        provided. By default None.\n    latent : bool, optional\n        Whether to return the predictive distribution of the latent\n        functions only (without observational noise), by default\n        False.\n\n    Returns\n    -------\n    Distribution\n        A tensorflow probability distribution object representing\n        the Gaussian mixture distribution of the Gaussian processes\n        (a MixtureSameFamily distribution).\n\n    Raises\n    ------\n    ValueError\n        If the GPState object contains no log weights and `log_weights` is None.\n    ValueError\n        If the number of particles and log weights are inconsistent.\n\n    \"\"\"\n    gpstate = self.state if gpstate is None else gpstate\n    data = self.data if data is None else data\n\n    if (gpstate.log_weights is None) and (log_weights is None):\n        raise ValueError(\n            \"The GPState object contains no log weights. This might be \"\n            \"because the state was produced with the MCMC sampler or the \"\n            \"initial state of the GPModel was used. Please either run \"\n            \"`fit_smc` or provide the log weights explicitly. (Note: If \"\n            \"you already ran `fit_smc` and this error occurs, make sure to \"\n            \"passed the the output of `fit_smc` to this method, or run the \"\n            \"`update_state` method with the output of `fit_smc` as input.)\"\n        )\n\n    log_weights = log_weights if log_weights is not None else gpstate.log_weights\n\n    assert log_weights is not None\n    if len(log_weights) != self.num_particles:\n        raise ValueError(\n            f\"Inconsistent number of particles and log weights, \"\n            f\"expected {self.num_particles} but got {len(log_weights)}.\"\n        )\n\n    individual_distributions = self.get_predictive_distributions(\n        xpredict,\n        data,\n        gpstate,\n        latent,\n    )\n\n    if num_particles is not None:\n        if key is None:\n            raise ValueError(\n                \"If `num_particles` is provided, `key` must also be provided. \"\n                \"This is the random key for sampling from the particles.\"\n            )\n        if num_particles &gt; self.num_particles:\n            raise ValueError(\n                f\"Number of particles to sample ({num_particles}) \"\n                f\"exceeds the total number of particles ({self.num_particles}).\"\n            )\n        # choose random sample of particles based on weights\n        weights = jnp.exp(log_weights)\n        particle_indices = jr.choice(\n            key,\n            jnp.arange(len(weights)),\n            p=weights,\n            shape=(int(num_particles),),\n            replace=False,\n        )\n        # normalize weights\n        log_weights = log_weights[particle_indices] - logsumexp(\n            log_weights[particle_indices]\n        )\n        # select particles\n        individual_distributions = [\n            individual_distributions[idx] for idx in particle_indices\n        ]\n\n    batched_distributions = batch_states(individual_distributions)\n\n    mixture_model = MixtureSameFamily(\n        mixture_distribution=Categorical(logits=log_weights),\n        components_distribution=batched_distributions,\n    )\n\n    return mixture_model\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel.get_particles","title":"<code>get_particles(gpstate=None)</code>","text":"<p>Get a list of Particle instances from a GP state.</p> <p>If no state is provided, the current state of the model is used.</p> <p>Parameters:</p> Name Type Description Default <code>gpstate</code> <code>Optional[GPState]</code> <p>The GP state to extract the particles from. If None, the current state of the model is used. By default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Particle]</code> <p>A list of Particle instances, corresponding to the individual states.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def get_particles(\n    self,\n    gpstate: tp.Optional[GPState] = None,\n) -&gt; list[Particle]:\n    \"\"\"\n    Get a list of Particle instances from a GP state.\n\n    If no state is provided, the current state of the model\n    is used.\n\n    Parameters\n    ----------\n    gpstate : tp.Optional[GPState], optional\n        The GP state to extract the particles from. If None,\n        the current state of the model is used. By default None.\n\n    Returns\n    -------\n    list[Particle]\n        A list of Particle instances, corresponding to the\n        individual states.\n    \"\"\"\n    if gpstate is None:\n        gpstate = self.state\n\n    unbatched_particle_states = unbatch_states(gpstate.particle_states)\n    return [\n        nnx.merge(self.particle_graphdef, state)\n        for state in unbatched_particle_states\n    ]\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel.get_predictive_distributions","title":"<code>get_predictive_distributions(xpredict, data=None, gpstate=None, latent=False)</code>","text":"<p>Calculate the predictive distributions for the individual particles in the GP state. The distributions are calculated at the points <code>x_predict</code> and conditioned on the training data (which was supplied to construct the model instance).</p> <p>The distributions are returned as a list of tensorflow probability distribution objects, see https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalFullCovariance</p> <p>If no state is provided, the current state of the model is used.</p> <p>If <code>latent</code> is True, the predictive distribution is that of the latent function, i.e. the distribution of the function values without the observational noise. If False, the predictive distribution of the full data-generating model is returned, which includes the observational noise.</p> <p>Parameters:</p> Name Type Description Default <code>xpredict</code> <code>Float[Array, ' D']</code> <p>The points to predict, as a 1D array.</p> required <code>data</code> <code>Optional[Dataset]</code> <p>The data to condition the predictive distribution on. If None, the training data of the model is used. By default None.</p> <code>None</code> <code>gpstate</code> <code>Optional[GPState]</code> <p>The GP state object, containing the particle states. If None, the current state of the model is used. By default None.</p> <code>None</code> <code>latent</code> <code>bool</code> <p>Whether to return the predictive distribution of the latent functions only (without observational noise), by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Distribution]</code> <p>A list of tensorflow probability distribution objects representing the predictive distributions of the Gaussian processes. (Specifically, a MultivariateNormalFullCovariance distribution from <code>tensorflow_probability.substrates.jax.distributions</code>).</p> Source code in <code>gallifrey/model.py</code> <pre><code>def get_predictive_distributions(\n    self,\n    xpredict: Float[Array, \" D\"],\n    data: tp.Optional[Dataset] = None,\n    gpstate: tp.Optional[GPState] = None,\n    latent: bool = False,\n) -&gt; list[Distribution]:\n    \"\"\"\n    Calculate the predictive distributions for the individual particles\n    in the GP state. The distributions are calculated at the points\n    `x_predict` and conditioned on the training data (which was supplied\n    to construct the model instance).\n\n    The distributions are returned as a list of tensorflow probability\n    distribution objects, see\n    https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalFullCovariance\n\n    If no state is provided, the current state of the model\n    is used.\n\n    If `latent` is True, the predictive distribution is that of the latent\n    function, i.e. the distribution of the function values without the\n    observational noise. If False, the predictive distribution of the full\n    data-generating model is returned, which includes the observational noise.\n\n    Parameters\n    ----------\n    xpredict : Float[Array, \" D\"]\n        The points to predict, as a 1D array.\n    data : tp.Optional[Dataset], optional\n        The data to condition the predictive distribution on. If None,\n        the training data of the model is used. By default None.\n    gpstate : tp.Optional[GPState], optional\n        The GP state object, containing the particle states. If None,\n        the current state of the model is used. By default None.\n    latent : bool, optional\n        Whether to return the predictive distribution of the latent\n        functions only (without observational noise), by default\n        False.\n\n    Returns\n    -------\n    list[Distribution]\n        A list of tensorflow probability distribution objects\n        representing the predictive distributions of the Gaussian\n        processes. (Specifically, a MultivariateNormalFullCovariance\n        distribution from `tensorflow_probability.substrates.jax.distributions`).\n\n    \"\"\"\n    gpstate = self.state if gpstate is None else gpstate\n    data = self.data if data is None else data\n\n    particles = self.get_particles(gpstate)\n    distributions = [\n        particle.predictive_distribution(\n            jnp.atleast_1d(xpredict).squeeze(),\n            data,\n            latent,\n        )\n        for particle in particles\n    ]\n\n    return distributions\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel.load_state","title":"<code>load_state(path)</code>  <code>classmethod</code>","text":"<p>Load a GP state from file.</p> <p>Note that only the state is loaded. It is assumed that the model configuration is consistent with how the state was saved. If the model configuration is different, the loaded state might not be consistent with the model.</p> <p>The model does not get update with the loaded state. To update the model, use the <code>update_gpstate</code> method with the loaded state.</p> <p>TODO: Implement loading the model configuration as well.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PosixPath</code> <p>The path where to load the GP state from.</p> required <p>Returns:</p> Type Description <code>GPState</code> <p>An instance of the GPState object, containing the loaded state.</p> Source code in <code>gallifrey/model.py</code> <pre><code>@classmethod\ndef load_state(\n    cls,\n    path: str | PosixPath,\n) -&gt; GPState:\n    \"\"\"\n    Load a GP state from file.\n\n    Note that only the state is loaded. It is assumed that the model\n    configuration is consistent with how the state was saved. If the\n    model configuration is different, the loaded state might not be\n    consistent with the model.\n\n    The model does not get update with the loaded state. To update the\n    model, use the `update_gpstate` method with the loaded state.\n\n    TODO: Implement loading the model configuration as well.\n\n    Parameters\n    ----------\n    path : str | PosixPath\n        The path where to load the GP state from.\n\n    Returns\n    -------\n    GPState\n        An instance of the GPState object, containing the loaded state.\n\n    \"\"\"\n    with open(path, \"rb\") as file:\n        gpstate = pickle.load(file)\n    return gpstate\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel.save_state","title":"<code>save_state(path, gpstate=None)</code>","text":"<p>Save a GP state to file.</p> <p>Note that only the state is saved and not the model itself. That means if the model is loaded with a different configuration (e.g. different kernel library), the state might not be consistent with the model.</p> <p>TODO: Implement saving the model configuration as well.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PosixPath</code> <p>The path where to save the GP state, must be an absolute path.</p> required <code>gpstate</code> <code>Optional[GPState]</code> <p>The GP state to save, If None, the current state of the model is used. By default None.</p> <code>None</code> Source code in <code>gallifrey/model.py</code> <pre><code>def save_state(\n    self,\n    path: str | PosixPath,\n    gpstate: tp.Optional[GPState] = None,\n) -&gt; None:\n    \"\"\"\n    Save a GP state to file.\n\n    Note that only the state is saved and not the model itself.\n    That means if the model is loaded with a different configuration\n    (e.g. different kernel library), the state might not be consistent\n    with the model.\n\n    TODO: Implement saving the model configuration as well.\n\n    Parameters\n    ----------\n    path : str | PosixPath\n        The path where to save the GP state, must\n        be an absolute path.\n    gpstate : tp.Optional[GPState], optional\n        The GP state to save, If None, the current state\n        of the model is used. By default None.\n\n    \"\"\"\n    if gpstate is None:\n        gpstate = self.state\n\n    with open(path, \"wb\") as file:\n        pickle.dump(gpstate, file)\n\n    return None\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.GPModel.update_state","title":"<code>update_state(gpstate)</code>","text":"<p>Update the GP state. Returns a new GPModel instance (no in-place update). If the number of particles in the new state is different from the current state, the num_particles attribute is updated.</p> <p>Note that no other attributes are updated. If the particle states were created using a different configuration, the model will not be consistent.</p> <p>Parameters:</p> Name Type Description Default <code>gpstate</code> <code>GPState</code> <p>The new GP state to update the model with.</p> required <p>Returns:</p> Type Description <code>GPModel</code> <p>A new GPModel instance with updated state.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def update_state(self, gpstate: GPState) -&gt; GPModel:\n    \"\"\"\n    Update the GP state. Returns a new GPModel instance (no in-place\n    update). If the number of particles in the new state is different\n    from the current state, the num_particles attribute is updated.\n\n    Note that no other attributes are updated. If the particle states\n    were created using a different configuration, the model will not be\n    consistent.\n\n    Parameters\n    ----------\n    gpstate : GPState\n        The new GP state to update the model with.\n\n    Returns\n    -------\n    GPModel\n        A new GPModel instance with updated state.\n\n    \"\"\"\n\n    new_gpmodel = deepcopy(self)\n    new_gpmodel.state = gpstate\n\n    return new_gpmodel\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.LinearSchedule","title":"<code>LinearSchedule</code>","text":"<p>               Bases: <code>Schedule</code></p> <p>Linear scheduler, adds roughly <code>n * percent</code> new observations at each step.</p> Source code in <code>gallifrey/schedule.py</code> <pre><code>class LinearSchedule(Schedule):\n    \"\"\"\n    Linear scheduler, adds roughly\n    `n * percent` new observations at each step.\n\n    \"\"\"\n\n    @staticmethod\n    def generate(\n        num_datapoints: int,\n        num_steps: int,\n        start: int = 1,\n    ) -&gt; tuple[int, ...]:\n        \"\"\"\n        Generate a linear annealing schedule.\n\n        Parameters\n        ----------\n        num_datapoints : int\n            The total number of datapoints.\n        num_steps : int\n            The number of steps in the schedule.\n        start : int, optional\n            The starting point of the schedule,\n            by default 1 (one observation).\n\n        Returns\n        -------\n        tuple[int, ...]\n            A tuple of integers representing the cumulative number of\n            observations at each step of the schedule. Length of the\n            tuple is `num_steps`.\n\n        \"\"\"\n        # if only one step, run total number of datapoints\n        if num_steps == 1:\n            start = num_datapoints\n\n        return tuple(\n            jnp.round(\n                jnp.linspace(\n                    start=start,\n                    stop=num_datapoints,\n                    num=num_steps,\n                    endpoint=True,\n                    dtype=float,\n                ),\n            )\n            .astype(int)\n            .tolist()\n        )\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.LinearSchedule.generate","title":"<code>generate(num_datapoints, num_steps, start=1)</code>  <code>staticmethod</code>","text":"<p>Generate a linear annealing schedule.</p> <p>Parameters:</p> Name Type Description Default <code>num_datapoints</code> <code>int</code> <p>The total number of datapoints.</p> required <code>num_steps</code> <code>int</code> <p>The number of steps in the schedule.</p> required <code>start</code> <code>int</code> <p>The starting point of the schedule, by default 1 (one observation).</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>A tuple of integers representing the cumulative number of observations at each step of the schedule. Length of the tuple is <code>num_steps</code>.</p> Source code in <code>gallifrey/schedule.py</code> <pre><code>@staticmethod\ndef generate(\n    num_datapoints: int,\n    num_steps: int,\n    start: int = 1,\n) -&gt; tuple[int, ...]:\n    \"\"\"\n    Generate a linear annealing schedule.\n\n    Parameters\n    ----------\n    num_datapoints : int\n        The total number of datapoints.\n    num_steps : int\n        The number of steps in the schedule.\n    start : int, optional\n        The starting point of the schedule,\n        by default 1 (one observation).\n\n    Returns\n    -------\n    tuple[int, ...]\n        A tuple of integers representing the cumulative number of\n        observations at each step of the schedule. Length of the\n        tuple is `num_steps`.\n\n    \"\"\"\n    # if only one step, run total number of datapoints\n    if num_steps == 1:\n        start = num_datapoints\n\n    return tuple(\n        jnp.round(\n            jnp.linspace(\n                start=start,\n                stop=num_datapoints,\n                num=num_steps,\n                endpoint=True,\n                dtype=float,\n            ),\n        )\n        .astype(int)\n        .tolist()\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.LogSchedule","title":"<code>LogSchedule</code>","text":"<p>               Bases: <code>Schedule</code></p> <p>A logarithmic scheduler, adds observations in a logarithmic fashion.</p> Source code in <code>gallifrey/schedule.py</code> <pre><code>class LogSchedule(Schedule):\n    \"\"\"\n    A logarithmic scheduler, adds observations in a logarithmic fashion.\n\n    \"\"\"\n\n    @staticmethod\n    def generate(\n        num_datapoints: int,\n        num_steps: int,\n        start: int = 1,\n    ) -&gt; tuple[int, ...]:\n        \"\"\"\n        Generate a logarithmic annealing schedule.\n\n        Parameters\n        ----------\n        num_datapoints : int\n            The total number of datapoints.\n        num_steps : int\n            The number of steps in the schedule.\n        start : int, optional\n            The starting point of the schedule,\n            by default 1 (one observation).\n\n        Returns\n        -------\n        tuple[int, ...]\n            A tuple of integers representing the cumulative number of\n            observations at each step of the schedule. Length of the\n            tuple is `num_steps`.\n        \"\"\"\n        # if only one step, run total number of datapoints\n        if num_steps == 1:\n            start = num_datapoints\n\n        return tuple(\n            jnp.round(\n                jnp.geomspace(\n                    start=start,\n                    stop=num_datapoints,\n                    num=num_steps,\n                    endpoint=True,\n                )\n            )\n            .astype(int)\n            .tolist()\n        )\n</code></pre>"},{"location":"autoapi/gallifrey/#gallifrey.LogSchedule.generate","title":"<code>generate(num_datapoints, num_steps, start=1)</code>  <code>staticmethod</code>","text":"<p>Generate a logarithmic annealing schedule.</p> <p>Parameters:</p> Name Type Description Default <code>num_datapoints</code> <code>int</code> <p>The total number of datapoints.</p> required <code>num_steps</code> <code>int</code> <p>The number of steps in the schedule.</p> required <code>start</code> <code>int</code> <p>The starting point of the schedule, by default 1 (one observation).</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>A tuple of integers representing the cumulative number of observations at each step of the schedule. Length of the tuple is <code>num_steps</code>.</p> Source code in <code>gallifrey/schedule.py</code> <pre><code>@staticmethod\ndef generate(\n    num_datapoints: int,\n    num_steps: int,\n    start: int = 1,\n) -&gt; tuple[int, ...]:\n    \"\"\"\n    Generate a logarithmic annealing schedule.\n\n    Parameters\n    ----------\n    num_datapoints : int\n        The total number of datapoints.\n    num_steps : int\n        The number of steps in the schedule.\n    start : int, optional\n        The starting point of the schedule,\n        by default 1 (one observation).\n\n    Returns\n    -------\n    tuple[int, ...]\n        A tuple of integers representing the cumulative number of\n        observations at each step of the schedule. Length of the\n        tuple is `num_steps`.\n    \"\"\"\n    # if only one step, run total number of datapoints\n    if num_steps == 1:\n        start = num_datapoints\n\n    return tuple(\n        jnp.round(\n            jnp.geomspace(\n                start=start,\n                stop=num_datapoints,\n                num=num_steps,\n                endpoint=True,\n            )\n        )\n        .astype(int)\n        .tolist()\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/data/","title":"data","text":""},{"location":"autoapi/gallifrey/data/#gallifrey.data.Dataset","title":"<code>Dataset</code>","text":"<p>A class to hold the data.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>Float[ndarray, ' D']</code> <p>The input x data (1D).</p> <code>y</code> <code>Float[ndarray, ' D']</code> <p>The input y data (1D).</p> <code>n</code> <code>int</code> <p>The number of data points.</p> Source code in <code>gallifrey/data.py</code> <pre><code>class Dataset:\n    \"\"\"\n    A class to hold the data.\n\n    Attributes\n    ----------\n    x : Float[jnp.ndarray, \" D\"]\n        The input x data (1D).\n    y : Float[jnp.ndarray, \" D\"]\n        The input y data (1D).\n    n : int\n        The number of data points.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        x: Float[jnp.ndarray, \" D\"],\n        y: Float[jnp.ndarray, \" D\"],\n    ):\n        \"\"\"\n        Initialize the Dataset instance.\n\n        Parameters\n        ----------\n        x : Float[jnp.ndarray, \" D\"]\n            The input x data.\n        y : Float[jnp.ndarray, \" D\"]\n            The input y data.\n        \"\"\"\n        self.x = jnp.asarray(x)\n        self.y = jnp.asarray(y)\n\n        self._validate_input()\n\n    @property\n    def n(self) -&gt; int:\n        \"\"\"\n        Get the number of data points.\n\n        Returns\n        -------\n        int\n            The number of data points.\n        \"\"\"\n        return self.x.shape[0]\n\n    def _validate_input(self) -&gt; None:\n        \"\"\"\n        Check that the number of x and y values are the same.\n\n        Raises\n        ------\n        ValueError\n            If the number of x and y values are not the same.\n        \"\"\"\n\n        if self.x.shape[0] != self.y.shape[0]:\n            raise ValueError(\"The length of x and y must be the same.\")\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return the string representation of the Dataset instance.\n\n        Returns\n        -------\n        str\n            The representation of the Dataset instance.\n        \"\"\"\n        return f\"Dataset(n={self.n},\\n x={self.x},\\n y={self.y})\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Simplified string representation of the Dataset instance.\n\n        Returns\n        -------\n        str\n            The simplified string representation.\n        \"\"\"\n        return f\"Dataset(n={self.n})\"\n</code></pre>"},{"location":"autoapi/gallifrey/data/#gallifrey.data.Dataset.n","title":"<code>n</code>  <code>property</code>","text":"<p>Get the number of data points.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of data points.</p>"},{"location":"autoapi/gallifrey/data/#gallifrey.data.Dataset.__init__","title":"<code>__init__(x, y)</code>","text":"<p>Initialize the Dataset instance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[ndarray, ' D']</code> <p>The input x data.</p> required <code>y</code> <code>Float[ndarray, ' D']</code> <p>The input y data.</p> required Source code in <code>gallifrey/data.py</code> <pre><code>def __init__(\n    self,\n    x: Float[jnp.ndarray, \" D\"],\n    y: Float[jnp.ndarray, \" D\"],\n):\n    \"\"\"\n    Initialize the Dataset instance.\n\n    Parameters\n    ----------\n    x : Float[jnp.ndarray, \" D\"]\n        The input x data.\n    y : Float[jnp.ndarray, \" D\"]\n        The input y data.\n    \"\"\"\n    self.x = jnp.asarray(x)\n    self.y = jnp.asarray(y)\n\n    self._validate_input()\n</code></pre>"},{"location":"autoapi/gallifrey/data/#gallifrey.data.Dataset.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Dataset instance.</p> <p>Returns:</p> Type Description <code>str</code> <p>The representation of the Dataset instance.</p> Source code in <code>gallifrey/data.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return the string representation of the Dataset instance.\n\n    Returns\n    -------\n    str\n        The representation of the Dataset instance.\n    \"\"\"\n    return f\"Dataset(n={self.n},\\n x={self.x},\\n y={self.y})\"\n</code></pre>"},{"location":"autoapi/gallifrey/data/#gallifrey.data.Dataset.__str__","title":"<code>__str__()</code>","text":"<p>Simplified string representation of the Dataset instance.</p> <p>Returns:</p> Type Description <code>str</code> <p>The simplified string representation.</p> Source code in <code>gallifrey/data.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Simplified string representation of the Dataset instance.\n\n    Returns\n    -------\n    str\n        The simplified string representation.\n    \"\"\"\n    return f\"Dataset(n={self.n})\"\n</code></pre>"},{"location":"autoapi/gallifrey/gpconfig/","title":"gpconfig","text":""},{"location":"autoapi/gallifrey/gpconfig/#gallifrey.gpconfig.GPConfig","title":"<code>GPConfig</code>","text":"<p>Config for the GP model.</p> <p>Attributes:</p> Name Type Description <code>max_depth</code> <code>int</code> <p>Maximum depth of the kernel tree. By default, 3.</p> <code>atoms</code> <code>list[AbstractAtom]</code> <p>List of atomic kernels to consider in the kernel tree. By default, the following kernels are included: - Linear - Periodic - RBF</p> <code>operators</code> <code>list[AbstractOperator]</code> <p>List of kernel operators to consider in the kernel tree. The operators are used to combine the atomic kernels (i.e., functions) in the kernel structure. By default, the following operators are included: - SumOperator - ProductOperator</p> <code>node_probabilities</code> <code>Float[ndarray, ' D']</code> <p>Probabilities for sampling the kernels and operators. This array should have a length equal to the sum of the number of kernels and operators. The first part of the array should contain the probabilities for sampling the kernels (in the order of them being listed in the <code>atoms</code> attribute), and the second part should contain the probabilities for sampling the operators (in the order of them being listed in the <code>operators</code> attribute). By default, the probabilities are set to be equal for all kernels, and half that probability for the operators (to encourage kernels with fewer terms)</p> <code>prior_transforms</code> <code>dict[str, Transformation]</code> <p>Dictionary containing the bijectors for transforming the kernel parameters to the prior distribution. Originally, the parameters are sampled from a standard normal distribution, and these bijectors transform the samples to the desired prior distribution. The keys are inherited from GPJax, and describe the domain of the kernel parameters. The values are the bijectors that transform the standard normal samples to the desired prior distribution. The bijection transformation should be implemented via TensorFlow Probability bijectors. By default, the following bijectors are included: - \"real\": Log-normal transform (normal -&gt; log-normal) - \"positive\": Log-normal transform (normal -&gt; log-normal) - \"sigmoid\": Logit-normal transform (normal -&gt; logit-normal) NOTE: The \"none\" key is reserved for internal use and should not be used.</p> <code>hmc_config</code> <code>dict[str, float]</code> <p>Configuration for the HMC sampler. The dictionary should contain the following keys: - \"step_size\": The step size of the HMC sampler. By default, 0.02. - \"inv_mass_matrix_scaling\": The scaling factor for the inverse mass matrix.     By default, 1.0. - \"num_integration_steps\": The number of integration steps for the HMC sampler.     By default, 10.</p> <code>noise_prior</code> <code>Distribution</code> <p>An instance of tensorflow_probability.substrates.jax.distributions.Distribution that samples the noise variance from the prior distribution. Must have methods \"sample\" and \"log_prob\". The default is an InverseGamma(1,1) distribution. NOTE: The noise variance prior is currently fixed to the InverseGamma(1,1) distribution, and cannot be changed. This is because the InverseGamma(1,1) distribution is needed to caluculate the Monte Carlo acceptance probabilities analytically. Other distributions are not currently supported.</p> <code>mean_function</code> <code>AbstractMeanFunction</code> <p>The mean function of the GP model. By default, a zero mean function is used. The constant is explicitly set to 0.0, (and is not a ParticleParameter from the gallifrey.parameter class, so it is not trainable). NOTE: Non-zero mean functions are not currently implemented, do not change this attribute.</p> Source code in <code>gallifrey/gpconfig.py</code> <pre><code>@struct.dataclass\nclass GPConfig:\n    \"\"\"\n    Config for the GP model.\n\n    Attributes\n    ----------\n\n    max_depth : int\n        Maximum depth of the kernel tree. By default, 3.\n\n    atoms : list[AbstractAtom]\n        List of atomic kernels to consider in the kernel tree.\n        By default, the following kernels are included:\n        - Linear\n        - Periodic\n        - RBF\n\n    operators : list[AbstractOperator]\n        List of kernel operators to consider in the kernel tree.\n        The operators are used to combine the atomic kernels (i.e., functions)\n        in the kernel structure. By default, the following operators are included:\n        - SumOperator\n        - ProductOperator\n\n    node_probabilities :  Float[jnp.ndarray, \" D\"]\n        Probabilities for sampling the kernels and operators. This array\n        should have a length equal to the sum of the number of kernels\n        and operators. The first part of the array should contain the\n        probabilities for sampling the kernels (in the order of them being listed\n        in the `atoms` attribute), and the second part should contain the\n        probabilities for sampling the operators (in the order of them being listed\n        in the `operators` attribute).\n        By default, the probabilities are set to be equal for all kernels, and\n        half that probability for the operators (to encourage kernels with\n        fewer terms)\n\n    prior_transforms : dict[str, Transformation]\n        Dictionary containing the bijectors for transforming the kernel parameters\n        to the prior distribution. Originally, the parameters are sampled from a\n        standard normal distribution, and these bijectors transform the samples to\n        the desired prior distribution. The keys are inherited from GPJax, and describe\n        the domain of the kernel parameters. The values are the bijectors that\n        transform the standard normal samples to the desired prior distribution. The\n        bijection transformation should be implemented via TensorFlow Probability\n        bijectors.\n        By default, the following bijectors are included:\n        - \"real\": Log-normal transform (normal -&gt; log-normal)\n        - \"positive\": Log-normal transform (normal -&gt; log-normal)\n        - \"sigmoid\": Logit-normal transform (normal -&gt; logit-normal)\n        NOTE: The \"none\" key is reserved for internal use and should not be used.\n\n    hmc_config : dict[str, float]\n        Configuration for the HMC sampler. The dictionary should contain the following\n        keys:\n        - \"step_size\": The step size of the HMC sampler. By default, 0.02.\n        - \"inv_mass_matrix_scaling\": The scaling factor for the inverse mass matrix.\n            By default, 1.0.\n        - \"num_integration_steps\": The number of integration steps for the HMC sampler.\n            By default, 10.\n\n    noise_prior : Distribution\n        An instance of tensorflow_probability.substrates.jax.distributions.Distribution\n        that samples the noise variance from the prior distribution. Must have methods\n        \"sample\" and \"log_prob\". The default is an InverseGamma(1,1) distribution.\n        NOTE: The noise variance prior is currently fixed to the InverseGamma(1,1)\n        distribution, and cannot be changed. This is because the InverseGamma(1,1)\n        distribution is needed to caluculate the Monte Carlo acceptance probabilities\n        analytically. Other distributions are not currently supported.\n\n    mean_function : AbstractMeanFunction\n        The mean function of the GP model. By default, a zero mean function is used.\n        The constant is explicitly set to 0.0, (and is not a ParticleParameter from\n        the gallifrey.parameter class, so it is not trainable).\n        NOTE: Non-zero mean functions are not currently implemented, do not change\n        this attribute.\n\n    \"\"\"\n\n    max_depth: int = 3\n\n    atoms: list[AbstractAtom] = field(\n        default_factory=lambda: [\n            # ConstantAtom(),\n            LinearAtom(),\n            # LinearWithShiftAtom(),\n            PeriodicAtom(),\n            # Matern12Atom(),\n            # Matern32Atom(),\n            # Matern52Atom(),\n            RBFAtom(),\n            # PoweredExponentialAtom(),\n            # RationalQuadraticAtom(),\n            # WhiteAtom(),\n        ]\n    )\n\n    operators: list[AbstractOperator] = field(\n        default_factory=lambda: [\n            SumOperator(),\n            ProductOperator(),\n        ]\n    )\n\n    node_probabilities: Float[jnp.ndarray, \" D\"] = field(\n        default_factory=lambda: jnp.array(\n            [\n                # 1.0,  # Constant\n                1.0,  # Linear\n                # 1.0, # LinearWithShift\n                1.0,  # Periodic\n                # 1.0, # Matern12\n                # 1.0, # Matern32\n                # 1.0, # Matern52\n                1.0,  # RBF\n                # 1.0, # PoweredExponential\n                # 1.0, # RationalQuadratic\n                # 1.0, # White\n                0.5,  # SumOperator\n                0.5,  # ProductOperator\n            ]\n        )\n    )\n\n    prior_transforms: dict[str, tfb.Bijector] = field(\n        default_factory=lambda: dict(\n            {\n                # this is the transformation y = exp(mu + sigma * z),\n                # with mu = 0 and sigma = 1,\n                # if z ~ normal(0, 1) then y ~ log-normal(mu, sigma)\n                \"real\": tfb.Chain(\n                    [\n                        tfb.Exp(),\n                        tfb.Shift(jnp.array(0.0)),\n                        tfb.Scale(jnp.array(1.0)),\n                    ]\n                ),\n                \"positive\": tfb.Chain(\n                    [\n                        tfb.Exp(),\n                        tfb.Shift(jnp.array(0.0)),\n                        tfb.Scale(jnp.array(1.0)),\n                    ]\n                ),\n                # this is the transformation y = 1/(1 + exp(-(mu + sigma * z))),\n                # with mu = 0 and sigma = 1,\n                # if z ~ normal(0, 1) then y ~ logit-normal(mu, sigma)\n                \"sigmoid\": tfb.Chain(\n                    [\n                        tfb.Sigmoid(\n                            low=jnp.array(0.0),\n                            high=jnp.array(\n                                0.95  # slightly below 1 to avoid numerical issues\n                            ),\n                        ),\n                        tfb.Shift(jnp.array(0.0)),\n                        tfb.Scale(jnp.array(1.0)),\n                    ]\n                ),\n            }\n        )\n    )\n\n    hmc_config: dict[str, float] = field(\n        default_factory=lambda: {\n            \"step_size\": 0.02,\n            \"inv_mass_matrix_scaling\": 1.0,\n            \"num_integration_steps\": 10,\n        }\n    )\n\n    @property\n    def noise_prior(self) -&gt; Distribution:\n        \"\"\"\n        The distribution of the noise variance prior,\n        specifically an InverseGamma(1,1) distribution.\n\n        Returns\n        -------\n        Distribution\n            The noise variance prior distribution.\n        \"\"\"\n        return InverseGamma(jnp.array(1.0), jnp.array(1.0))\n</code></pre>"},{"location":"autoapi/gallifrey/gpconfig/#gallifrey.gpconfig.GPConfig.noise_prior","title":"<code>noise_prior</code>  <code>property</code>","text":"<p>The distribution of the noise variance prior, specifically an InverseGamma(1,1) distribution.</p> <p>Returns:</p> Type Description <code>Distribution</code> <p>The noise variance prior distribution.</p>"},{"location":"autoapi/gallifrey/model/","title":"model","text":""},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel","title":"<code>GPModel</code>","text":"<p>The GP model class.</p> <p>Attributes:</p> Name Type Description <code>num_particles</code> <code>ScalarInt</code> <p>Number of particles in the model.</p> <code>config</code> <code>GPConfig</code> <p>The config instance for the GP model (see gallifrey.config.GPConfig for details).</p> <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior object, containing various useful methods and attributes for the kernel sampling. Attributes set through config. (See gallifrey.kernels.prior.KernelPrior for details.)</p> <code>noise_prior</code> <code>Distribution</code> <p>The prior distribution for the noise variance, inherited from the config. (See gallifrey.config.GPConfig for details.)</p> <code>kernel_library</code> <code>KernelLibrary</code> <p>The kernel library, containing the atomic kernels, operators, and prior transforms. (See gallifrey.kernels.library.KernelLibrary for details.)</p> <code>x</code> <code>Float[ndarray, ' D']</code> <p>The input x data.</p> <code>y</code> <code>Float[ndarray, ' D']</code> <p>The input y data.</p> <code>noise_variance</code> <p>The initial noise variance if fixed, otherwise None.</p> <code>fix_noise</code> <code>bool</code> <p>Flag indicating whether the noise variance is fixed or learned. This is set to True if the noise variance is provided as an input parameter.</p> <code>x_transform</code> <code>Optional[Callable]</code> <p>A (optional) transformation applied to the input x data.</p> <code>y_transform</code> <code>Optional[Callable]</code> <p>A (optional) transformation applied to the input y data.</p> <code>x_transformed</code> <code>Float[ndarray, ' D']</code> <p>The transformed input x data.</p> <code>y_transformed</code> <code>Float[ndarray, ' D']</code> <p>The transformed noise variance if fixed, otherwise None.</p> <code>noise_variance_transformed</code> <code>Float[ndarray, ' D']</code> <p>The transformed noise variance, if noise variance is provided.</p> <code>dataset</code> <code>Dataset</code> <p>A dataset instance containing the transformed x and y data, processed for the sampler.</p> <code>particle_graphdef</code> <code>GraphDef</code> <p>Graph definition for the Particle object, shared across all particles. This can be used to reconstruct the Particle object from a particle state.</p> <code>state</code> <code>GPState</code> <p>The state of the GP model, containing the particle states and other relevant information. (See gallifrey.inference.state.GPState for details.)</p> <code>hmc_sampler_factory</code> <code>Callable</code> <p>Factory function to create HMC samplers (using blackjax), configured based on <code>config.hmc_config</code></p> <p>Methods:</p> Name Description <code>update_state</code> <p>Update the internal gpstate of the model using a new GPState object.</p> <code>save_state</code> <p>Save a GP state to file. Note that currently only the state is saved and not the model itself. That means if the model is loaded with a different configuration (e.g. different kernel library), the state might not be consistent with the model.</p> <code>load_state</code> <p>Load a GP state from file. Note that currently only the state is loaded and not the model itself. That means if the model is loaded with a different configuration (e.g. different kernel library), the state might not be consistent with the model.</p> <code>fit_mcmc</code> <p>Fit the GP model using MCMC.</p> <code>fit_smc</code> <p>Fit the GP model using SMC.</p> <code>get_particles</code> <p>Get a list of Particle instances from a GP state. If no GP state is provided, the current GP state of the model is used.</p> <code>get_predictive_distributions</code> <p>Calculate the predictive distributions for the individual particles in the GP state. The distributions are calculated at the points <code>x_predict</code> and conditioned on the training data (which was supplied to construct the model instance).</p> <code>get_mixture_distribution</code> <p>Get the mixture distribution of an SMC state (A weighted sum of the predictive distributions of the individual particles). The model should be created using the <code>fit_smc</code> method.</p> <code>display</code> <p>Print a summary of the GP model, including particle kernels and noise variances.</p> Source code in <code>gallifrey/model.py</code> <pre><code>class GPModel:\n    \"\"\"\n    The GP model class.\n\n    Attributes\n    ----------\n\n    num_particles : ScalarInt\n        Number of particles in the model.\n    config : GPConfig\n        The config instance for the GP model (see\n        gallifrey.config.GPConfig for details).\n    kernel_prior : KernelPrior\n        The kernel prior object, containing various\n        useful methods and attributes for the kernel\n        sampling. Attributes set through config. (See\n        gallifrey.kernels.prior.KernelPrior for details.)\n    noise_prior : Distribution\n        The prior distribution for the noise variance, inherited\n        from the config. (See gallifrey.config.GPConfig for details.)\n    kernel_library : KernelLibrary\n        The kernel library, containing the atomic kernels, operators,\n        and prior transforms. (See gallifrey.kernels.library.KernelLibrary\n        for details.)\n    x :  Float[jnp.ndarray, \" D\"]\n        The input x data.\n    y :  Float[jnp.ndarray, \" D\"]\n        The input y data.\n    noise_variance:\n        The initial noise variance if fixed, otherwise None.\n    fix_noise : bool\n        Flag indicating whether the noise variance is fixed or learned.\n        This is set to True if the noise variance is provided\n        as an input parameter.\n    x_transform : Optional[Callable]\n        A (optional) transformation applied to the input x data.\n    y_transform : Optional[Callable]\n        A (optional) transformation applied to the input y data.\n    x_transformed :  Float[jnp.ndarray, \" D\"]\n        The transformed input x data.\n    y_transformed :  Float[jnp.ndarray, \" D\"]\n        The transformed noise variance if fixed, otherwise None.\n    noise_variance_transformed :  Float[jnp.ndarray, \" D\"]\n        The transformed noise variance, if noise variance is provided.\n    dataset : Dataset\n        A dataset instance containing the transformed x and y data, processed\n        for the sampler.\n    particle_graphdef : nnx.GraphDef\n        Graph definition for the Particle object, shared across all particles.\n        This can be used to reconstruct the Particle object from a particle state.\n    state : GPState\n        The state of the GP model, containing the particle states and\n        other relevant information. (See gallifrey.inference.state.GPState\n        for details.)\n    hmc_sampler_factory : Callable\n        Factory function to create HMC samplers (using blackjax), configured based on\n        `config.hmc_config`\n\n    Methods\n    -------\n    update_state\n        Update the internal gpstate of the model using a new GPState object.\n    save_state\n        Save a GP state to file. Note that currently only the state is saved\n        and not the model itself. That means if the model is loaded with a different\n        configuration (e.g. different kernel library), the state might not be\n        consistent with the model.\n    load_state\n        Load a GP state from file. Note that currently only the state is loaded\n        and not the model itself. That means if the model is loaded with a different\n        configuration (e.g. different kernel library), the state might not be\n        consistent with the model.\n    fit_mcmc\n        Fit the GP model using MCMC.\n    fit_smc\n        Fit the GP model using SMC.\n    get_particles\n        Get a list of Particle instances from a GP state. If no GP state\n        is provided, the current GP state of the model is used.\n    get_predictive_distributions\n        Calculate the predictive distributions for the individual particles in the\n        GP state. The distributions are calculated at the points `x_predict` and\n        conditioned on the training data (which was supplied to construct the model\n        instance).\n    get_mixture_distribution\n        Get the mixture distribution of an SMC state (A weighted sum of the predictive\n        distributions of the individual particles). The model should be created using\n        the `fit_smc` method.\n    display\n        Print a summary of the GP model, including particle kernels and noise variances.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        key: PRNGKeyArray,\n        x: Float[Array, \" D\"],\n        y: Float[Array, \" D\"],\n        num_particles: ScalarInt,\n        noise_variance: tp.Optional[ScalarFloat] = None,\n        x_transform: tp.Type[Transform] = LinearTransform,\n        y_transform: tp.Type[Transform] = LinearTransform,\n        config: tp.Optional[GPConfig] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the GP model.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            The random key for the initial sampling.\n        x : Float[Array, \" D\"]\n            Input data, array of shape (D,).\n        y : Float[Array, \" D\"]\n            Target data, array of shape (D,).\n        num_particles : ScalarInt\n            The number of particles in the model.\n        noise_variance : tp.Optional[ScalarFloat], optional\n            The variance of the observation noise. If\n            None, the noise variance is sampled and treated\n            as a trainable parameter. By default None.\n            NOTE: Currently heteroscadastic noise is not\n            supported.\n        x_transform : Transform, optional\n            A transformation applied to the input x data, must\n            be an instance of gallifrey.inference.transforms.Transform\n            class. By default LinearTransform. (Used to normalise\n            data for easier training.)\n        y_transform : Optional[Callable], optional\n            A transformation applied to the input y data, must\n            be an instance of gallifrey.inference.transforms.Transform\n            class. By default LinearTransform. (Used to normalise\n            data for easier training.)\n        config : tp.Optional[GPConfig], optional\n            The configuration object for the GP model. Contains\n            information of the kernel and parameter priors, the\n            mean function, the max depth of the tree kernel, etc.\n            By default None, in which case the default configuration\n            is used (see gallifrey.config.GPConfig for details).\n        \"\"\"\n        if len(x) != len(y):\n            raise ValueError(\n                f\"Input data x and y must have the same length, \"\n                \"but got len(x)={len(x)} and len(y)={len(y)}.\"\n            )\n\n        # set basic attributes\n        self.config = config if config is not None else GPConfig()\n        self.fix_noise = True if noise_variance is not None else False\n\n        # create kernel prior\n        kernel_library = KernelLibrary(\n            atoms=deepcopy(self.config.atoms),\n            operators=deepcopy(self.config.operators),\n            prior_transforms=deepcopy(self.config.prior_transforms),\n        )\n\n        self.kernel_prior = KernelPrior(\n            kernel_library,\n            max_depth=deepcopy(self.config.max_depth),\n            num_datapoints=len(x),\n            probs=deepcopy(self.config.node_probabilities),\n        )\n\n        # preprocess data (apply transformations, and set attributes)\n        self._preprocess_data(\n            x,\n            y,\n            x_transform,\n            y_transform,\n            noise_variance,\n        )\n\n        # create a particle_graphdef attributes (using a randomly\n        # initilized particle state). The graphdef attribute,\n        # is used to create particle instances from the particle states\n        self.particle_graphdef, particle_state = initialize_particle_state(\n            self.kernel_prior.sample(jr.PRNGKey(0)),\n            self.kernel_prior,\n            self._sample_noise_variance(jr.PRNGKey(0)),\n            self.fix_noise,\n        )\n\n        # sample initial particles and create initial GP state\n        key, particle_key = jr.split(key)\n        self._initialize_state(particle_key, num_particles)\n\n        # create HMC sampler factory for rejuvenation\n        self.hmc_sampler_factory = create_hmc_sampler_factory(\n            self.config.hmc_config,\n            int(self.kernel_prior.max_kernel_parameter + (not self.fix_noise)),\n        )\n\n    def _preprocess_data(\n        self,\n        x: Float[Array, \" D\"],\n        y: Float[Array, \" D\"],\n        x_transform: tp.Type[Transform],\n        y_transform: tp.Type[Transform],\n        noise_variance: tp.Optional[ScalarFloat],\n    ) -&gt; None:\n        \"\"\"\n        Preprocesses the input data by applying transformations\n        and creating a Dataset object.\n\n        Applies the specified transformations (`x_transform` and `y_transform`)\n        to the input data `x` and `y`. If a fixed `noise_variance` is provided,\n        it is also transformed using `y_transform`.\n        Finally, it creates a `Dataset` object with the transformed\n        data, which is used for GPJax computations.\n\n\n        Parameters\n        ----------\n        x :  Float[jnp.ndarray, \" D\"]\n            The input x data.\n        y :  Float[jnp.ndarray, \" D\"]\n            The input y data.\n        x_transform : tp.Type[Transform]\n            A transformation applied to the input x data.\n        y_transform : tp.Type[Transform]\n            A transformation applied to the input y data.\n        noise_variance : tp.Optional[ScalarFloat]\n            The variance of the observation noise.\n        \"\"\"\n        # set attributes\n        self.x = jnp.asarray(x)\n        self.y = jnp.asarray(y)\n\n        # create transformation from data and apply to x and y\n        self.x_transform = x_transform.from_data_range(self.x, 0, 1)\n        self.y_transform = y_transform.from_data_width(self.y, 1)\n\n        self.x_transformed = self.x_transform.apply(self.x)\n        self.y_transformed = self.y_transform.apply(self.y)\n\n        # transform noise variance if provided\n        if noise_variance is not None:\n            self.noise_variance: ScalarFloat | None = jnp.asarray(noise_variance)\n            self.noise_variance_transformed: ScalarFloat | None = jnp.asarray(\n                self.y_transform.apply_var(noise_variance)\n            )\n        else:\n            self.noise_variance = None\n            self.noise_variance_transformed = None\n\n        # ensure that x_transformed and y_transformed are still 1D arrays and of\n        # the same length\n        if (not self.x_transformed.ndim == 1) or (not self.y_transformed.ndim == 1):\n            raise ValueError(\n                \"x_transformed and y_transformed must be 1D arrays. \"\n                \"Please check the transformation functions.\"\n            )\n        if len(self.x_transformed) != len(self.y_transformed):\n            raise ValueError(\n                \"x_transformed and y_transformed must have the same length.\"\n            )\n\n        self.data = Dataset(\n            self.x_transformed,\n            self.y_transformed,\n        )\n\n    def _sample_noise_variance(\n        self,\n        key: PRNGKeyArray,\n    ) -&gt; ScalarFloat:\n        \"\"\"\n        Sample the noise variance from the prior distribution, if no\n        noise variance is provided. If a noise variance is provided, it\n        is returned as is.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for sampling.\n\n        Returns\n        -------\n         Float[jnp.ndarray, \" D\"]\n            The sampled noise variance, one for each particle. (If noise_variance\n            is provided, the same value is returned for all particles.)\n        \"\"\"\n\n        if self.noise_variance is None:\n            key, noise_key = jr.split(key)\n            variance: ScalarFloat = jnp.array(\n                self.config.noise_prior.sample(\n                    seed=noise_key,\n                ),\n                dtype=self.x.dtype,\n            )\n        else:\n            assert self.noise_variance_transformed is not None\n            variance = self.noise_variance_transformed\n        return variance\n\n    def _initialize_state(\n        self,\n        key: PRNGKeyArray,\n        num_particles: ScalarInt,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the state of the GP model. This primarily\n        entails creating the initial particle states by sampling\n        the kernel and noise variance (if not fixed).\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for sampling.\n        num_particles : ScalarInt\n            Number of particles in the model.\n\n        Returns\n        -------\n        GPState\n            A GPState object representing the state of the GP model (see\n            gallifrey.inference.state.GPState for details).\n        \"\"\"\n\n        def make_particle_state(key: PRNGKeyArray) -&gt; nnx.State:\n            \"\"\"\n            Create partice by sampling the kernel and noise variance\n            (if not fixed), and initializing the particle state.\n            \"\"\"\n            key, kernel_key, variance_key = jr.split(key, 3)\n            kernel_state = self.kernel_prior.sample(kernel_key)\n            noise_variance = self._sample_noise_variance(variance_key)\n\n            _, particle_state = initialize_particle_state(\n                kernel_state,\n                self.kernel_prior,\n                noise_variance,\n                self.fix_noise,\n            )\n\n            return particle_state\n\n        # create particle states by looping over the number of particles,\n        # and batch into a single state object\n        num_particles = int(num_particles)\n        particle_states = batch_states(\n            [make_particle_state(key) for key in jr.split(key, num_particles)]\n        )\n\n        self.state = GPState(\n            particle_states=particle_states,\n            num_particles=num_particles,\n            num_data_points=self.data.n,\n            mcmc_accepted=jnp.zeros(num_particles),\n            hmc_accepted=jnp.zeros(num_particles),\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the GPModel.\n\n        Returns\n        -------\n        str\n            A string representation of the GPModel.\n        \"\"\"\n        return f\"GPModel(num_particles={self.num_particles})\"\n\n    @property\n    def kernel_library(self) -&gt; KernelLibrary:\n        \"\"\"\n        Get the kernel library.\n\n        Returns\n        -------\n        KernelLibrary\n            The kernel library.\n        \"\"\"\n        return self.kernel_prior.kernel_library\n\n    @property\n    def noise_prior(self) -&gt; Distribution:\n        \"\"\"\n        Get the noise prior distribution.\n\n        Returns\n        -------\n        InverseGamma\n            The noise prior distribution.\n        \"\"\"\n        return self.config.noise_prior\n\n    @property\n    def num_particles(self) -&gt; ScalarInt:\n        \"\"\"\n        Get the number of particles in the model.\n\n        Returns\n        -------\n        ScalarInt\n            The number of particles.\n        \"\"\"\n        return self.state.num_particles\n\n    def update_state(self, gpstate: GPState) -&gt; GPModel:\n        \"\"\"\n        Update the GP state. Returns a new GPModel instance (no in-place\n        update). If the number of particles in the new state is different\n        from the current state, the num_particles attribute is updated.\n\n        Note that no other attributes are updated. If the particle states\n        were created using a different configuration, the model will not be\n        consistent.\n\n        Parameters\n        ----------\n        gpstate : GPState\n            The new GP state to update the model with.\n\n        Returns\n        -------\n        GPModel\n            A new GPModel instance with updated state.\n\n        \"\"\"\n\n        new_gpmodel = deepcopy(self)\n        new_gpmodel.state = gpstate\n\n        return new_gpmodel\n\n    def save_state(\n        self,\n        path: str | PosixPath,\n        gpstate: tp.Optional[GPState] = None,\n    ) -&gt; None:\n        \"\"\"\n        Save a GP state to file.\n\n        Note that only the state is saved and not the model itself.\n        That means if the model is loaded with a different configuration\n        (e.g. different kernel library), the state might not be consistent\n        with the model.\n\n        TODO: Implement saving the model configuration as well.\n\n        Parameters\n        ----------\n        path : str | PosixPath\n            The path where to save the GP state, must\n            be an absolute path.\n        gpstate : tp.Optional[GPState], optional\n            The GP state to save, If None, the current state\n            of the model is used. By default None.\n\n        \"\"\"\n        if gpstate is None:\n            gpstate = self.state\n\n        with open(path, \"wb\") as file:\n            pickle.dump(gpstate, file)\n\n        return None\n\n    @classmethod\n    def load_state(\n        cls,\n        path: str | PosixPath,\n    ) -&gt; GPState:\n        \"\"\"\n        Load a GP state from file.\n\n        Note that only the state is loaded. It is assumed that the model\n        configuration is consistent with how the state was saved. If the\n        model configuration is different, the loaded state might not be\n        consistent with the model.\n\n        The model does not get update with the loaded state. To update the\n        model, use the `update_gpstate` method with the loaded state.\n\n        TODO: Implement loading the model configuration as well.\n\n        Parameters\n        ----------\n        path : str | PosixPath\n            The path where to load the GP state from.\n\n        Returns\n        -------\n        GPState\n            An instance of the GPState object, containing the loaded state.\n\n        \"\"\"\n        with open(path, \"rb\") as file:\n            gpstate = pickle.load(file)\n        return gpstate\n\n    def fit_mcmc(\n        self,\n        key: PRNGKeyArray,\n        n_mcmc: ScalarInt,\n        n_hmc: ScalarInt,\n        verbosity: ScalarInt = 0,\n    ) -&gt; tuple[GPState, GPState]:\n        \"\"\"\n        Fits the GP model using MCMC.\n        It perfoms n_mcmc iterations of the structure, and for each\n        accepted structure move performs n_hmc iterations of the HMC\n        sampler over the parameters.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for MCMC sampling.\n        n_mcmc : ScalarInt\n            Number of MCMC iterations over kernel structure.\n        n_hmc : ScalarInt\n            Number of HMC steps for continuous parameters. Only used if\n            the structure move is accepted.\n        verbosity : ScalarInt, optional\n            The verbosity level, by default 0. Debugging information\n            is printed if `verbosity &gt; 1`.\n\n        Returns\n        -------\n        GPState\n            The final state of the model, wrapped in an GPState object (see\n            gallifrey.inference.state.GPState for details).\n        GPState\n            The history over all MCMC iterations, wrapped in an GPState object.\n\n        \"\"\"\n        if not isinstance(n_mcmc, int):\n            raise TypeError(\n                f\"Expected `n_mcmc` to be an integer, but got {type(n_mcmc)}.\"\n            )\n        if not isinstance(n_hmc, int):\n            raise TypeError(\n                f\"Expected `n_hmc` to be an integer, but got {type(n_hmc)}.\"\n            )\n        if n_mcmc &lt;= 0:\n            raise ValueError(\n                f\"Expected `n_mcmc` to be a positive integer, but got {n_mcmc}.\"\n            )\n        if n_hmc &lt;= 0:\n            raise ValueError(\n                f\"Expected `n_hmc` to be a positive integer, but got {n_hmc}.\"\n            )\n\n        def wrapper(\n            key: PRNGKeyArray,\n            state: nnx.State,\n        ) -&gt; tuple[nnx.State, nnx.State, ScalarInt, ScalarInt]:\n            \"\"\"Wrapper around the rejuvenate_particle function using\n            GPmodel attributes.\"\"\"\n            return rejuvenate_particle(\n                key,\n                state,\n                self.data,\n                self.kernel_prior,\n                self.noise_prior,\n                n_mcmc=n_mcmc,\n                n_hmc=n_hmc,\n                fix_noise=self.fix_noise,\n                hmc_sampler_factory=self.hmc_sampler_factory,\n                verbosity=verbosity,\n            )\n\n        final_state, history, accepted_mcmc, accepted_hmc = pmap(\n            jit(wrapper), in_axes=0\n        )(\n            jr.split(key, int(self.num_particles)),\n            self.state.particle_states,  # use states batched over 0th axis\n        )\n\n        # print information\n        if verbosity &gt; 0:\n            for i, acc_mcmc, acc_hmc in zip(\n                range(self.num_particles), accepted_mcmc, accepted_hmc\n            ):\n                print(\n                    f\"Particle {i+1} | Accepted: MCMC[{acc_mcmc}/{n_mcmc}] \"\n                    f\" HMC[{acc_hmc}/{acc_mcmc*n_hmc}]\"\n                )\n\n        # wrap final state and history in GPState objects, for consistency\n        # with the SMC algorithm\n        final_state_wrapped = GPState(\n            particle_states=final_state,\n            num_particles=self.num_particles,\n            num_data_points=self.data.n,\n            mcmc_accepted=accepted_mcmc,\n            hmc_accepted=accepted_hmc,\n        )\n\n        history_wrapped = GPState(\n            particle_states=history,\n            num_particles=self.num_particles,\n            num_data_points=self.data.n,\n            mcmc_accepted=accepted_mcmc,\n            hmc_accepted=accepted_hmc,\n        )\n\n        return final_state_wrapped, history_wrapped\n\n    def fit_smc(\n        self,\n        key: PRNGKeyArray,\n        annealing_schedule: tuple[int, ...],\n        n_mcmc: ScalarInt,\n        n_hmc: ScalarInt,\n        verbosity: int = 0,\n    ) -&gt; tuple[GPState, GPState]:\n        \"\"\"\n        Fits the GP model using SMC.\n\n        For a detailed description of the SMC algorithm, see the\n        'gallofrey.inference.smc.smc_loop' function.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            The random key for the SMC sampling.\n        annealing_schedule : tuple[int, ...]\n            The data annealing schedule for the SMC algorithm,\n            number of data points to consider at each step.\n            NOTE: Must be given in form of a tuple of integers, for\n            jax compatibility. Easily generated using the `generate`\n            method of the `Schedule` in `gallifrey.schedule`.\n        n_mcmc : ScalarInt\n            Number of MCMC iterations over kernel structure per\n            SMC step.\n        n_hmc : ScalarInt\n            Number of HMC steps for continuous parameters per\n            SMC step. Only used if the structure move is accepted.\n        verbosity : int, optional\n            The verbosity level, by default 0. Debugging information\n            is printed if `verbosity &gt; 1`.\n\n        Returns\n        -------\n        GPState\n            The final SMC state. Contains the final particle states\n            and the final weights (among other things, see\n            'gallifrey.inference.state.GPState' for details).\n        GPState\n            The history of the SMC algorithm. Contains the particle\n            states and weights at each step of the algorithm.\n\n        \"\"\"\n\n        final_smc_state, history = smc_loop(\n            key,\n            self.state.particle_states,\n            annealing_schedule,\n            int(self.num_particles),\n            self.data,\n            self.kernel_prior,\n            self.noise_prior,\n            self.fix_noise,\n            self.hmc_sampler_factory,\n            n_mcmc,\n            n_hmc,\n            verbosity=verbosity,\n        )\n\n        batched_history: GPState = batch_states(history)\n\n        return final_smc_state, batched_history\n\n    def get_particles(\n        self,\n        gpstate: tp.Optional[GPState] = None,\n    ) -&gt; list[Particle]:\n        \"\"\"\n        Get a list of Particle instances from a GP state.\n\n        If no state is provided, the current state of the model\n        is used.\n\n        Parameters\n        ----------\n        gpstate : tp.Optional[GPState], optional\n            The GP state to extract the particles from. If None,\n            the current state of the model is used. By default None.\n\n        Returns\n        -------\n        list[Particle]\n            A list of Particle instances, corresponding to the\n            individual states.\n        \"\"\"\n        if gpstate is None:\n            gpstate = self.state\n\n        unbatched_particle_states = unbatch_states(gpstate.particle_states)\n        return [\n            nnx.merge(self.particle_graphdef, state)\n            for state in unbatched_particle_states\n        ]\n\n    def get_predictive_distributions(\n        self,\n        xpredict: Float[Array, \" D\"],\n        data: tp.Optional[Dataset] = None,\n        gpstate: tp.Optional[GPState] = None,\n        latent: bool = False,\n    ) -&gt; list[Distribution]:\n        \"\"\"\n        Calculate the predictive distributions for the individual particles\n        in the GP state. The distributions are calculated at the points\n        `x_predict` and conditioned on the training data (which was supplied\n        to construct the model instance).\n\n        The distributions are returned as a list of tensorflow probability\n        distribution objects, see\n        https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalFullCovariance\n\n        If no state is provided, the current state of the model\n        is used.\n\n        If `latent` is True, the predictive distribution is that of the latent\n        function, i.e. the distribution of the function values without the\n        observational noise. If False, the predictive distribution of the full\n        data-generating model is returned, which includes the observational noise.\n\n        Parameters\n        ----------\n        xpredict : Float[Array, \" D\"]\n            The points to predict, as a 1D array.\n        data : tp.Optional[Dataset], optional\n            The data to condition the predictive distribution on. If None,\n            the training data of the model is used. By default None.\n        gpstate : tp.Optional[GPState], optional\n            The GP state object, containing the particle states. If None,\n            the current state of the model is used. By default None.\n        latent : bool, optional\n            Whether to return the predictive distribution of the latent\n            functions only (without observational noise), by default\n            False.\n\n        Returns\n        -------\n        list[Distribution]\n            A list of tensorflow probability distribution objects\n            representing the predictive distributions of the Gaussian\n            processes. (Specifically, a MultivariateNormalFullCovariance\n            distribution from `tensorflow_probability.substrates.jax.distributions`).\n\n        \"\"\"\n        gpstate = self.state if gpstate is None else gpstate\n        data = self.data if data is None else data\n\n        particles = self.get_particles(gpstate)\n        distributions = [\n            particle.predictive_distribution(\n                jnp.atleast_1d(xpredict).squeeze(),\n                data,\n                latent,\n            )\n            for particle in particles\n        ]\n\n        return distributions\n\n    def get_mixture_distribution(\n        self,\n        xpredict: Float[Array, \" D\"],\n        gpstate: tp.Optional[GPState] = None,\n        data: tp.Optional[Dataset] = None,\n        log_weights: tp.Optional[Float[Array, \" N\"]] = None,\n        num_particles: tp.Optional[ScalarInt] = None,\n        key: tp.Optional[PRNGKeyArray] = None,\n        latent: bool = False,\n    ) -&gt; Distribution:\n        \"\"\"\n        Get the mixture distribution of an SMC state.\n\n        The predictive distributions for an SMC ensemble are\n        the individual predictive (Gaussion) distributions of the\n        particles, weighted by the particle weights. The resulting\n        distribution is a Gaussian mixture model, implemented as\n        a `MixtureSameFamily` distribution from `tensorflow_probability`, see\n        https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MixtureSameFamily\n\n        The mixture distribution is calculated at the points `xpredict`\n        and conditioned on the training data (which was supplied to construct\n        the model instance).\n\n        The input should be an GPState object as returned by the `fit_smc` method.\n        Alternatively, the log weights can be provided explicitly using the\n        `log_weights` argument.\n\n\n        Parameters\n        ----------\n        xpredict : Float[Array, \" D\"]\n            The points to predict, as a 1D array.\n        gpstate : tp.Optional[GPState], optional\n            The GP state object, containing the particle states and log weights for\n            the mixture distribution. If None, the current state of the model is used.\n            By default None.\n        data : tp.Optional[Dataset], optional\n            The data to condition the predictive distribution on. If None,\n            the training data of the model is used. By default None.\n        log_weights : Float[Array, \" N\"], optional\n            The log weights of the particles. If None, the log weights from the\n            GP state are used. An error is raised if the log weights are not provided\n            and the GP state does not contain log weights. By default None.\n        num_particles : tp.Optional[ScalarInt], optional\n            Number of particles to include in the mixture distribution. If None,\n            all particles in the state are included. If provided, a random sample\n            of particles is chosen based on the weights. By default None.\n        key : tp.Optional[PRNGKeyArray], optional\n            Random key for sampling the particles. Required if `num_particles` is\n            provided. By default None.\n        latent : bool, optional\n            Whether to return the predictive distribution of the latent\n            functions only (without observational noise), by default\n            False.\n\n        Returns\n        -------\n        Distribution\n            A tensorflow probability distribution object representing\n            the Gaussian mixture distribution of the Gaussian processes\n            (a MixtureSameFamily distribution).\n\n        Raises\n        ------\n        ValueError\n            If the GPState object contains no log weights and `log_weights` is None.\n        ValueError\n            If the number of particles and log weights are inconsistent.\n\n        \"\"\"\n        gpstate = self.state if gpstate is None else gpstate\n        data = self.data if data is None else data\n\n        if (gpstate.log_weights is None) and (log_weights is None):\n            raise ValueError(\n                \"The GPState object contains no log weights. This might be \"\n                \"because the state was produced with the MCMC sampler or the \"\n                \"initial state of the GPModel was used. Please either run \"\n                \"`fit_smc` or provide the log weights explicitly. (Note: If \"\n                \"you already ran `fit_smc` and this error occurs, make sure to \"\n                \"passed the the output of `fit_smc` to this method, or run the \"\n                \"`update_state` method with the output of `fit_smc` as input.)\"\n            )\n\n        log_weights = log_weights if log_weights is not None else gpstate.log_weights\n\n        assert log_weights is not None\n        if len(log_weights) != self.num_particles:\n            raise ValueError(\n                f\"Inconsistent number of particles and log weights, \"\n                f\"expected {self.num_particles} but got {len(log_weights)}.\"\n            )\n\n        individual_distributions = self.get_predictive_distributions(\n            xpredict,\n            data,\n            gpstate,\n            latent,\n        )\n\n        if num_particles is not None:\n            if key is None:\n                raise ValueError(\n                    \"If `num_particles` is provided, `key` must also be provided. \"\n                    \"This is the random key for sampling from the particles.\"\n                )\n            if num_particles &gt; self.num_particles:\n                raise ValueError(\n                    f\"Number of particles to sample ({num_particles}) \"\n                    f\"exceeds the total number of particles ({self.num_particles}).\"\n                )\n            # choose random sample of particles based on weights\n            weights = jnp.exp(log_weights)\n            particle_indices = jr.choice(\n                key,\n                jnp.arange(len(weights)),\n                p=weights,\n                shape=(int(num_particles),),\n                replace=False,\n            )\n            # normalize weights\n            log_weights = log_weights[particle_indices] - logsumexp(\n                log_weights[particle_indices]\n            )\n            # select particles\n            individual_distributions = [\n                individual_distributions[idx] for idx in particle_indices\n            ]\n\n        batched_distributions = batch_states(individual_distributions)\n\n        mixture_model = MixtureSameFamily(\n            mixture_distribution=Categorical(logits=log_weights),\n            components_distribution=batched_distributions,\n        )\n\n        return mixture_model\n\n    def display(\n        self,\n        gpstate: tp.Optional[GPState] = None,\n        num_particles: tp.Optional[ScalarInt] = None,\n    ) -&gt; None:\n        \"\"\"\n        Prints a summary of the GP model, including particle kernels\n        and noise variances.\n\n        If no particle states are provided, the current batched state of the model\n        is used.\n\n        Iterates through each particle, merges its graph definition and\n        state, and prints the particle index, noise variance, and the kernel structure.\n        Useful for inspecting the current state of the particle ensemble.\n\n        Parameters\n        ----------\n        gpstate : tp.Optional[GPState], optional\n            The GP state object, containing the particle states. If None, the current\n            state of the model is used. By default None.\n        num_particles : tp.Optional[ScalarInt], optional\n            Number of particles to display. If None, all particles are displayed.\n            By default None.\n\n        \"\"\"\n        gpstate = gpstate if gpstate is not None else self.state\n        num_particles = self.num_particles if num_particles is None else num_particles\n\n        particles = self.get_particles(gpstate)\n        for i in range(num_particles):\n            print(\"=\" * 50)\n            if gpstate.log_weights is not None:\n                print(\n                    f\"Particle {i+1} \"\n                    f\"| Weight: {jnp.exp(gpstate.log_weights[i]):.2f} \"\n                    f\"| Variance: {particles[i].noise_variance.value} \"\n                )\n            else:\n                print(\n                    f\"Particle {i+1} \"\n                    f\"| Variance: {particles[i].noise_variance.value} \"\n                )\n            print(f\"{particles[i].kernel}\")\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel.kernel_library","title":"<code>kernel_library</code>  <code>property</code>","text":"<p>Get the kernel library.</p> <p>Returns:</p> Type Description <code>KernelLibrary</code> <p>The kernel library.</p>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel.noise_prior","title":"<code>noise_prior</code>  <code>property</code>","text":"<p>Get the noise prior distribution.</p> <p>Returns:</p> Type Description <code>InverseGamma</code> <p>The noise prior distribution.</p>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel.num_particles","title":"<code>num_particles</code>  <code>property</code>","text":"<p>Get the number of particles in the model.</p> <p>Returns:</p> Type Description <code>ScalarInt</code> <p>The number of particles.</p>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel.__init__","title":"<code>__init__(key, x, y, num_particles, noise_variance=None, x_transform=LinearTransform, y_transform=LinearTransform, config=None)</code>","text":"<p>Initialize the GP model.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key for the initial sampling.</p> required <code>x</code> <code>Float[Array, ' D']</code> <p>Input data, array of shape (D,).</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>Target data, array of shape (D,).</p> required <code>num_particles</code> <code>ScalarInt</code> <p>The number of particles in the model.</p> required <code>noise_variance</code> <code>Optional[ScalarFloat]</code> <p>The variance of the observation noise. If None, the noise variance is sampled and treated as a trainable parameter. By default None. NOTE: Currently heteroscadastic noise is not supported.</p> <code>None</code> <code>x_transform</code> <code>Transform</code> <p>A transformation applied to the input x data, must be an instance of gallifrey.inference.transforms.Transform class. By default LinearTransform. (Used to normalise data for easier training.)</p> <code>LinearTransform</code> <code>y_transform</code> <code>Optional[Callable]</code> <p>A transformation applied to the input y data, must be an instance of gallifrey.inference.transforms.Transform class. By default LinearTransform. (Used to normalise data for easier training.)</p> <code>LinearTransform</code> <code>config</code> <code>Optional[GPConfig]</code> <p>The configuration object for the GP model. Contains information of the kernel and parameter priors, the mean function, the max depth of the tree kernel, etc. By default None, in which case the default configuration is used (see gallifrey.config.GPConfig for details).</p> <code>None</code> Source code in <code>gallifrey/model.py</code> <pre><code>def __init__(\n    self,\n    key: PRNGKeyArray,\n    x: Float[Array, \" D\"],\n    y: Float[Array, \" D\"],\n    num_particles: ScalarInt,\n    noise_variance: tp.Optional[ScalarFloat] = None,\n    x_transform: tp.Type[Transform] = LinearTransform,\n    y_transform: tp.Type[Transform] = LinearTransform,\n    config: tp.Optional[GPConfig] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the GP model.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key for the initial sampling.\n    x : Float[Array, \" D\"]\n        Input data, array of shape (D,).\n    y : Float[Array, \" D\"]\n        Target data, array of shape (D,).\n    num_particles : ScalarInt\n        The number of particles in the model.\n    noise_variance : tp.Optional[ScalarFloat], optional\n        The variance of the observation noise. If\n        None, the noise variance is sampled and treated\n        as a trainable parameter. By default None.\n        NOTE: Currently heteroscadastic noise is not\n        supported.\n    x_transform : Transform, optional\n        A transformation applied to the input x data, must\n        be an instance of gallifrey.inference.transforms.Transform\n        class. By default LinearTransform. (Used to normalise\n        data for easier training.)\n    y_transform : Optional[Callable], optional\n        A transformation applied to the input y data, must\n        be an instance of gallifrey.inference.transforms.Transform\n        class. By default LinearTransform. (Used to normalise\n        data for easier training.)\n    config : tp.Optional[GPConfig], optional\n        The configuration object for the GP model. Contains\n        information of the kernel and parameter priors, the\n        mean function, the max depth of the tree kernel, etc.\n        By default None, in which case the default configuration\n        is used (see gallifrey.config.GPConfig for details).\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\n            f\"Input data x and y must have the same length, \"\n            \"but got len(x)={len(x)} and len(y)={len(y)}.\"\n        )\n\n    # set basic attributes\n    self.config = config if config is not None else GPConfig()\n    self.fix_noise = True if noise_variance is not None else False\n\n    # create kernel prior\n    kernel_library = KernelLibrary(\n        atoms=deepcopy(self.config.atoms),\n        operators=deepcopy(self.config.operators),\n        prior_transforms=deepcopy(self.config.prior_transforms),\n    )\n\n    self.kernel_prior = KernelPrior(\n        kernel_library,\n        max_depth=deepcopy(self.config.max_depth),\n        num_datapoints=len(x),\n        probs=deepcopy(self.config.node_probabilities),\n    )\n\n    # preprocess data (apply transformations, and set attributes)\n    self._preprocess_data(\n        x,\n        y,\n        x_transform,\n        y_transform,\n        noise_variance,\n    )\n\n    # create a particle_graphdef attributes (using a randomly\n    # initilized particle state). The graphdef attribute,\n    # is used to create particle instances from the particle states\n    self.particle_graphdef, particle_state = initialize_particle_state(\n        self.kernel_prior.sample(jr.PRNGKey(0)),\n        self.kernel_prior,\n        self._sample_noise_variance(jr.PRNGKey(0)),\n        self.fix_noise,\n    )\n\n    # sample initial particles and create initial GP state\n    key, particle_key = jr.split(key)\n    self._initialize_state(particle_key, num_particles)\n\n    # create HMC sampler factory for rejuvenation\n    self.hmc_sampler_factory = create_hmc_sampler_factory(\n        self.config.hmc_config,\n        int(self.kernel_prior.max_kernel_parameter + (not self.fix_noise)),\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the GPModel.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the GPModel.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the GPModel.\n\n    Returns\n    -------\n    str\n        A string representation of the GPModel.\n    \"\"\"\n    return f\"GPModel(num_particles={self.num_particles})\"\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel.display","title":"<code>display(gpstate=None, num_particles=None)</code>","text":"<p>Prints a summary of the GP model, including particle kernels and noise variances.</p> <p>If no particle states are provided, the current batched state of the model is used.</p> <p>Iterates through each particle, merges its graph definition and state, and prints the particle index, noise variance, and the kernel structure. Useful for inspecting the current state of the particle ensemble.</p> <p>Parameters:</p> Name Type Description Default <code>gpstate</code> <code>Optional[GPState]</code> <p>The GP state object, containing the particle states. If None, the current state of the model is used. By default None.</p> <code>None</code> <code>num_particles</code> <code>Optional[ScalarInt]</code> <p>Number of particles to display. If None, all particles are displayed. By default None.</p> <code>None</code> Source code in <code>gallifrey/model.py</code> <pre><code>def display(\n    self,\n    gpstate: tp.Optional[GPState] = None,\n    num_particles: tp.Optional[ScalarInt] = None,\n) -&gt; None:\n    \"\"\"\n    Prints a summary of the GP model, including particle kernels\n    and noise variances.\n\n    If no particle states are provided, the current batched state of the model\n    is used.\n\n    Iterates through each particle, merges its graph definition and\n    state, and prints the particle index, noise variance, and the kernel structure.\n    Useful for inspecting the current state of the particle ensemble.\n\n    Parameters\n    ----------\n    gpstate : tp.Optional[GPState], optional\n        The GP state object, containing the particle states. If None, the current\n        state of the model is used. By default None.\n    num_particles : tp.Optional[ScalarInt], optional\n        Number of particles to display. If None, all particles are displayed.\n        By default None.\n\n    \"\"\"\n    gpstate = gpstate if gpstate is not None else self.state\n    num_particles = self.num_particles if num_particles is None else num_particles\n\n    particles = self.get_particles(gpstate)\n    for i in range(num_particles):\n        print(\"=\" * 50)\n        if gpstate.log_weights is not None:\n            print(\n                f\"Particle {i+1} \"\n                f\"| Weight: {jnp.exp(gpstate.log_weights[i]):.2f} \"\n                f\"| Variance: {particles[i].noise_variance.value} \"\n            )\n        else:\n            print(\n                f\"Particle {i+1} \"\n                f\"| Variance: {particles[i].noise_variance.value} \"\n            )\n        print(f\"{particles[i].kernel}\")\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel.fit_mcmc","title":"<code>fit_mcmc(key, n_mcmc, n_hmc, verbosity=0)</code>","text":"<p>Fits the GP model using MCMC. It perfoms n_mcmc iterations of the structure, and for each accepted structure move performs n_hmc iterations of the HMC sampler over the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for MCMC sampling.</p> required <code>n_mcmc</code> <code>ScalarInt</code> <p>Number of MCMC iterations over kernel structure.</p> required <code>n_hmc</code> <code>ScalarInt</code> <p>Number of HMC steps for continuous parameters. Only used if the structure move is accepted.</p> required <code>verbosity</code> <code>ScalarInt</code> <p>The verbosity level, by default 0. Debugging information is printed if <code>verbosity &gt; 1</code>.</p> <code>0</code> <p>Returns:</p> Type Description <code>GPState</code> <p>The final state of the model, wrapped in an GPState object (see gallifrey.inference.state.GPState for details).</p> <code>GPState</code> <p>The history over all MCMC iterations, wrapped in an GPState object.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def fit_mcmc(\n    self,\n    key: PRNGKeyArray,\n    n_mcmc: ScalarInt,\n    n_hmc: ScalarInt,\n    verbosity: ScalarInt = 0,\n) -&gt; tuple[GPState, GPState]:\n    \"\"\"\n    Fits the GP model using MCMC.\n    It perfoms n_mcmc iterations of the structure, and for each\n    accepted structure move performs n_hmc iterations of the HMC\n    sampler over the parameters.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for MCMC sampling.\n    n_mcmc : ScalarInt\n        Number of MCMC iterations over kernel structure.\n    n_hmc : ScalarInt\n        Number of HMC steps for continuous parameters. Only used if\n        the structure move is accepted.\n    verbosity : ScalarInt, optional\n        The verbosity level, by default 0. Debugging information\n        is printed if `verbosity &gt; 1`.\n\n    Returns\n    -------\n    GPState\n        The final state of the model, wrapped in an GPState object (see\n        gallifrey.inference.state.GPState for details).\n    GPState\n        The history over all MCMC iterations, wrapped in an GPState object.\n\n    \"\"\"\n    if not isinstance(n_mcmc, int):\n        raise TypeError(\n            f\"Expected `n_mcmc` to be an integer, but got {type(n_mcmc)}.\"\n        )\n    if not isinstance(n_hmc, int):\n        raise TypeError(\n            f\"Expected `n_hmc` to be an integer, but got {type(n_hmc)}.\"\n        )\n    if n_mcmc &lt;= 0:\n        raise ValueError(\n            f\"Expected `n_mcmc` to be a positive integer, but got {n_mcmc}.\"\n        )\n    if n_hmc &lt;= 0:\n        raise ValueError(\n            f\"Expected `n_hmc` to be a positive integer, but got {n_hmc}.\"\n        )\n\n    def wrapper(\n        key: PRNGKeyArray,\n        state: nnx.State,\n    ) -&gt; tuple[nnx.State, nnx.State, ScalarInt, ScalarInt]:\n        \"\"\"Wrapper around the rejuvenate_particle function using\n        GPmodel attributes.\"\"\"\n        return rejuvenate_particle(\n            key,\n            state,\n            self.data,\n            self.kernel_prior,\n            self.noise_prior,\n            n_mcmc=n_mcmc,\n            n_hmc=n_hmc,\n            fix_noise=self.fix_noise,\n            hmc_sampler_factory=self.hmc_sampler_factory,\n            verbosity=verbosity,\n        )\n\n    final_state, history, accepted_mcmc, accepted_hmc = pmap(\n        jit(wrapper), in_axes=0\n    )(\n        jr.split(key, int(self.num_particles)),\n        self.state.particle_states,  # use states batched over 0th axis\n    )\n\n    # print information\n    if verbosity &gt; 0:\n        for i, acc_mcmc, acc_hmc in zip(\n            range(self.num_particles), accepted_mcmc, accepted_hmc\n        ):\n            print(\n                f\"Particle {i+1} | Accepted: MCMC[{acc_mcmc}/{n_mcmc}] \"\n                f\" HMC[{acc_hmc}/{acc_mcmc*n_hmc}]\"\n            )\n\n    # wrap final state and history in GPState objects, for consistency\n    # with the SMC algorithm\n    final_state_wrapped = GPState(\n        particle_states=final_state,\n        num_particles=self.num_particles,\n        num_data_points=self.data.n,\n        mcmc_accepted=accepted_mcmc,\n        hmc_accepted=accepted_hmc,\n    )\n\n    history_wrapped = GPState(\n        particle_states=history,\n        num_particles=self.num_particles,\n        num_data_points=self.data.n,\n        mcmc_accepted=accepted_mcmc,\n        hmc_accepted=accepted_hmc,\n    )\n\n    return final_state_wrapped, history_wrapped\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel.fit_smc","title":"<code>fit_smc(key, annealing_schedule, n_mcmc, n_hmc, verbosity=0)</code>","text":"<p>Fits the GP model using SMC.</p> <p>For a detailed description of the SMC algorithm, see the 'gallofrey.inference.smc.smc_loop' function.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key for the SMC sampling.</p> required <code>annealing_schedule</code> <code>tuple[int, ...]</code> <p>The data annealing schedule for the SMC algorithm, number of data points to consider at each step. NOTE: Must be given in form of a tuple of integers, for jax compatibility. Easily generated using the <code>generate</code> method of the <code>Schedule</code> in <code>gallifrey.schedule</code>.</p> required <code>n_mcmc</code> <code>ScalarInt</code> <p>Number of MCMC iterations over kernel structure per SMC step.</p> required <code>n_hmc</code> <code>ScalarInt</code> <p>Number of HMC steps for continuous parameters per SMC step. Only used if the structure move is accepted.</p> required <code>verbosity</code> <code>int</code> <p>The verbosity level, by default 0. Debugging information is printed if <code>verbosity &gt; 1</code>.</p> <code>0</code> <p>Returns:</p> Type Description <code>GPState</code> <p>The final SMC state. Contains the final particle states and the final weights (among other things, see 'gallifrey.inference.state.GPState' for details).</p> <code>GPState</code> <p>The history of the SMC algorithm. Contains the particle states and weights at each step of the algorithm.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def fit_smc(\n    self,\n    key: PRNGKeyArray,\n    annealing_schedule: tuple[int, ...],\n    n_mcmc: ScalarInt,\n    n_hmc: ScalarInt,\n    verbosity: int = 0,\n) -&gt; tuple[GPState, GPState]:\n    \"\"\"\n    Fits the GP model using SMC.\n\n    For a detailed description of the SMC algorithm, see the\n    'gallofrey.inference.smc.smc_loop' function.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key for the SMC sampling.\n    annealing_schedule : tuple[int, ...]\n        The data annealing schedule for the SMC algorithm,\n        number of data points to consider at each step.\n        NOTE: Must be given in form of a tuple of integers, for\n        jax compatibility. Easily generated using the `generate`\n        method of the `Schedule` in `gallifrey.schedule`.\n    n_mcmc : ScalarInt\n        Number of MCMC iterations over kernel structure per\n        SMC step.\n    n_hmc : ScalarInt\n        Number of HMC steps for continuous parameters per\n        SMC step. Only used if the structure move is accepted.\n    verbosity : int, optional\n        The verbosity level, by default 0. Debugging information\n        is printed if `verbosity &gt; 1`.\n\n    Returns\n    -------\n    GPState\n        The final SMC state. Contains the final particle states\n        and the final weights (among other things, see\n        'gallifrey.inference.state.GPState' for details).\n    GPState\n        The history of the SMC algorithm. Contains the particle\n        states and weights at each step of the algorithm.\n\n    \"\"\"\n\n    final_smc_state, history = smc_loop(\n        key,\n        self.state.particle_states,\n        annealing_schedule,\n        int(self.num_particles),\n        self.data,\n        self.kernel_prior,\n        self.noise_prior,\n        self.fix_noise,\n        self.hmc_sampler_factory,\n        n_mcmc,\n        n_hmc,\n        verbosity=verbosity,\n    )\n\n    batched_history: GPState = batch_states(history)\n\n    return final_smc_state, batched_history\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel.get_mixture_distribution","title":"<code>get_mixture_distribution(xpredict, gpstate=None, data=None, log_weights=None, num_particles=None, key=None, latent=False)</code>","text":"<p>Get the mixture distribution of an SMC state.</p> <p>The predictive distributions for an SMC ensemble are the individual predictive (Gaussion) distributions of the particles, weighted by the particle weights. The resulting distribution is a Gaussian mixture model, implemented as a <code>MixtureSameFamily</code> distribution from <code>tensorflow_probability</code>, see https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MixtureSameFamily</p> <p>The mixture distribution is calculated at the points <code>xpredict</code> and conditioned on the training data (which was supplied to construct the model instance).</p> <p>The input should be an GPState object as returned by the <code>fit_smc</code> method. Alternatively, the log weights can be provided explicitly using the <code>log_weights</code> argument.</p> <p>Parameters:</p> Name Type Description Default <code>xpredict</code> <code>Float[Array, ' D']</code> <p>The points to predict, as a 1D array.</p> required <code>gpstate</code> <code>Optional[GPState]</code> <p>The GP state object, containing the particle states and log weights for the mixture distribution. If None, the current state of the model is used. By default None.</p> <code>None</code> <code>data</code> <code>Optional[Dataset]</code> <p>The data to condition the predictive distribution on. If None, the training data of the model is used. By default None.</p> <code>None</code> <code>log_weights</code> <code>Float[Array, ' N']</code> <p>The log weights of the particles. If None, the log weights from the GP state are used. An error is raised if the log weights are not provided and the GP state does not contain log weights. By default None.</p> <code>None</code> <code>num_particles</code> <code>Optional[ScalarInt]</code> <p>Number of particles to include in the mixture distribution. If None, all particles in the state are included. If provided, a random sample of particles is chosen based on the weights. By default None.</p> <code>None</code> <code>key</code> <code>Optional[PRNGKeyArray]</code> <p>Random key for sampling the particles. Required if <code>num_particles</code> is provided. By default None.</p> <code>None</code> <code>latent</code> <code>bool</code> <p>Whether to return the predictive distribution of the latent functions only (without observational noise), by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Distribution</code> <p>A tensorflow probability distribution object representing the Gaussian mixture distribution of the Gaussian processes (a MixtureSameFamily distribution).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the GPState object contains no log weights and <code>log_weights</code> is None.</p> <code>ValueError</code> <p>If the number of particles and log weights are inconsistent.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def get_mixture_distribution(\n    self,\n    xpredict: Float[Array, \" D\"],\n    gpstate: tp.Optional[GPState] = None,\n    data: tp.Optional[Dataset] = None,\n    log_weights: tp.Optional[Float[Array, \" N\"]] = None,\n    num_particles: tp.Optional[ScalarInt] = None,\n    key: tp.Optional[PRNGKeyArray] = None,\n    latent: bool = False,\n) -&gt; Distribution:\n    \"\"\"\n    Get the mixture distribution of an SMC state.\n\n    The predictive distributions for an SMC ensemble are\n    the individual predictive (Gaussion) distributions of the\n    particles, weighted by the particle weights. The resulting\n    distribution is a Gaussian mixture model, implemented as\n    a `MixtureSameFamily` distribution from `tensorflow_probability`, see\n    https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MixtureSameFamily\n\n    The mixture distribution is calculated at the points `xpredict`\n    and conditioned on the training data (which was supplied to construct\n    the model instance).\n\n    The input should be an GPState object as returned by the `fit_smc` method.\n    Alternatively, the log weights can be provided explicitly using the\n    `log_weights` argument.\n\n\n    Parameters\n    ----------\n    xpredict : Float[Array, \" D\"]\n        The points to predict, as a 1D array.\n    gpstate : tp.Optional[GPState], optional\n        The GP state object, containing the particle states and log weights for\n        the mixture distribution. If None, the current state of the model is used.\n        By default None.\n    data : tp.Optional[Dataset], optional\n        The data to condition the predictive distribution on. If None,\n        the training data of the model is used. By default None.\n    log_weights : Float[Array, \" N\"], optional\n        The log weights of the particles. If None, the log weights from the\n        GP state are used. An error is raised if the log weights are not provided\n        and the GP state does not contain log weights. By default None.\n    num_particles : tp.Optional[ScalarInt], optional\n        Number of particles to include in the mixture distribution. If None,\n        all particles in the state are included. If provided, a random sample\n        of particles is chosen based on the weights. By default None.\n    key : tp.Optional[PRNGKeyArray], optional\n        Random key for sampling the particles. Required if `num_particles` is\n        provided. By default None.\n    latent : bool, optional\n        Whether to return the predictive distribution of the latent\n        functions only (without observational noise), by default\n        False.\n\n    Returns\n    -------\n    Distribution\n        A tensorflow probability distribution object representing\n        the Gaussian mixture distribution of the Gaussian processes\n        (a MixtureSameFamily distribution).\n\n    Raises\n    ------\n    ValueError\n        If the GPState object contains no log weights and `log_weights` is None.\n    ValueError\n        If the number of particles and log weights are inconsistent.\n\n    \"\"\"\n    gpstate = self.state if gpstate is None else gpstate\n    data = self.data if data is None else data\n\n    if (gpstate.log_weights is None) and (log_weights is None):\n        raise ValueError(\n            \"The GPState object contains no log weights. This might be \"\n            \"because the state was produced with the MCMC sampler or the \"\n            \"initial state of the GPModel was used. Please either run \"\n            \"`fit_smc` or provide the log weights explicitly. (Note: If \"\n            \"you already ran `fit_smc` and this error occurs, make sure to \"\n            \"passed the the output of `fit_smc` to this method, or run the \"\n            \"`update_state` method with the output of `fit_smc` as input.)\"\n        )\n\n    log_weights = log_weights if log_weights is not None else gpstate.log_weights\n\n    assert log_weights is not None\n    if len(log_weights) != self.num_particles:\n        raise ValueError(\n            f\"Inconsistent number of particles and log weights, \"\n            f\"expected {self.num_particles} but got {len(log_weights)}.\"\n        )\n\n    individual_distributions = self.get_predictive_distributions(\n        xpredict,\n        data,\n        gpstate,\n        latent,\n    )\n\n    if num_particles is not None:\n        if key is None:\n            raise ValueError(\n                \"If `num_particles` is provided, `key` must also be provided. \"\n                \"This is the random key for sampling from the particles.\"\n            )\n        if num_particles &gt; self.num_particles:\n            raise ValueError(\n                f\"Number of particles to sample ({num_particles}) \"\n                f\"exceeds the total number of particles ({self.num_particles}).\"\n            )\n        # choose random sample of particles based on weights\n        weights = jnp.exp(log_weights)\n        particle_indices = jr.choice(\n            key,\n            jnp.arange(len(weights)),\n            p=weights,\n            shape=(int(num_particles),),\n            replace=False,\n        )\n        # normalize weights\n        log_weights = log_weights[particle_indices] - logsumexp(\n            log_weights[particle_indices]\n        )\n        # select particles\n        individual_distributions = [\n            individual_distributions[idx] for idx in particle_indices\n        ]\n\n    batched_distributions = batch_states(individual_distributions)\n\n    mixture_model = MixtureSameFamily(\n        mixture_distribution=Categorical(logits=log_weights),\n        components_distribution=batched_distributions,\n    )\n\n    return mixture_model\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel.get_particles","title":"<code>get_particles(gpstate=None)</code>","text":"<p>Get a list of Particle instances from a GP state.</p> <p>If no state is provided, the current state of the model is used.</p> <p>Parameters:</p> Name Type Description Default <code>gpstate</code> <code>Optional[GPState]</code> <p>The GP state to extract the particles from. If None, the current state of the model is used. By default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Particle]</code> <p>A list of Particle instances, corresponding to the individual states.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def get_particles(\n    self,\n    gpstate: tp.Optional[GPState] = None,\n) -&gt; list[Particle]:\n    \"\"\"\n    Get a list of Particle instances from a GP state.\n\n    If no state is provided, the current state of the model\n    is used.\n\n    Parameters\n    ----------\n    gpstate : tp.Optional[GPState], optional\n        The GP state to extract the particles from. If None,\n        the current state of the model is used. By default None.\n\n    Returns\n    -------\n    list[Particle]\n        A list of Particle instances, corresponding to the\n        individual states.\n    \"\"\"\n    if gpstate is None:\n        gpstate = self.state\n\n    unbatched_particle_states = unbatch_states(gpstate.particle_states)\n    return [\n        nnx.merge(self.particle_graphdef, state)\n        for state in unbatched_particle_states\n    ]\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel.get_predictive_distributions","title":"<code>get_predictive_distributions(xpredict, data=None, gpstate=None, latent=False)</code>","text":"<p>Calculate the predictive distributions for the individual particles in the GP state. The distributions are calculated at the points <code>x_predict</code> and conditioned on the training data (which was supplied to construct the model instance).</p> <p>The distributions are returned as a list of tensorflow probability distribution objects, see https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalFullCovariance</p> <p>If no state is provided, the current state of the model is used.</p> <p>If <code>latent</code> is True, the predictive distribution is that of the latent function, i.e. the distribution of the function values without the observational noise. If False, the predictive distribution of the full data-generating model is returned, which includes the observational noise.</p> <p>Parameters:</p> Name Type Description Default <code>xpredict</code> <code>Float[Array, ' D']</code> <p>The points to predict, as a 1D array.</p> required <code>data</code> <code>Optional[Dataset]</code> <p>The data to condition the predictive distribution on. If None, the training data of the model is used. By default None.</p> <code>None</code> <code>gpstate</code> <code>Optional[GPState]</code> <p>The GP state object, containing the particle states. If None, the current state of the model is used. By default None.</p> <code>None</code> <code>latent</code> <code>bool</code> <p>Whether to return the predictive distribution of the latent functions only (without observational noise), by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Distribution]</code> <p>A list of tensorflow probability distribution objects representing the predictive distributions of the Gaussian processes. (Specifically, a MultivariateNormalFullCovariance distribution from <code>tensorflow_probability.substrates.jax.distributions</code>).</p> Source code in <code>gallifrey/model.py</code> <pre><code>def get_predictive_distributions(\n    self,\n    xpredict: Float[Array, \" D\"],\n    data: tp.Optional[Dataset] = None,\n    gpstate: tp.Optional[GPState] = None,\n    latent: bool = False,\n) -&gt; list[Distribution]:\n    \"\"\"\n    Calculate the predictive distributions for the individual particles\n    in the GP state. The distributions are calculated at the points\n    `x_predict` and conditioned on the training data (which was supplied\n    to construct the model instance).\n\n    The distributions are returned as a list of tensorflow probability\n    distribution objects, see\n    https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalFullCovariance\n\n    If no state is provided, the current state of the model\n    is used.\n\n    If `latent` is True, the predictive distribution is that of the latent\n    function, i.e. the distribution of the function values without the\n    observational noise. If False, the predictive distribution of the full\n    data-generating model is returned, which includes the observational noise.\n\n    Parameters\n    ----------\n    xpredict : Float[Array, \" D\"]\n        The points to predict, as a 1D array.\n    data : tp.Optional[Dataset], optional\n        The data to condition the predictive distribution on. If None,\n        the training data of the model is used. By default None.\n    gpstate : tp.Optional[GPState], optional\n        The GP state object, containing the particle states. If None,\n        the current state of the model is used. By default None.\n    latent : bool, optional\n        Whether to return the predictive distribution of the latent\n        functions only (without observational noise), by default\n        False.\n\n    Returns\n    -------\n    list[Distribution]\n        A list of tensorflow probability distribution objects\n        representing the predictive distributions of the Gaussian\n        processes. (Specifically, a MultivariateNormalFullCovariance\n        distribution from `tensorflow_probability.substrates.jax.distributions`).\n\n    \"\"\"\n    gpstate = self.state if gpstate is None else gpstate\n    data = self.data if data is None else data\n\n    particles = self.get_particles(gpstate)\n    distributions = [\n        particle.predictive_distribution(\n            jnp.atleast_1d(xpredict).squeeze(),\n            data,\n            latent,\n        )\n        for particle in particles\n    ]\n\n    return distributions\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel.load_state","title":"<code>load_state(path)</code>  <code>classmethod</code>","text":"<p>Load a GP state from file.</p> <p>Note that only the state is loaded. It is assumed that the model configuration is consistent with how the state was saved. If the model configuration is different, the loaded state might not be consistent with the model.</p> <p>The model does not get update with the loaded state. To update the model, use the <code>update_gpstate</code> method with the loaded state.</p> <p>TODO: Implement loading the model configuration as well.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PosixPath</code> <p>The path where to load the GP state from.</p> required <p>Returns:</p> Type Description <code>GPState</code> <p>An instance of the GPState object, containing the loaded state.</p> Source code in <code>gallifrey/model.py</code> <pre><code>@classmethod\ndef load_state(\n    cls,\n    path: str | PosixPath,\n) -&gt; GPState:\n    \"\"\"\n    Load a GP state from file.\n\n    Note that only the state is loaded. It is assumed that the model\n    configuration is consistent with how the state was saved. If the\n    model configuration is different, the loaded state might not be\n    consistent with the model.\n\n    The model does not get update with the loaded state. To update the\n    model, use the `update_gpstate` method with the loaded state.\n\n    TODO: Implement loading the model configuration as well.\n\n    Parameters\n    ----------\n    path : str | PosixPath\n        The path where to load the GP state from.\n\n    Returns\n    -------\n    GPState\n        An instance of the GPState object, containing the loaded state.\n\n    \"\"\"\n    with open(path, \"rb\") as file:\n        gpstate = pickle.load(file)\n    return gpstate\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel.save_state","title":"<code>save_state(path, gpstate=None)</code>","text":"<p>Save a GP state to file.</p> <p>Note that only the state is saved and not the model itself. That means if the model is loaded with a different configuration (e.g. different kernel library), the state might not be consistent with the model.</p> <p>TODO: Implement saving the model configuration as well.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PosixPath</code> <p>The path where to save the GP state, must be an absolute path.</p> required <code>gpstate</code> <code>Optional[GPState]</code> <p>The GP state to save, If None, the current state of the model is used. By default None.</p> <code>None</code> Source code in <code>gallifrey/model.py</code> <pre><code>def save_state(\n    self,\n    path: str | PosixPath,\n    gpstate: tp.Optional[GPState] = None,\n) -&gt; None:\n    \"\"\"\n    Save a GP state to file.\n\n    Note that only the state is saved and not the model itself.\n    That means if the model is loaded with a different configuration\n    (e.g. different kernel library), the state might not be consistent\n    with the model.\n\n    TODO: Implement saving the model configuration as well.\n\n    Parameters\n    ----------\n    path : str | PosixPath\n        The path where to save the GP state, must\n        be an absolute path.\n    gpstate : tp.Optional[GPState], optional\n        The GP state to save, If None, the current state\n        of the model is used. By default None.\n\n    \"\"\"\n    if gpstate is None:\n        gpstate = self.state\n\n    with open(path, \"wb\") as file:\n        pickle.dump(gpstate, file)\n\n    return None\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.GPModel.update_state","title":"<code>update_state(gpstate)</code>","text":"<p>Update the GP state. Returns a new GPModel instance (no in-place update). If the number of particles in the new state is different from the current state, the num_particles attribute is updated.</p> <p>Note that no other attributes are updated. If the particle states were created using a different configuration, the model will not be consistent.</p> <p>Parameters:</p> Name Type Description Default <code>gpstate</code> <code>GPState</code> <p>The new GP state to update the model with.</p> required <p>Returns:</p> Type Description <code>GPModel</code> <p>A new GPModel instance with updated state.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def update_state(self, gpstate: GPState) -&gt; GPModel:\n    \"\"\"\n    Update the GP state. Returns a new GPModel instance (no in-place\n    update). If the number of particles in the new state is different\n    from the current state, the num_particles attribute is updated.\n\n    Note that no other attributes are updated. If the particle states\n    were created using a different configuration, the model will not be\n    consistent.\n\n    Parameters\n    ----------\n    gpstate : GPState\n        The new GP state to update the model with.\n\n    Returns\n    -------\n    GPModel\n        A new GPModel instance with updated state.\n\n    \"\"\"\n\n    new_gpmodel = deepcopy(self)\n    new_gpmodel.state = gpstate\n\n    return new_gpmodel\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.batch_states","title":"<code>batch_states(states)</code>","text":"<p>Batch a list of states (or general PyTrees) into a single state with batched parameters.</p> <p>This function takes a list of individual particle states (<code>states</code>) and transforms it into a single <code>nnx.State</code> object where each parameter is batched across all particles. This is useful for parallelized computations over particles.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>list[State]</code> <p>List of individual particle states.</p> required <p>Returns:</p> Type Description <code>State</code> <p>A single state object with parameters batched across particles.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def batch_states(states: list[PyTree]) -&gt; PyTree:\n    \"\"\"\n    Batch a list of states (or general PyTrees) into a single\n    state with batched parameters.\n\n    This function takes a list of individual particle states\n    (`states`) and transforms it into a single `nnx.State` object\n    where each parameter is batched across all particles. This is useful for\n    parallelized computations over particles.\n\n    Parameters\n    ----------\n    states : list[nnx.State]\n        List of individual particle states.\n\n    Returns\n    -------\n    nnx.State\n        A single state object with parameters batched across particles.\n    \"\"\"\n    return jtu.tree_map(lambda *xs: jnp.array(xs), *states)\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.initialize_particle_state","title":"<code>initialize_particle_state(kernel_state, kernel_prior, noise_variance, fix_noise)</code>","text":"<p>Initialize an instance of the particle class as a particle with a Gaussian likelihood and the provided input parameters, and returns the graphdef and state of the particle.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_state</code> <code>State</code> <p>The kernel state.</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior object, containing various useful methods and attributes for the kernel sampling. Attributes set through config. (See gallifrey.kernels.prior.KernelPrior for details.)</p> required <code>noise_variance</code> <code>ScalarFloat | Float[ndarray, ' D']</code> <p>The noise variance.</p> required <code>fix_noise</code> <code>bool</code> <p>If True, the noise variance is fixed and not trainable.</p> required <p>Returns:</p> Type Description <code>Particle</code> <p>The initialized particle instance.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def initialize_particle_state(\n    kernel_state: nnx.State,\n    kernel_prior: KernelPrior,\n    noise_variance: ScalarFloat | Float[jnp.ndarray, \" D\"],\n    fix_noise: bool,\n) -&gt; tuple[nnx.GraphDef, nnx.State]:\n    \"\"\"\n    Initialize an instance of the particle class as\n    a particle with a Gaussian likelihood and the provided input\n    parameters, and returns the graphdef and state of the particle.\n\n    Parameters\n    ----------\n    kernel_state : nnx.State\n        The kernel state.\n    kernel_prior : KernelPrior\n        The kernel prior object, containing various\n        useful methods and attributes for the kernel\n        sampling. Attributes set through config. (See\n        gallifrey.kernels.prior.KernelPrior for details.)\n    noise_variance : ScalarFloat | Float[jnp.ndarray, \" D\"]\n        The noise variance.\n    fix_noise : bool\n        If True, the noise variance is fixed and not trainable.\n\n    Returns\n    -------\n    Particle\n        The initialized particle instance.\n    \"\"\"\n    kernel = nnx.merge(kernel_prior.graphdef, kernel_state)\n\n    particle = Particle(\n        kernel=kernel,\n        noise_variance=noise_variance,\n        trainable_noise_variance=not fix_noise,\n    )\n\n    graphdef, state = nnx.split(particle)\n    return graphdef, state\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.sample_noise_variance","title":"<code>sample_noise_variance(key, data, n)</code>","text":"<p>Sample the noise variance. These are the noise variances assigned to the particles when creating the model without set noise_variance. The sample function is an InverseGamma distribution, with concentration = n/2 and scale = 1.0.</p> <p>NOTE: This is specifically not the same as the noise prior distribution, which is an InverseGamma(1, 1) distribution. The distribution here is chosen to increase the initial acceptance rate of the MCMC sampler.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for sampling.</p> required <code>data</code> <code>Dataset</code> <p>A dataset object containing the input x and output y.</p> required <code>n</code> <code>ScalarInt</code> <p>Number of samples to draw.</p> required <p>Returns:</p> Type Description <code> Float[jnp.ndarray, \" D\"]</code> <p>The sampled noise variance.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def sample_noise_variance(\n    key: PRNGKeyArray,\n    data: Dataset,\n    n: ScalarInt,\n) -&gt; Float[jnp.ndarray, \" D\"]:\n    \"\"\"\n    Sample the noise variance. These are the noise variances\n    assigned to the particles when creating the model without\n    set noise_variance. The sample function is an\n    InverseGamma distribution, with concentration = n/2 and\n    scale = 1.0.\n\n    NOTE: This is specifically not the same as the noise prior\n    distribution, which is an InverseGamma(1, 1) distribution.\n    The distribution here is chosen to increase the initial\n    acceptance rate of the MCMC sampler.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for sampling.\n    data : Dataset\n        A dataset object containing the input x and output y.\n    n : ScalarInt\n        Number of samples to draw.\n    Returns\n    -------\n     Float[jnp.ndarray, \" D\"]\n        The sampled noise variance.\n    \"\"\"\n\n    inv_gamma = InverseGamma(concentration=jnp.array(data.n / 2), scale=jnp.array(1.0))\n\n    return jnp.array(inv_gamma.sample(seed=key, sample_shape=(n,)))\n</code></pre>"},{"location":"autoapi/gallifrey/model/#gallifrey.model.unbatch_states","title":"<code>unbatch_states(state)</code>","text":"<p>Unbatch a single state (or general PyTree) with batched parameters into a list of individual states.</p> <p>This function takes a single <code>nnx.State</code> object where each parameter is batched across all particles and transforms it into a list of individual particle states.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>A single state object with parameters batched across particles.</p> required <p>Returns:</p> Type Description <code>list[State]</code> <p>List of individual particle states.</p> Source code in <code>gallifrey/model.py</code> <pre><code>def unbatch_states(state: PyTree) -&gt; list[PyTree]:\n    \"\"\"\n    Unbatch a single state (or general PyTree) with batched parameters\n    into a list of individual states.\n\n    This function takes a single `nnx.State` object where each parameter is\n    batched across all particles and transforms it into a list of individual particle\n    states.\n\n    Parameters\n    ----------\n    state : nnx.State\n        A single state object with parameters batched across particles.\n\n    Returns\n    -------\n    list[nnx.State]\n        List of individual particle states.\n    \"\"\"\n    number_of_states = len(jtu.tree_leaves(state)[0])\n    return [jtu.tree_map(lambda x: x[i], state) for i in range(number_of_states)]\n</code></pre>"},{"location":"autoapi/gallifrey/parameter/","title":"parameter","text":""},{"location":"autoapi/gallifrey/parameter/#gallifrey.parameter.KernelParameter","title":"<code>KernelParameter</code>","text":"<p>               Bases: <code>ParticleParameter</code></p> <p>A class to define the kernel parameter, which is a subclass of the <code>nnx.Variable</code> class. Similar to the Parameter class in GPJax, but holds an array of kernel parameters.</p> Source code in <code>gallifrey/parameter.py</code> <pre><code>class KernelParameter(ParticleParameter):\n    \"\"\"\n    A class to define the kernel parameter, which\n    is a subclass of the `nnx.Variable` class.\n    Similar to the Parameter class in GPJax, but holds\n    an array of kernel parameters.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value: Float[jnp.ndarray, \"M N\"],\n        **kwargs: tp.Any,\n    ):\n        \"\"\"\n        Initialize the ParticleParameter class.\n\n        Parameters\n        ----------\n        value : Float[jnp.ndarray, \"M N\"]\n            The array of kernel parameters, of shape\n            (max_leaves, max_atom_parameter)\n\n        \"\"\"\n        super().__init__(\n            value=jnp.asarray(value),\n            **kwargs,\n        )\n</code></pre>"},{"location":"autoapi/gallifrey/parameter/#gallifrey.parameter.KernelParameter.__init__","title":"<code>__init__(value, **kwargs)</code>","text":"<p>Initialize the ParticleParameter class.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Float[ndarray, 'M N']</code> <p>The array of kernel parameters, of shape (max_leaves, max_atom_parameter)</p> required Source code in <code>gallifrey/parameter.py</code> <pre><code>def __init__(\n    self,\n    value: Float[jnp.ndarray, \"M N\"],\n    **kwargs: tp.Any,\n):\n    \"\"\"\n    Initialize the ParticleParameter class.\n\n    Parameters\n    ----------\n    value : Float[jnp.ndarray, \"M N\"]\n        The array of kernel parameters, of shape\n        (max_leaves, max_atom_parameter)\n\n    \"\"\"\n    super().__init__(\n        value=jnp.asarray(value),\n        **kwargs,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/parameter/#gallifrey.parameter.NoiseParameter","title":"<code>NoiseParameter</code>","text":"<p>               Bases: <code>ParticleParameter</code></p> <p>Class to define the noise variance parameter.</p> Source code in <code>gallifrey/parameter.py</code> <pre><code>class NoiseParameter(ParticleParameter):\n    \"\"\"\n    Class to define the noise variance parameter.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        noise_variance: ScalarFloat | ScalarInt,\n        **kwargs: tp.Any,\n    ):\n        \"\"\"\n        Initialize the ParticleParameter class.\n\n        Parameters\n        ----------\n        noise_variance : ScalarFloat | ScalarInt\n            The value of the noise variance.\n\n        \"\"\"\n        super().__init__(\n            value=jnp.asarray(noise_variance),\n            **kwargs,\n        )\n</code></pre>"},{"location":"autoapi/gallifrey/parameter/#gallifrey.parameter.NoiseParameter.__init__","title":"<code>__init__(noise_variance, **kwargs)</code>","text":"<p>Initialize the ParticleParameter class.</p> <p>Parameters:</p> Name Type Description Default <code>noise_variance</code> <code>ScalarFloat | ScalarInt</code> <p>The value of the noise variance.</p> required Source code in <code>gallifrey/parameter.py</code> <pre><code>def __init__(\n    self,\n    noise_variance: ScalarFloat | ScalarInt,\n    **kwargs: tp.Any,\n):\n    \"\"\"\n    Initialize the ParticleParameter class.\n\n    Parameters\n    ----------\n    noise_variance : ScalarFloat | ScalarInt\n        The value of the noise variance.\n\n    \"\"\"\n    super().__init__(\n        value=jnp.asarray(noise_variance),\n        **kwargs,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/parameter/#gallifrey.parameter.ParticleParameter","title":"<code>ParticleParameter</code>","text":"<p>               Bases: <code>Variable</code></p> <p>A class to define the particle parameter, which is a subclass of the <code>nnx.Variable</code> class.</p> Source code in <code>gallifrey/parameter.py</code> <pre><code>class ParticleParameter(nnx.Variable):\n    \"\"\"\n    A class to define the particle parameter, which\n    is a subclass of the `nnx.Variable` class.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"autoapi/gallifrey/parameter/#gallifrey.parameter.move_parameters","title":"<code>move_parameters(kernel_state, mapping, max_depth, reset_unused=True)</code>","text":"<p>A function to move parameters from one leaf to another based on the mapping.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_state</code> <code>State</code> <p>The original kernel state, must contain the attribute: - parameters - max_depth - tree_expression - is_operator</p> required <code>mapping</code> <code>Float[ndarray, 'M N']</code> <p>The mapping between the old and new (level order) indices of the nodes in the tree. The first column contains the old indices, and the second column contains the new indices.</p> required <code>max_depth</code> <code>int</code> <p>The maximum depth of the tree.</p> required <code>reset_unused</code> <code>bool</code> <p>Whether to reset the unused parameters to a default fill value, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>State</code> <p>The kernel state with the moved parameters.</p> Source code in <code>gallifrey/parameter.py</code> <pre><code>@partial(jit, static_argnames=(\"max_depth\", \"reset_unused\"))\ndef move_parameters(\n    kernel_state: nnx.State,\n    mapping: Int[jnp.ndarray, \"M N\"],\n    max_depth: int,\n    reset_unused: bool = True,\n) -&gt; nnx.State:\n    \"\"\"\n    A function to move parameters from one leaf\n    to another based on the mapping.\n\n    Parameters\n    ----------\n    kernel_state : nnx.State\n        The original kernel state, must contain the\n        attribute:\n        - parameters\n        - max_depth\n        - tree_expression\n        - is_operator\n    mapping : Float[jnp.ndarray, \"M N\"]\n        The mapping between the old and new (level order) indices of the nodes\n        in the tree. The first column contains the old indices, and the second\n        column contains the new indices.\n    max_depth : int\n        The maximum depth of the tree.\n    reset_unused : bool, optional\n        Whether to reset the unused parameters to a default fill value,\n        by default True.\n\n    Returns\n    -------\n    nnx.State\n        The kernel state with the moved parameters.\n    \"\"\"\n    # get needed state attributes\n    max_nodes = int(calculate_max_nodes(max_depth))\n    tree_expression: jnp.ndarray = kernel_state.tree_expression.value  # type: ignore\n    is_operator: jnp.ndarray = kernel_state.is_operator.value  # type: ignore\n\n    # get two versions of the kernel parameters, one to update and\n    # to use for copying (in case some leaves are overwritten, before\n    # they are moved).\n    old_kernel_params = kernel_state[\"parameters\"].value\n    new_kernel_params = old_kernel_params.copy()  # type: ignore\n\n    def move_leaf_params(\n        new_kernel_params: jnp.ndarray,\n        indices: jnp.ndarray,\n    ) -&gt; jnp.ndarray:\n        \"\"\"Inner function to move the leaf parameters.\"\"\"\n\n        old_idx, new_idx = indices\n\n        # check whether the moved node is an operator or an atom,\n        # only atoms have parameters that can be moved\n        is_atom = is_operator[tree_expression[new_idx]]\n\n        def do_move(new_kernel_params: jnp.ndarray) -&gt; jnp.ndarray:\n            \"\"\"Move parameters for atom nodes.\"\"\"\n\n            # get old and new leaf indices\n            new_leaf_idx = get_parameter_leaf_idx(new_idx, max_depth)\n            old_leaf_idx = get_parameter_leaf_idx(old_idx, max_depth)\n\n            # map old parameter to new location\n            new_kernel_params = new_kernel_params.at[new_leaf_idx].set(\n                old_kernel_params[old_leaf_idx]\n            )\n\n            return new_kernel_params\n\n        def dont_move(new_kernel_params: jnp.ndarray) -&gt; jnp.ndarray:\n            \"\"\"Do not move parameters for operator nodes.\"\"\"\n            return new_kernel_params\n\n        new_kernel_params = lax.cond(\n            is_atom,\n            lambda x: dont_move(x),\n            lambda x: do_move(x),\n            new_kernel_params,\n        )\n        return new_kernel_params\n\n    def loop_body(loop_state: tuple) -&gt; tuple:\n        \"\"\"Loop over indices in mapping, update kernel parameters.\"\"\"\n        new_kernel_params, i = loop_state\n        indices = mapping[i]\n        new_kernel_params = move_leaf_params(new_kernel_params, indices)\n        return new_kernel_params, i + 1\n\n    def condition(loop_state: tuple) -&gt; Bool[jnp.ndarray, \" \"]:\n        \"\"\"Break when all moves are done, assuming empty slots are\n        indicated by negative indices (and that mapping is sorted).\"\"\"\n        _, i = loop_state\n        indices = mapping[i]\n        return indices[0] &gt;= 0\n\n    new_kernel_params, _ = bounded_while_loop(\n        condition,\n        loop_body,\n        (new_kernel_params, 0),\n        max_steps=max_nodes,\n        kind=\"bounded\",\n    )\n\n    # update kernel state\n    kernel_state[\"parameters\"] = kernel_state[\"parameters\"].replace(  # type: ignore\n        new_kernel_params,\n    )\n\n    if reset_unused:\n        # reset now empty leaf parameter back to -1\n        kernel_state = reset_unused_parameters(\n            kernel_state,\n            max_nodes,\n        )\n\n    return kernel_state\n</code></pre>"},{"location":"autoapi/gallifrey/parameter/#gallifrey.parameter.reset_unused_parameters","title":"<code>reset_unused_parameters(kernel_state, max_leaves, fill_value=-1.0)</code>","text":"<p>Reset parameter in parmaeter array for unused leaf nodes with default fill value. Leaves might still contain values after e.g. a move operation, this function resets them.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_state</code> <code>State</code> <p>The kernel state, must contain the attribute: - parameters - leaf_level_map</p> required <code>max_leaves</code> <code>int</code> <p>The maximum number of leaf nodes.</p> required <code>fill_value</code> <code>ScalarFloat</code> <p>The new value to fill the leaves with, by default -1.0.</p> <code>-1.0</code> <p>Returns:</p> Type Description <code>State</code> <p>The kernel state with the reset parameters.</p> Source code in <code>gallifrey/parameter.py</code> <pre><code>@partial(jit, static_argnames=(\"max_leaves\"))\ndef reset_unused_parameters(\n    kernel_state: nnx.State,\n    max_leaves: int,\n    fill_value: ScalarFloat = -1.0,\n) -&gt; nnx.State:\n    \"\"\"\n    Reset parameter in parmaeter array for unused leaf nodes\n    with default fill value. Leaves might still contain values\n    after e.g. a move operation, this function resets them.\n\n    Parameters\n    ----------\n    kernel_state : nnx.State\n        The kernel state, must contain the attribute:\n        - parameters\n        - leaf_level_map\n    max_leaves : int\n        The maximum number of leaf nodes.\n    fill_value : ScalarFloat, optional\n        The new value to fill the leaves with, by default -1.0.\n\n    Returns\n    -------\n    nnx.State\n        The kernel state with the reset parameters.\n    \"\"\"\n\n    leaf_level_map: jnp.ndarray = kernel_state.leaf_level_map.value  # type: ignore\n    kernel_params: jnp.ndarray = kernel_state.parameters.value  # type: ignore\n\n    def reset_scan_func(kernel_params: jnp.ndarray, i: ScalarInt) -&gt; tuple:\n        \"\"\"Scan over leafs, if unused reset to fill_value.\"\"\"\n        kernel_params = lax.cond(\n            leaf_level_map[i] &gt;= 0,\n            lambda: kernel_params,\n            lambda: kernel_params.at[i].set(fill_value),\n        )\n        return kernel_params, i\n\n    kernel_params, _ = lax.scan(\n        reset_scan_func,\n        kernel_params,\n        jnp.arange(max_leaves),\n    )\n\n    kernel_state[\"parameters\"] = kernel_state[\"parameters\"].replace(  # type: ignore\n        kernel_params,\n    )\n    return kernel_state\n</code></pre>"},{"location":"autoapi/gallifrey/parameter/#gallifrey.parameter.transform_kernel_parameters","title":"<code>transform_kernel_parameters(kernel_state, num_parameter_array, max_leaves, max_atom_parameters, support_mapping_array, bijectors, inverse=False)</code>","text":"<p>A function takes a kernel state and transforms its parameter based on the support mapping and bijectors.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_state</code> <code>State</code> <p>The original kernel state to be filled with new parameters. The state must contain the following attributes: - tree_expression: The tree expression that describes the kernel structure. - leaf_level_map: A array that maps the index of the leaf parameter array - parameters: The kernel parameters to be transformed.</p> required <code>num_parameter_array</code> <code>Int[ndarray, ' D']</code> <p>An array that contains the number of parameters for each atom in the kernel structure. Needs to be in same order as the atom library.</p> required <code>max_leaves</code> <code>int</code> <p>The maximum number of leaves in the tree.</p> required <code>max_atom_parameters</code> <code>int</code> <p>The maximum number of parameters any atom will take.</p> required <code>support_mapping_array</code> <code>Int[ndarray, 'M N']</code> <p>An array that maps the support of the parameters to the corresponding bijector index.</p> required <code>bijectors</code> <code>tuple[Bijector, ...]</code> <p>A tuple of bijectors to transform the parameters to the desired support. Must be tensorflow bijectors with forward and inverse methods.</p> required <code>inverse</code> <code>ScalarBool</code> <p>If True, the inverse transformation is applied, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>State</code> <p>The state with transformed parameters.</p> Source code in <code>gallifrey/parameter.py</code> <pre><code>@partial(\n    jit,\n    static_argnames=(\n        \"max_leaves\",\n        \"max_atom_parameters\",\n        \"bijectors\",\n    ),\n)\ndef transform_kernel_parameters(\n    kernel_state: nnx.State,\n    num_parameter_array: Int[jnp.ndarray, \" D\"],\n    max_leaves: int,\n    max_atom_parameters: int,\n    support_mapping_array: Int[jnp.ndarray, \"M N\"],\n    bijectors: tuple[tfb.Bijector, ...],\n    inverse: ScalarBool = False,\n) -&gt; nnx.State:\n    \"\"\"\n    A function takes a kernel state and transforms its parameter\n    based on the support mapping and bijectors.\n\n    Parameters\n    ----------\n    kernel_state : nnx.State\n        The original kernel state to be filled with new parameters.\n        The state must contain the following attributes:\n        - tree_expression: The tree expression that describes the kernel structure.\n        - leaf_level_map: A array that maps the index of the leaf parameter array\n        - parameters: The kernel parameters to be transformed.\n    num_parameter_array : Int[jnp.ndarray, \" D\"]\n        An array that contains the number of parameters for each\n        atom in the kernel structure. Needs to be in same order\n        as the atom library.\n    max_leaves : int\n        The maximum number of leaves in the tree.\n    max_atom_parameters : int\n        The maximum number of parameters any atom will take.\n    support_mapping_array : Int[jnp.ndarray, \"M N\"]\n        An array that maps the support of the parameters to the\n        corresponding bijector index.\n    bijectors : tuple[tp.Bijector, ...]\n        A tuple of bijectors to transform the parameters to the\n        desired support. Must be tensorflow bijectors with forward\n        and inverse methods.\n    inverse : ScalarBool, optional\n        If True, the inverse transformation is applied, by default False.\n\n    Returns\n    -------\n    nnx.State\n        The state with transformed parameters.\n\n    \"\"\"\n\n    tree_expression: jnp.ndarray = kernel_state.tree_expression.value  # type: ignore\n    leaf_level_map: jnp.ndarray = kernel_state.leaf_level_map.value  # type: ignore\n\n    forward_bijectors = [bijector.forward for bijector in bijectors]\n    inverse_bijectors = [bijector.inverse for bijector in bijectors]\n\n    def transform_forward(idx_and_param: tuple[ScalarInt, ScalarFloat]) -&gt; ScalarFloat:\n        \"\"\"Forward transform based on idx (in support array).\"\"\"\n        idx, param = idx_and_param\n        return lax.switch(\n            idx,\n            forward_bijectors,\n            param,\n        )\n\n    def transform_inverse(idx_and_param: tuple[ScalarInt, ScalarFloat]) -&gt; ScalarFloat:\n        \"\"\"Inverse transform based on idx (in support array).\"\"\"\n        idx, param = idx_and_param\n        return lax.switch(\n            idx,\n            inverse_bijectors,\n            param,\n        )\n\n    def process_params(\n        kernel_parameters: Float[jnp.ndarray, \"M N\"],\n        indices: Int[jnp.ndarray, \"2\"],\n    ) -&gt; tuple[\n        Float[jnp.ndarray, \"M N\"],\n        Int[jnp.ndarray, \"2\"],\n    ]:\n        \"\"\"Process parameter based on if it's active and considered.\"\"\"\n\n        # unpack indices\n        leaf_idx, parameter_idx = indices\n\n        # check whether the parameter is active\n        node_value = tree_expression[leaf_level_map[leaf_idx]]\n        atom_num_parameters = num_parameter_array[node_value]\n\n        is_active_node = leaf_level_map[leaf_idx] &gt;= 0\n        is_active_param = parameter_idx &lt; atom_num_parameters\n        is_active = is_active_node &amp; is_active_param\n\n        def process_active(indices: tuple) -&gt; ScalarFloat:\n            \"\"\"If active, transform parameter.\"\"\"\n            leaf_idx, parameter_idx = indices\n\n            # select parameter and transform back to standard normal\n            param = kernel_parameters[leaf_idx, parameter_idx]\n\n            transformed_param = lax.cond(\n                inverse,\n                transform_inverse,\n                transform_forward,\n                (support_mapping_array[node_value, parameter_idx], param),\n            )\n            return transformed_param\n\n        def process_inactive(indices: tuple) -&gt; ScalarFloat:\n            \"\"\"If inactive, return as is.\"\"\"\n            return kernel_parameters[leaf_idx, parameter_idx]\n\n        # process parameter, get transformed parameter\n        transformed_parameter = lax.cond(\n            is_active,\n            process_active,\n            process_inactive,\n            (leaf_idx, parameter_idx),\n        )\n\n        # update kernel parameter\n        kernel_parameters = kernel_parameters.at[leaf_idx, parameter_idx].set(\n            transformed_parameter\n        )\n\n        return kernel_parameters, indices\n\n    # get initial kernel parameters\n    kernel_parameters: jnp.ndarray = kernel_state.parameters.value  # type: ignore\n\n    # get all parameter index combinations\n    parameter_indices = jnp.indices((max_leaves, max_atom_parameters)).reshape(2, -1).T\n\n    # scan over all parameters and update kernel parameters\n    transformed_kernel_parameters, _ = lax.scan(\n        process_params,\n        kernel_parameters,\n        parameter_indices,\n    )\n\n    kernel_state[\"parameters\"] = kernel_state[\"parameters\"].replace(\n        transformed_kernel_parameters,\n    )  # type: ignore\n\n    return kernel_state\n</code></pre>"},{"location":"autoapi/gallifrey/particles/","title":"particles","text":""},{"location":"autoapi/gallifrey/particles/#gallifrey.particles.Particle","title":"<code>Particle</code>","text":"<p>               Bases: <code>Module</code></p> <p>The <code>Particle</code> class. This class is based on of the GPJax <code>ConjugatePosterior</code>.</p> <p>It takes a kernel (instance of <code>TreeKernel</code>), noise variance (float), and jitter (float) as input. The jitter is used to ensure that the covariance matrix is positive definite (avoiding numerical instabilities in case of small eigenvalues).</p> <p>In contrast to the <code>ConjugatePosterior</code> class, the <code>Particle</code> currently has more limited features, specifically we     - do not support mean functions (that implies a zero mean function,       in use cases the data should be appropriately centered).     - do not support likelihoods other than Gaussian.</p> <p>Attributes:</p> Name Type Description <code>kernel</code> <code>TreeKernel</code> <p>The kernel defining the covariance function of the Gaussian process. Must be a <code>TreeKernel</code> instance (from <code>gallifrey.kernels</code>).</p> <code>noise_variance</code> <code>Variable | NoiseParameter</code> <p>The (observational) noise variance of the Gaussian process. Depending on the value of the <code>trainable_noise_variance</code> parameter during initialization, the noise variance will be either considered a trainable parameter (instance of <code>NoiseParameter</code>) or a fixed parameter (instance of <code>nnx.Variable</code>).</p> <code>jitter</code> <code>ScalarFloat</code> <p>The jitter term to ensure numerical stability of the covariance matrix.</p> Source code in <code>gallifrey/particles.py</code> <pre><code>class Particle(nnx.Module):\n    \"\"\"\n    The `Particle` class. This class is based on of the GPJax\n    `ConjugatePosterior`.\n\n    It takes a kernel (instance of `TreeKernel`), noise variance\n    (float), and jitter (float) as input. The jitter is used to\n    ensure that the covariance matrix is positive definite (avoiding\n    numerical instabilities in case of small eigenvalues).\n\n    In contrast to the `ConjugatePosterior` class, the `Particle` currently\n    has more limited features, specifically we\n        - do not support mean functions (that implies a zero mean function,\n          in use cases the data should be appropriately centered).\n        - do not support likelihoods other than Gaussian.\n\n    Attributes\n    ----------\n    kernel : TreeKernel\n        The kernel defining the covariance function of the Gaussian\n        process. Must be a `TreeKernel` instance (from `gallifrey.kernels`).\n\n    noise_variance : nnx.Variable | NoiseParameter\n        The (observational) noise variance of the Gaussian process. Depending\n        on the value of the `trainable_noise_variance` parameter during initialization,\n        the noise variance will be either considered a trainable parameter (instance\n        of `NoiseParameter`) or a fixed parameter (instance of `nnx.Variable`).\n\n    jitter : ScalarFloat\n        The jitter term to ensure numerical stability of the covariance\n        matrix.\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel: TreeKernel,\n        noise_variance: ScalarFloat,\n        trainable_noise_variance: bool = False,\n        jitter: float = 1e-6,\n    ):\n        \"\"\"\n        Initialize the particle.\n\n        Parameters\n        ----------\n        kernel : TreeKernel\n            The kernel defining the covariance function of the Gaussian\n            process. Must be a `TreeKernel` instance (from `gallifrey.kernels`).\n        noise_variance : ScalarFloat\n            The (observational) noise variance of the Gaussian process.\n        trainable_noise_variance : bool, optional\n            Whether to treat the noise variance as a trainable parameter,\n            by default False.\n        jitter : float, optional\n            The jitter term to ensure numerical stability of the covariance\n            matrix, by default 1e-6.\n        \"\"\"\n        self.kernel = kernel\n        noise_variance = jnp.asarray(noise_variance)\n        if trainable_noise_variance:\n            self.noise_variance = NoiseParameter(noise_variance)  # type: ignore\n        else:\n            self.noise_variance = nnx.Variable(noise_variance)  # type: ignore\n\n        self.jitter = jitter\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Print a string representation of the particle.\n\n        Returns\n        -------\n        str\n            A string representation of the particle.\n        \"\"\"\n        variance = f\"Noise Variance : {self.noise_variance.value}\"\n        kernel = self.kernel.__str__()\n\n        return f\"{variance}\\n{kernel}\"\n\n    def display(self) -&gt; None:\n        \"\"\"\n        Display the particle.\n        \"\"\"\n        print(self.__str__())\n\n    def predictive_distribution(\n        self,\n        xpredict: Float[Array, \" D\"],\n        data: Dataset,\n        latent: bool = False,\n    ) -&gt; Distribution:\n        \"\"\"\n        Calculate the predictive distribution of the Gaussian process\n        for the particle.\n\n        Inputs will be the points to predict and the training data that\n        the GP gets conditioned on.\n\n        The distribution returned will be a MultivariateNormalFullCovariance\n        distribution from `tensorflow_probability.substrates.jax.distributions`, see\n        https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalFullCovariance\n\n        If `latent` is True, the predictive distribution of the latent\n        function is returned, i.e. the distribution of the function\n        values without the observational noise. If False, the predictive\n        distribution of the full data-generating model is returned, which\n        includes the observational noise\n\n\n        Parameters\n        ----------\n        xpredict : Float[Array, \" D\"]\n            The points to predict, as a 1D array.\n        data : Dataset\n            The training data that the GP is conditioned on,\n            must be a `Dataset` instance from `gallifrey.data`.\n        latent : bool, optional\n            Whether to return the predictive distribution of the latent\n            function (without observational noise), by default False.\n\n        Returns\n        -------\n        Distribution\n            A tensorflow probability distribution object representing\n            the predictive distribution of the Gaussian process. (Specifically,\n            a MultivariateNormalFullCovariance distribution from\n            `tensorflow_probability.substrates.jax.distributions`).\n        \"\"\"\n\n        # unpack conditioning data\n        xtrain, ytrain = data.x, data.y\n\n        # the points to be predicted, as correct type\n        t = jnp.asarray(xpredict)\n\n        # calculate covariance matrix (\u03a3 = Kxx + Io\u00b2) for conditioning data\n        Kxx = self.kernel.gram(xtrain)\n        Sigma = calculate_covariance_matrix(\n            Kxx,\n            self.noise_variance.value,\n            self.jitter,\n        )\n\n        # calculate gram and cross_covariance for prediction points\n        Ktt = self.kernel.gram(t)\n        Kxt = self.kernel.cross_covariance(xtrain, t)\n\n        # solve for \u03a3\u207b\u00b9Kxt | [len(xtrain),len(t)] using cholesky decomposition\n        cho_fac = linalg.cho_factor(Sigma)\n        Sigma_inv_Kxt = linalg.cho_solve(cho_fac, Kxt)\n\n        # calculate predictive mean  Ktx (Kxx + Io\u00b2)\u207b\u00b9y (assumes zero mean function)\n        predictive_mean = jnp.matmul(Sigma_inv_Kxt.T, ytrain)\n\n        # calculate latent covariance function  Ktt  -  Ktx (Kxx + Io\u00b2)\u207b\u00b9 Kxt\n        latent_covariance = Ktt - jnp.matmul(Kxt.T, Sigma_inv_Kxt)\n        latent_covariance += jnp.eye(latent_covariance.shape[0]) * self.jitter\n\n        # The covariance matrix that we've calculated so far, is the covariance\n        # of the latent distribution as estimated by the Gaussian process. To\n        # get the full covariance matrix of the model (for the data generating\n        # process), we need to add the noise observational noise variance.\n        if latent:\n            predictive_covariance = latent_covariance\n        else:\n            # add noise variance to the diagonal of the covariance matrix\n            predictive_covariance = (\n                latent_covariance\n                + jnp.eye(latent_covariance.shape[0]) * self.noise_variance\n            )\n\n        return MultivariateNormalFullCovariance(\n            predictive_mean,\n            predictive_covariance,\n        )\n</code></pre>"},{"location":"autoapi/gallifrey/particles/#gallifrey.particles.Particle.__init__","title":"<code>__init__(kernel, noise_variance, trainable_noise_variance=False, jitter=1e-06)</code>","text":"<p>Initialize the particle.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>TreeKernel</code> <p>The kernel defining the covariance function of the Gaussian process. Must be a <code>TreeKernel</code> instance (from <code>gallifrey.kernels</code>).</p> required <code>noise_variance</code> <code>ScalarFloat</code> <p>The (observational) noise variance of the Gaussian process.</p> required <code>trainable_noise_variance</code> <code>bool</code> <p>Whether to treat the noise variance as a trainable parameter, by default False.</p> <code>False</code> <code>jitter</code> <code>float</code> <p>The jitter term to ensure numerical stability of the covariance matrix, by default 1e-6.</p> <code>1e-06</code> Source code in <code>gallifrey/particles.py</code> <pre><code>def __init__(\n    self,\n    kernel: TreeKernel,\n    noise_variance: ScalarFloat,\n    trainable_noise_variance: bool = False,\n    jitter: float = 1e-6,\n):\n    \"\"\"\n    Initialize the particle.\n\n    Parameters\n    ----------\n    kernel : TreeKernel\n        The kernel defining the covariance function of the Gaussian\n        process. Must be a `TreeKernel` instance (from `gallifrey.kernels`).\n    noise_variance : ScalarFloat\n        The (observational) noise variance of the Gaussian process.\n    trainable_noise_variance : bool, optional\n        Whether to treat the noise variance as a trainable parameter,\n        by default False.\n    jitter : float, optional\n        The jitter term to ensure numerical stability of the covariance\n        matrix, by default 1e-6.\n    \"\"\"\n    self.kernel = kernel\n    noise_variance = jnp.asarray(noise_variance)\n    if trainable_noise_variance:\n        self.noise_variance = NoiseParameter(noise_variance)  # type: ignore\n    else:\n        self.noise_variance = nnx.Variable(noise_variance)  # type: ignore\n\n    self.jitter = jitter\n</code></pre>"},{"location":"autoapi/gallifrey/particles/#gallifrey.particles.Particle.__str__","title":"<code>__str__()</code>","text":"<p>Print a string representation of the particle.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the particle.</p> Source code in <code>gallifrey/particles.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Print a string representation of the particle.\n\n    Returns\n    -------\n    str\n        A string representation of the particle.\n    \"\"\"\n    variance = f\"Noise Variance : {self.noise_variance.value}\"\n    kernel = self.kernel.__str__()\n\n    return f\"{variance}\\n{kernel}\"\n</code></pre>"},{"location":"autoapi/gallifrey/particles/#gallifrey.particles.Particle.display","title":"<code>display()</code>","text":"<p>Display the particle.</p> Source code in <code>gallifrey/particles.py</code> <pre><code>def display(self) -&gt; None:\n    \"\"\"\n    Display the particle.\n    \"\"\"\n    print(self.__str__())\n</code></pre>"},{"location":"autoapi/gallifrey/particles/#gallifrey.particles.Particle.predictive_distribution","title":"<code>predictive_distribution(xpredict, data, latent=False)</code>","text":"<p>Calculate the predictive distribution of the Gaussian process for the particle.</p> <p>Inputs will be the points to predict and the training data that the GP gets conditioned on.</p> <p>The distribution returned will be a MultivariateNormalFullCovariance distribution from <code>tensorflow_probability.substrates.jax.distributions</code>, see https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalFullCovariance</p> <p>If <code>latent</code> is True, the predictive distribution of the latent function is returned, i.e. the distribution of the function values without the observational noise. If False, the predictive distribution of the full data-generating model is returned, which includes the observational noise</p> <p>Parameters:</p> Name Type Description Default <code>xpredict</code> <code>Float[Array, ' D']</code> <p>The points to predict, as a 1D array.</p> required <code>data</code> <code>Dataset</code> <p>The training data that the GP is conditioned on, must be a <code>Dataset</code> instance from <code>gallifrey.data</code>.</p> required <code>latent</code> <code>bool</code> <p>Whether to return the predictive distribution of the latent function (without observational noise), by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Distribution</code> <p>A tensorflow probability distribution object representing the predictive distribution of the Gaussian process. (Specifically, a MultivariateNormalFullCovariance distribution from <code>tensorflow_probability.substrates.jax.distributions</code>).</p> Source code in <code>gallifrey/particles.py</code> <pre><code>def predictive_distribution(\n    self,\n    xpredict: Float[Array, \" D\"],\n    data: Dataset,\n    latent: bool = False,\n) -&gt; Distribution:\n    \"\"\"\n    Calculate the predictive distribution of the Gaussian process\n    for the particle.\n\n    Inputs will be the points to predict and the training data that\n    the GP gets conditioned on.\n\n    The distribution returned will be a MultivariateNormalFullCovariance\n    distribution from `tensorflow_probability.substrates.jax.distributions`, see\n    https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalFullCovariance\n\n    If `latent` is True, the predictive distribution of the latent\n    function is returned, i.e. the distribution of the function\n    values without the observational noise. If False, the predictive\n    distribution of the full data-generating model is returned, which\n    includes the observational noise\n\n\n    Parameters\n    ----------\n    xpredict : Float[Array, \" D\"]\n        The points to predict, as a 1D array.\n    data : Dataset\n        The training data that the GP is conditioned on,\n        must be a `Dataset` instance from `gallifrey.data`.\n    latent : bool, optional\n        Whether to return the predictive distribution of the latent\n        function (without observational noise), by default False.\n\n    Returns\n    -------\n    Distribution\n        A tensorflow probability distribution object representing\n        the predictive distribution of the Gaussian process. (Specifically,\n        a MultivariateNormalFullCovariance distribution from\n        `tensorflow_probability.substrates.jax.distributions`).\n    \"\"\"\n\n    # unpack conditioning data\n    xtrain, ytrain = data.x, data.y\n\n    # the points to be predicted, as correct type\n    t = jnp.asarray(xpredict)\n\n    # calculate covariance matrix (\u03a3 = Kxx + Io\u00b2) for conditioning data\n    Kxx = self.kernel.gram(xtrain)\n    Sigma = calculate_covariance_matrix(\n        Kxx,\n        self.noise_variance.value,\n        self.jitter,\n    )\n\n    # calculate gram and cross_covariance for prediction points\n    Ktt = self.kernel.gram(t)\n    Kxt = self.kernel.cross_covariance(xtrain, t)\n\n    # solve for \u03a3\u207b\u00b9Kxt | [len(xtrain),len(t)] using cholesky decomposition\n    cho_fac = linalg.cho_factor(Sigma)\n    Sigma_inv_Kxt = linalg.cho_solve(cho_fac, Kxt)\n\n    # calculate predictive mean  Ktx (Kxx + Io\u00b2)\u207b\u00b9y (assumes zero mean function)\n    predictive_mean = jnp.matmul(Sigma_inv_Kxt.T, ytrain)\n\n    # calculate latent covariance function  Ktt  -  Ktx (Kxx + Io\u00b2)\u207b\u00b9 Kxt\n    latent_covariance = Ktt - jnp.matmul(Kxt.T, Sigma_inv_Kxt)\n    latent_covariance += jnp.eye(latent_covariance.shape[0]) * self.jitter\n\n    # The covariance matrix that we've calculated so far, is the covariance\n    # of the latent distribution as estimated by the Gaussian process. To\n    # get the full covariance matrix of the model (for the data generating\n    # process), we need to add the noise observational noise variance.\n    if latent:\n        predictive_covariance = latent_covariance\n    else:\n        # add noise variance to the diagonal of the covariance matrix\n        predictive_covariance = (\n            latent_covariance\n            + jnp.eye(latent_covariance.shape[0]) * self.noise_variance\n        )\n\n    return MultivariateNormalFullCovariance(\n        predictive_mean,\n        predictive_covariance,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/particles/#gallifrey.particles.transform_particle_parameters","title":"<code>transform_particle_parameters(particle_state, kernel_prior, inverse=False)</code>","text":"<p>Transform parameter of a particle state (kernel parameters and noise variance) based on the support mapping and bijectors.</p> <p>This function is primarily used to transform the parameters between a constrained and unconstrained space.</p> <p>Parameters:</p> Name Type Description Default <code>particle_state</code> <code>State</code> <p>The original particle state.</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior that contains the support mapping and bijectors.</p> required <code>inverse</code> <code>ScalarBool</code> <p>If True, the inverse transformation is applied, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>State</code> <p>The particle state with transformed parameters.</p> Source code in <code>gallifrey/particles.py</code> <pre><code>def transform_particle_parameters(\n    particle_state: nnx.State,\n    kernel_prior: KernelPrior,\n    inverse: ScalarBool = False,\n) -&gt; nnx.State:\n    \"\"\"\n    Transform parameter of a particle state (kernel parameters\n    and noise variance) based on the support mapping and bijectors.\n\n    This function is primarily used to transform the parameters\n    between a constrained and unconstrained space.\n\n    Parameters\n    ----------\n    particle_state : nnx.State\n        The original particle state.\n    kernel_prior : KernelPrior\n        The kernel prior that contains the support mapping and bijectors.\n    inverse : ScalarBool, optional\n        If True, the inverse transformation is applied, by default False.\n\n    Returns\n    -------\n    nnx.State\n        The particle state with transformed parameters.\n    \"\"\"\n    # get parameters and state\n    num_parameter_array = kernel_prior.parameter_prior.num_parameter_array\n    max_leaves = kernel_prior.parameter_prior.max_leaves\n    max_atom_parameters = kernel_prior.parameter_prior.max_atom_parameters\n    support_mapping_array = kernel_prior.parameter_prior.support_mapping_array\n    support_bijectors = kernel_prior.support_bijectors\n\n    kernel_state = particle_state.kernel\n    noise_variance = jnp.array(particle_state.noise_variance.value)  # type: ignore\n\n    # transform the noise standard deviation (using softplus bijector)\n    transformed_noise_variance = lax.cond(\n        inverse,\n        SOFTPLUS_BIJECTOR.inverse,\n        SOFTPLUS_BIJECTOR.forward,\n        noise_variance,\n    )\n\n    # transform the kernel parameters\n    transformed_kernel_state = transform_kernel_parameters(\n        kernel_state,\n        num_parameter_array,\n        max_leaves,\n        max_atom_parameters,\n        support_mapping_array,\n        support_bijectors,\n        inverse,\n    )\n\n    # create a new state with the transformed parameters\n    new_state = deepcopy(particle_state)\n    new_state.kernel = transformed_kernel_state  # type: ignore\n    new_state.noise_variance = (  # type: ignore\n        particle_state.noise_variance.replace(\n            transformed_noise_variance,\n        )  # type: ignore\n    )\n\n    return new_state\n</code></pre>"},{"location":"autoapi/gallifrey/schedule/","title":"schedule","text":""},{"location":"autoapi/gallifrey/schedule/#gallifrey.schedule.LinearSchedule","title":"<code>LinearSchedule</code>","text":"<p>               Bases: <code>Schedule</code></p> <p>Linear scheduler, adds roughly <code>n * percent</code> new observations at each step.</p> Source code in <code>gallifrey/schedule.py</code> <pre><code>class LinearSchedule(Schedule):\n    \"\"\"\n    Linear scheduler, adds roughly\n    `n * percent` new observations at each step.\n\n    \"\"\"\n\n    @staticmethod\n    def generate(\n        num_datapoints: int,\n        num_steps: int,\n        start: int = 1,\n    ) -&gt; tuple[int, ...]:\n        \"\"\"\n        Generate a linear annealing schedule.\n\n        Parameters\n        ----------\n        num_datapoints : int\n            The total number of datapoints.\n        num_steps : int\n            The number of steps in the schedule.\n        start : int, optional\n            The starting point of the schedule,\n            by default 1 (one observation).\n\n        Returns\n        -------\n        tuple[int, ...]\n            A tuple of integers representing the cumulative number of\n            observations at each step of the schedule. Length of the\n            tuple is `num_steps`.\n\n        \"\"\"\n        # if only one step, run total number of datapoints\n        if num_steps == 1:\n            start = num_datapoints\n\n        return tuple(\n            jnp.round(\n                jnp.linspace(\n                    start=start,\n                    stop=num_datapoints,\n                    num=num_steps,\n                    endpoint=True,\n                    dtype=float,\n                ),\n            )\n            .astype(int)\n            .tolist()\n        )\n</code></pre>"},{"location":"autoapi/gallifrey/schedule/#gallifrey.schedule.LinearSchedule.generate","title":"<code>generate(num_datapoints, num_steps, start=1)</code>  <code>staticmethod</code>","text":"<p>Generate a linear annealing schedule.</p> <p>Parameters:</p> Name Type Description Default <code>num_datapoints</code> <code>int</code> <p>The total number of datapoints.</p> required <code>num_steps</code> <code>int</code> <p>The number of steps in the schedule.</p> required <code>start</code> <code>int</code> <p>The starting point of the schedule, by default 1 (one observation).</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>A tuple of integers representing the cumulative number of observations at each step of the schedule. Length of the tuple is <code>num_steps</code>.</p> Source code in <code>gallifrey/schedule.py</code> <pre><code>@staticmethod\ndef generate(\n    num_datapoints: int,\n    num_steps: int,\n    start: int = 1,\n) -&gt; tuple[int, ...]:\n    \"\"\"\n    Generate a linear annealing schedule.\n\n    Parameters\n    ----------\n    num_datapoints : int\n        The total number of datapoints.\n    num_steps : int\n        The number of steps in the schedule.\n    start : int, optional\n        The starting point of the schedule,\n        by default 1 (one observation).\n\n    Returns\n    -------\n    tuple[int, ...]\n        A tuple of integers representing the cumulative number of\n        observations at each step of the schedule. Length of the\n        tuple is `num_steps`.\n\n    \"\"\"\n    # if only one step, run total number of datapoints\n    if num_steps == 1:\n        start = num_datapoints\n\n    return tuple(\n        jnp.round(\n            jnp.linspace(\n                start=start,\n                stop=num_datapoints,\n                num=num_steps,\n                endpoint=True,\n                dtype=float,\n            ),\n        )\n        .astype(int)\n        .tolist()\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/schedule/#gallifrey.schedule.LogSchedule","title":"<code>LogSchedule</code>","text":"<p>               Bases: <code>Schedule</code></p> <p>A logarithmic scheduler, adds observations in a logarithmic fashion.</p> Source code in <code>gallifrey/schedule.py</code> <pre><code>class LogSchedule(Schedule):\n    \"\"\"\n    A logarithmic scheduler, adds observations in a logarithmic fashion.\n\n    \"\"\"\n\n    @staticmethod\n    def generate(\n        num_datapoints: int,\n        num_steps: int,\n        start: int = 1,\n    ) -&gt; tuple[int, ...]:\n        \"\"\"\n        Generate a logarithmic annealing schedule.\n\n        Parameters\n        ----------\n        num_datapoints : int\n            The total number of datapoints.\n        num_steps : int\n            The number of steps in the schedule.\n        start : int, optional\n            The starting point of the schedule,\n            by default 1 (one observation).\n\n        Returns\n        -------\n        tuple[int, ...]\n            A tuple of integers representing the cumulative number of\n            observations at each step of the schedule. Length of the\n            tuple is `num_steps`.\n        \"\"\"\n        # if only one step, run total number of datapoints\n        if num_steps == 1:\n            start = num_datapoints\n\n        return tuple(\n            jnp.round(\n                jnp.geomspace(\n                    start=start,\n                    stop=num_datapoints,\n                    num=num_steps,\n                    endpoint=True,\n                )\n            )\n            .astype(int)\n            .tolist()\n        )\n</code></pre>"},{"location":"autoapi/gallifrey/schedule/#gallifrey.schedule.LogSchedule.generate","title":"<code>generate(num_datapoints, num_steps, start=1)</code>  <code>staticmethod</code>","text":"<p>Generate a logarithmic annealing schedule.</p> <p>Parameters:</p> Name Type Description Default <code>num_datapoints</code> <code>int</code> <p>The total number of datapoints.</p> required <code>num_steps</code> <code>int</code> <p>The number of steps in the schedule.</p> required <code>start</code> <code>int</code> <p>The starting point of the schedule, by default 1 (one observation).</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>A tuple of integers representing the cumulative number of observations at each step of the schedule. Length of the tuple is <code>num_steps</code>.</p> Source code in <code>gallifrey/schedule.py</code> <pre><code>@staticmethod\ndef generate(\n    num_datapoints: int,\n    num_steps: int,\n    start: int = 1,\n) -&gt; tuple[int, ...]:\n    \"\"\"\n    Generate a logarithmic annealing schedule.\n\n    Parameters\n    ----------\n    num_datapoints : int\n        The total number of datapoints.\n    num_steps : int\n        The number of steps in the schedule.\n    start : int, optional\n        The starting point of the schedule,\n        by default 1 (one observation).\n\n    Returns\n    -------\n    tuple[int, ...]\n        A tuple of integers representing the cumulative number of\n        observations at each step of the schedule. Length of the\n        tuple is `num_steps`.\n    \"\"\"\n    # if only one step, run total number of datapoints\n    if num_steps == 1:\n        start = num_datapoints\n\n    return tuple(\n        jnp.round(\n            jnp.geomspace(\n                start=start,\n                stop=num_datapoints,\n                num=num_steps,\n                endpoint=True,\n            )\n        )\n        .astype(int)\n        .tolist()\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/schedule/#gallifrey.schedule.Schedule","title":"<code>Schedule</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for SMC data annealing schedules.</p> Source code in <code>gallifrey/schedule.py</code> <pre><code>class Schedule(ABC):\n    \"\"\"\n    Abstract base class for SMC data annealing schedules.\n\n    \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def generate(\n        num_datapoints: int,\n        num_steps: int,\n        start: int = 1,\n    ) -&gt; tuple[int, ...]:\n        \"\"\"\n        Generate an annealing schedule.\n\n        Parameters\n        ----------\n        num_datapoints : int\n            The total number of datapoints.\n        num_steps : int\n            The number of steps in the schedule.\n        start : int, optional\n            The starting point of the schedule,\n            by default 1 (one observation).\n\n        Returns\n        -------\n        tuple[int, ...]\n            A tuple of integers representing the cumulative number of\n            observations at each step of the schedule. Length of the\n            tuple is `num_steps`.\n\n        \"\"\"\n        pass\n</code></pre>"},{"location":"autoapi/gallifrey/schedule/#gallifrey.schedule.Schedule.generate","title":"<code>generate(num_datapoints, num_steps, start=1)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Generate an annealing schedule.</p> <p>Parameters:</p> Name Type Description Default <code>num_datapoints</code> <code>int</code> <p>The total number of datapoints.</p> required <code>num_steps</code> <code>int</code> <p>The number of steps in the schedule.</p> required <code>start</code> <code>int</code> <p>The starting point of the schedule, by default 1 (one observation).</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>A tuple of integers representing the cumulative number of observations at each step of the schedule. Length of the tuple is <code>num_steps</code>.</p> Source code in <code>gallifrey/schedule.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef generate(\n    num_datapoints: int,\n    num_steps: int,\n    start: int = 1,\n) -&gt; tuple[int, ...]:\n    \"\"\"\n    Generate an annealing schedule.\n\n    Parameters\n    ----------\n    num_datapoints : int\n        The total number of datapoints.\n    num_steps : int\n        The number of steps in the schedule.\n    start : int, optional\n        The starting point of the schedule,\n        by default 1 (one observation).\n\n    Returns\n    -------\n    tuple[int, ...]\n        A tuple of integers representing the cumulative number of\n        observations at each step of the schedule. Length of the\n        tuple is `num_steps`.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/gallifrey/inference/","title":"inference","text":""},{"location":"autoapi/gallifrey/inference/#gallifrey.inference.GPState","title":"<code>GPState</code>","text":"<p>A dataclass to hold the state of the SMC or MCMC algorithm.</p> <p>Attributes:</p> Name Type Description <code>particle_states</code> <code>State</code> <p>The particle states (batched into a single State object).</p> <code>num_particles</code> <code>Int[ArrayLike, '...']</code> <p>The number of particles.</p> <code>num_data_points</code> <code>Int[ArrayLike, '...']</code> <p>The number of data points used in the SMC round. (All data points are used in the MCMC algorithm.)</p> <code>mcmc_accepted</code> <code>Int[ArrayLike, '...']</code> <p>The number of accepted MCMC steps per particle.</p> <code>hmc_accepted</code> <code>Int[ArrayLike, '...']</code> <p>The number of accepted HMC steps per particle.</p> <code>log_weights</code> <code>Float[ArrayLike, '...']</code> <p>The log weights of the particles (normalised). (Only used in the SMC algorithm, default is None.)</p> <code>marginal_log_likelihoods</code> <code>Float[ArrayLike, '...']</code> <p>The marginal log likelihoods of the particles, at the current point in the anneaing schedule. (Only used in the SMC algorithm, default is None.)</p> <code>resampled</code> <code>Bool[ArrayLike, '...']</code> <p>Whether the particles have been resampled in the current round. (Only used in the SMC algorithm, default is None.)</p> <code>key</code> <code>Optional[PRNGKeyArray | ArrayLike]</code> <p>The random key for this round. (Only used in the SMC algorithm, default is None.)</p> Source code in <code>gallifrey/inference/state.py</code> <pre><code>@struct.dataclass\nclass GPState:\n    \"\"\"\n    A dataclass to hold the state of the SMC or\n    MCMC algorithm.\n\n    Attributes\n    ----------\n    particle_states : nnx.State\n        The particle states (batched into a single State object).\n    num_particles : Int[ArrayLike, \"...\"]\n        The number of particles.\n    num_data_points : Int[ArrayLike, \"...\"]\n        The number of data points used in the SMC round. (All data\n        points are used in the MCMC algorithm.)\n    mcmc_accepted : Int[ArrayLike, \"...\"]\n        The number of accepted MCMC steps per particle.\n    hmc_accepted : Int[ArrayLike, \"...\"]\n        The number of accepted HMC steps per particle.\n    log_weights : Float[ArrayLike, \"...\"]\n        The log weights of the particles (normalised). (Only\n        used in the SMC algorithm, default is None.)\n    marginal_log_likelihoods : Float[ArrayLike, \"...\"]\n        The marginal log likelihoods of the particles, at the current\n        point in the anneaing schedule. (Only used in the SMC\n        algorithm, default is None.)\n    resampled : Bool[ArrayLike, \"...\"]\n        Whether the particles have been resampled in the current round.\n        (Only used in the SMC algorithm, default is None.)\n    key : tp.Optional[PRNGKeyArray | ArrayLike]\n        The random key for this round. (Only used in the SMC\n        algorithm, default is None.)\n\n    \"\"\"\n\n    # overly permissive type hints to allow for checkpointing\n    # TODO: find a better way to handle this\n\n    particle_states: tp.Any\n    num_particles: tp.Any\n    num_data_points: tp.Any  # num data points used in the SMC round\n    mcmc_accepted: tp.Any\n    hmc_accepted: tp.Any\n    log_weights: tp.Any = None\n    marginal_log_likelihoods: tp.Any = None\n    resampled: tp.Any = None\n    key: tp.Any = None\n\n    @classmethod\n    def from_dict(cls, state_dict: dict) -&gt; GPState:\n        \"\"\"\n        Create a GPState object from a dictionary.\n\n        Parameters\n        ----------\n        state_dict : dict\n            The dictionary containing the state. Must contain keys\n            matching the attributes of the GPState class.\n        \"\"\"\n        return GPState(**state_dict)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/#gallifrey.inference.GPState.from_dict","title":"<code>from_dict(state_dict)</code>  <code>classmethod</code>","text":"<p>Create a GPState object from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict</code> <p>The dictionary containing the state. Must contain keys matching the attributes of the GPState class.</p> required Source code in <code>gallifrey/inference/state.py</code> <pre><code>@classmethod\ndef from_dict(cls, state_dict: dict) -&gt; GPState:\n    \"\"\"\n    Create a GPState object from a dictionary.\n\n    Parameters\n    ----------\n    state_dict : dict\n        The dictionary containing the state. Must contain keys\n        matching the attributes of the GPState class.\n    \"\"\"\n    return GPState(**state_dict)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/hmc/","title":"hmc","text":""},{"location":"autoapi/gallifrey/inference/hmc/#gallifrey.inference.hmc.create_hmc_sampler_factory","title":"<code>create_hmc_sampler_factory(hmc_config, num_parameter)</code>","text":"<p>Create a Blackjax HMC sampler factory. The returned function takes an objective function and returns a Blackjax HMC sampler.</p> <p>This wrapper is used to apply the HMC config to the HMC sampler.</p> <p>The inverse mass matrix is diagonal with ones, and scaled by the inverse mass matrix scaling factor in the config.</p> <p>Parameters:</p> Name Type Description Default <code>hmc_config</code> <code>dict[str, float]</code> <p>A dictionary containing the HMC config parameters, must contain the keys: - \"step_size\" - \"inv_mass_matrix_scaling\" - \"num_integration_steps\"</p> required <code>num_parameter</code> <code>int</code> <p>The (maximum) number of parameters in the model.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A function that takes an objective function and returns a Blackjax HMC sampler.</p> Source code in <code>gallifrey/inference/hmc.py</code> <pre><code>def create_hmc_sampler_factory(\n    hmc_config: dict[str, float],\n    num_parameter: int,\n) -&gt; tp.Callable:\n    \"\"\"\n    Create a Blackjax HMC sampler factory.\n    The returned function takes an objective function and returns\n    a Blackjax HMC sampler.\n\n    This wrapper is used to apply the HMC config to the HMC sampler.\n\n    The inverse mass matrix is diagonal with ones, and scaled by the\n    inverse mass matrix scaling factor in the config.\n\n    Parameters\n    ----------\n    hmc_config : dict[str, float]\n        A dictionary containing the HMC config parameters, must\n        contain the keys:\n        - \"step_size\"\n        - \"inv_mass_matrix_scaling\"\n        - \"num_integration_steps\"\n    num_parameter : int\n        The (maximum) number of parameters in the model.\n\n    Returns\n    -------\n    tp.Callable\n        A function that takes an objective function and returns\n        a Blackjax HMC sampler.\n    \"\"\"\n\n    def hmc_sampler_factory(\n        objective_function: tp.Callable,\n    ) -&gt; blackjax.SamplingAlgorithm:\n\n        step_size = hmc_config[\"step_size\"]\n        inv_mass_matrix = hmc_config[\"inv_mass_matrix_scaling\"] * jnp.diag(\n            jnp.ones(num_parameter)\n        )\n        num_integration_steps = hmc_config[\"num_integration_steps\"]\n\n        hmc_sampler = blackjax.hmc(\n            objective_function,\n            step_size,\n            inv_mass_matrix,\n            num_integration_steps,\n        )\n\n        return hmc_sampler\n\n    return hmc_sampler_factory\n</code></pre>"},{"location":"autoapi/gallifrey/inference/hmc/#gallifrey.inference.hmc.get_hmc_objective","title":"<code>get_hmc_objective(particle_state, data, kernel_prior, noise_prior)</code>","text":"<p>Wrapper function to create the HMC objective function.</p> <p>The HMC objective function is the sum of the marginal log likelihood and the log prior of the kernel and noise parameters.</p> <p>The wrapper splits the particle state into a parameter state ( used for sampling) and a static state. The resulting objective function takes the particle parameter state in the unconstrained space, transforms it to the constrained space, and returns the value of the objective function.</p> <p>Parameters:</p> Name Type Description Default <code>particle_state</code> <code>State</code> <p>The particle state.</p> required <code>data</code> <code>Dataset</code> <p>A dataset object containing the input x and output y.</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior object.</p> required <code>noise_prior</code> <code>Distribution</code> <p>The noise prior distribution.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The HMC objective function, which takes the unconstrained particle parameter state and returns the value of the objective function.</p> Source code in <code>gallifrey/inference/hmc.py</code> <pre><code>def get_hmc_objective(\n    particle_state: nnx.State,\n    data: Dataset,\n    kernel_prior: KernelPrior,\n    noise_prior: Distribution,\n) -&gt; tp.Callable:\n    \"\"\"\n    Wrapper function to create the HMC objective function.\n\n    The HMC objective function is the sum of the marginal log likelihood\n    and the log prior of the kernel and noise parameters.\n\n    The wrapper splits the particle state into a parameter state (\n    used for sampling) and a static state.\n    The resulting objective function takes the particle parameter\n    state in the unconstrained space, transforms it to the constrained\n    space, and returns the value of the objective function.\n\n    Parameters\n    ----------\n    particle_state : nnx.State\n        The particle state.\n    data : Dataset\n        A dataset object containing the input x and output y.\n    kernel_prior : KernelPrior\n        The kernel prior object.\n    noise_prior : Distribution\n        The noise prior distribution.\n\n    Returns\n    -------\n    tp.Callable\n        The HMC objective function, which takes the unconstrained\n        particle parameter state and returns the value of the objective\n        function.\n    \"\"\"\n\n    _, *particle_static_state = particle_state.split(ParticleParameter, ...)\n\n    @jit\n    def hmc_objective(\n        unconstrained_particle_parameter_state: nnx.State,\n    ) -&gt; ScalarFloat:\n        \"\"\"HMC objective function\"\"\"\n\n        unconstrained_particle_state = nnx.State.merge(\n            unconstrained_particle_parameter_state,\n            *particle_static_state,\n        )\n\n        constrained_particle_state = transform_particle_parameters(\n            unconstrained_particle_state,\n            kernel_prior,\n            inverse=False,\n        )\n\n        kernel_state = constrained_particle_state.kernel\n        kernel = kernel_prior.reconstruct_kernel(kernel_state)\n\n        # calcuate marginal log likelihood\n        kernel_gram = kernel._gram_train(data.x)\n        noise_variance = jnp.array(\n            constrained_particle_state.noise_variance.value,\n        )  # type: ignore\n        mll = calculate_marginal_log_likelihood(\n            kernel_gram,\n            noise_variance,\n            data,\n        )\n\n        # calculate priors\n        log_prob_kernel_parameter_prior = kernel_prior.parameter_prior.log_prob(\n            kernel_state\n        )\n        log_prob_noise_parameter_prior = jnp.asarray(\n            noise_prior.log_prob(noise_variance),\n            dtype=data.x.dtype,\n        ).squeeze()\n\n        return mll + log_prob_kernel_parameter_prior + log_prob_noise_parameter_prior\n\n    return hmc_objective\n</code></pre>"},{"location":"autoapi/gallifrey/inference/hmc/#gallifrey.inference.hmc.hmc_inference_loop","title":"<code>hmc_inference_loop(key, hmc_kernel, initial_state, n_hmc, transform=lambda state, info: (state, info))</code>","text":"<p>Run an HMC inference loop.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key.</p> required <code>hmc_kernel</code> <code>Callable</code> <p>The HMC kernel (e.g. HMC, NUTS), derived from one of blackjax's HMC sampling algorithms.</p> required <code>initial_state</code> <code>HMCState</code> <p>The initial state of the HMC sampler.</p> required <code>n_hmc</code> <code>ScalarInt</code> <p>The number of HMC steps.</p> required <code>transform</code> <code>Callable</code> <p>A transformation of the trace of states (and info) to be returned. This is useful for computing determinstic variables, or returning a subset of the states. By default, the states are returned as a tuple (state, info).</p> <code>lambda state, info: (state, info)</code> <p>Returns:</p> Type Description <code>HMCState</code> <p>The final state.</p> <code>Any</code> <p>The history of states. The output type depends on the transform function. By default, the states are a tuple of the state and the info.</p> Source code in <code>gallifrey/inference/hmc.py</code> <pre><code>def hmc_inference_loop(\n    key: PRNGKeyArray,\n    hmc_kernel: tp.Callable,\n    initial_state: HMCState,\n    n_hmc: ScalarInt,\n    transform: tp.Callable = lambda state, info: (state, info),\n) -&gt; tuple[HMCState, tp.Any]:\n    \"\"\"\n    Run an HMC inference loop.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key.\n    hmc_kernel : tp.Callable\n        The HMC kernel (e.g. HMC, NUTS), derived\n        from one of blackjax's HMC sampling algorithms.\n    initial_state : HMCState\n        The initial state of the HMC sampler.\n    n_hmc : ScalarInt\n        The number of HMC steps.\n    transform : tp.Callable, optional\n        A transformation of the trace of states (and info) to be returned.\n        This is useful for computing determinstic variables, or\n        returning a subset of the states. By default, the states are\n        returned as a tuple (state, info).\n\n    Returns\n    -------\n    HMCState\n        The final state.\n    tp.Any\n        The history of states. The output type depends on the\n        transform function. By default, the states are a tuple of\n        the state and the info.\n    \"\"\"\n\n    @jit\n    def one_step(state: HMCState, key: PRNGKeyArray) -&gt; tuple[HMCState, tp.Any]:\n        new_state, info = hmc_kernel(key, state)\n        return new_state, transform(new_state, info)\n\n    keys = jr.split(key, int(n_hmc))\n    final_state, history = jax.lax.scan(\n        one_step,\n        initial_state,\n        keys,\n    )\n\n    return final_state, history\n</code></pre>"},{"location":"autoapi/gallifrey/inference/parameter_rejuvenation/","title":"parameter_rejuvenation","text":""},{"location":"autoapi/gallifrey/inference/parameter_rejuvenation/#gallifrey.inference.parameter_rejuvenation.rejuvenate_particle_parameters","title":"<code>rejuvenate_particle_parameters(key, particle_state, data, kernel_prior, noise_prior, n_hmc, hmc_sampler_factory)</code>","text":"<p>Rejuvenate GP parameters using HMC.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key.</p> required <code>particle_state</code> <code>State</code> <p>The current particle state.</p> required <code>data</code> <code>Dataset</code> <p>The dataset object containing the input x and output y.</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior object.</p> required <code>noise_prior</code> <code>Distribution</code> <p>The noise prior distribution.</p> required <code>n_hmc</code> <code>ScalarInt</code> <p>The number of HMC steps.</p> required <code>hmc_sampler_factory</code> <code>Callable</code> <p>The HMC sampler factory function, which takes an objective function and returns a Blackjax HMC sampler.</p> required <p>Returns:</p> Type Description <code>State</code> <p>The particle state with rejuvenated parameters.</p> <code>ScalarInt</code> <p>The number of accepted HMC steps.</p> Source code in <code>gallifrey/inference/parameter_rejuvenation.py</code> <pre><code>@partial(\n    jit,\n    static_argnames=(\n        \"data\",\n        \"kernel_prior\",\n        \"noise_prior\",\n        \"n_hmc\",\n        \"hmc_sampler_factory\",\n    ),\n)\ndef rejuvenate_particle_parameters(\n    key: PRNGKeyArray,\n    particle_state: nnx.State,\n    data: Dataset,\n    kernel_prior: KernelPrior,\n    noise_prior: Distribution,\n    n_hmc: ScalarInt,\n    hmc_sampler_factory: tp.Callable,\n) -&gt; tuple[nnx.State, ScalarInt]:\n    \"\"\"\n    Rejuvenate GP parameters using HMC.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key.\n    particle_state : nnx.State\n        The current particle state.\n    data : Dataset\n        The dataset object containing\n        the input x and output y.\n    kernel_prior : KernelPrior\n        The kernel prior object.\n    noise_prior : Distribution\n        The noise prior distribution.\n    n_hmc : ScalarInt\n        The number of HMC steps.\n    hmc_sampler_factory : tp.Callable\n        The HMC sampler factory function,\n        which takes an objective function and\n        returns a Blackjax HMC sampler.\n\n    Returns\n    -------\n    nnx.State\n        The particle state with rejuvenated\n        parameters.\n    ScalarInt\n        The number of accepted HMC steps.\n\n    \"\"\"\n\n    # get HMC objective and sampler\n    hmc_objective = get_hmc_objective(\n        particle_state,\n        data,\n        kernel_prior,\n        noise_prior,\n    )\n    hmc_sampler = hmc_sampler_factory(hmc_objective)\n\n    # transform particle parameters to unconstrained space and\n    # create initial HMC state\n    unconstrained_particle_state = transform_particle_parameters(\n        particle_state,\n        kernel_prior,\n        inverse=True,\n    )\n    unconstrained_parameter_state, *static_state = unconstrained_particle_state.split(\n        ParticleParameter, ...\n    )\n    initial_hmc_state = hmc_sampler.init(unconstrained_parameter_state)\n\n    # run the HMC inference loop\n    key, sample_key = jr.split(key)\n    final_parameter_state, other = hmc_inference_loop(\n        sample_key,\n        jax.jit(hmc_sampler.step),\n        initial_hmc_state,\n        n_hmc,\n    )\n    history, info = other\n    # HMCInfo contains (maybe we could return more):\n    # - momentum\n    # - acceptance_rate\n    # - is_accepted\n    # - is_divergent\n    # - energy\n    # - proposal\n    # - step_size\n    # - num_integration_steps\n\n    # put particle state back together and transform to constrained space\n    new_particle_state = nnx.State.merge(\n        final_parameter_state.position,  # type: ignore\n        *static_state,\n    )\n    constrained_particle_state = transform_particle_parameters(\n        new_particle_state,\n        kernel_prior,\n        inverse=False,\n    )\n\n    return constrained_particle_state, info.is_accepted.sum()\n</code></pre>"},{"location":"autoapi/gallifrey/inference/rejuvenation/","title":"rejuvenation","text":""},{"location":"autoapi/gallifrey/inference/rejuvenation/#gallifrey.inference.rejuvenation.rejuvenate_particle","title":"<code>rejuvenate_particle(key, particle_state, data, kernel_prior, noise_prior, n_mcmc, n_hmc, fix_noise, hmc_sampler_factory, verbosity=0)</code>","text":"<p>Rejuvenates a particle's state using a structure MCMC move followed by parameter HMC moves.</p> <p>This function performs a sequence of rejuvenation steps for a single particle state. It iterates <code>n_mcmc</code> times, applying the <code>rejuvenation_step</code> function in each iteration.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for the MCMC sampling.</p> required <code>particle_state</code> <code>State</code> <p>The current state of the particle to be rejuvenated.</p> required <code>data</code> <code>Dataset</code> <p>The dataset object containing the input and output data.</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior for the kernel structure.</p> required <code>noise_prior</code> <code>Distribution</code> <p>The prior distribution for the noise variance.</p> required <code>n_mcmc</code> <code>ScalarInt</code> <p>The number of MCMC iterations (rejuvenation steps) to perform.</p> required <code>n_hmc</code> <code>ScalarInt</code> <p>The number of Hamiltonian Monte Carlo (HMC) steps to perform within each parameter rejuvenation step (inside <code>rejuvenation_step</code>).</p> required <code>fix_noise</code> <code>bool</code> <p>A boolean flag indicating whether the noise variance is fixed or trainable.</p> required <code>hmc_sampler_factory</code> <code>Callable</code> <p>A factory function that creates an HMC sampler.</p> required <code>verbosity</code> <code>int</code> <p>The verbosity level. Debug information is printed if <code>verbosity &gt; 0</code>.</p> <code>0</code> <p>Returns:</p> Type Description <code>State</code> <p>The final rejuvenated particle state after <code>n_mcmc</code> iterations.</p> <code>State</code> <p>The history of the rejuvenation steps after every MCMC iteration.</p> <code>ScalarInt</code> <p>The total number of MCMC steps accepted during the rejuvenation process.</p> <code>ScalarInt</code> <p>The total number of HMC steps accepted during the rejuvenation process.</p> Source code in <code>gallifrey/inference/rejuvenation.py</code> <pre><code>@partial(\n    jit,\n    static_argnames=(\n        \"data\",\n        \"kernel_prior\",\n        \"noise_prior\",\n        \"n_mcmc\",\n        \"n_hmc\",\n        \"fix_noise\",\n        \"hmc_sampler_factory\",\n        \"verbosity\",\n    ),\n)\ndef rejuvenate_particle(\n    key: PRNGKeyArray,\n    particle_state: nnx.State,\n    data: Dataset,\n    kernel_prior: KernelPrior,\n    noise_prior: Distribution,\n    n_mcmc: ScalarInt,\n    n_hmc: ScalarInt,\n    fix_noise: bool,\n    hmc_sampler_factory: tp.Callable,\n    verbosity: int = 0,\n) -&gt; tuple[nnx.State, nnx.State, ScalarInt, ScalarInt]:\n    \"\"\"\n    Rejuvenates a particle's state using a structure MCMC move followed by\n    parameter HMC moves.\n\n    This function performs a sequence of rejuvenation steps for a\n    single particle state. It iterates `n_mcmc` times, applying the\n    `rejuvenation_step` function in each iteration.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for the MCMC sampling.\n    particle_state : nnx.State\n        The current state of the particle to be rejuvenated.\n    data : Dataset\n        The dataset object containing the input and output data.\n    kernel_prior : KernelPrior\n        The kernel prior for the kernel structure.\n    noise_prior : Distribution\n        The prior distribution for the noise variance.\n    n_mcmc : ScalarInt\n        The number of MCMC iterations (rejuvenation steps) to perform.\n    n_hmc : ScalarInt\n        The number of Hamiltonian Monte Carlo (HMC) steps to perform within each\n        parameter rejuvenation step (inside `rejuvenation_step`).\n    fix_noise : bool\n        A boolean flag indicating whether the noise variance is fixed or trainable.\n    hmc_sampler_factory : tp.Callable\n        A factory function that creates an HMC sampler.\n    verbosity : int\n        The verbosity level. Debug information is printed if `verbosity &gt; 0`.\n\n    Returns\n    -------\n    nnx.State\n        The final rejuvenated particle state after `n_mcmc` iterations.\n    nnx.State\n        The history of the rejuvenation steps after every MCMC iteration.\n    ScalarInt\n        The total number of MCMC steps accepted during the rejuvenation process.\n    ScalarInt\n        The total number of HMC steps accepted during the rejuvenation process.\n    \"\"\"\n\n    def scan_func(\n        loop_state: tuple[nnx.State, Int[jnp.ndarray, \"\"], Int[jnp.ndarray, \"\"]],\n        key: PRNGKeyArray,\n    ) -&gt; tuple[tuple[nnx.State, Int[jnp.ndarray, \"\"], Int[jnp.ndarray, \"\"]], nnx.State]:\n        \"\"\"\n        A wrapper function for a single `rejuvenation_step` to be\n        passed to the scan function.\n        \"\"\"\n        particle_state, accepted_mcmc, accepted_hmc = loop_state\n\n        new_state, mcmc_accepted, n_hmc_accepted = rejuvenation_step(\n            key,\n            particle_state,\n            data,\n            kernel_prior,\n            noise_prior,\n            n_hmc,\n            fix_noise,\n            hmc_sampler_factory,\n            verbosity,\n        )\n        accepted_mcmc = accepted_mcmc + mcmc_accepted\n        accepted_hmc = accepted_hmc + n_hmc_accepted\n\n        return (new_state, accepted_mcmc, accepted_hmc), new_state\n\n    final_loop_state, history = lax.scan(\n        scan_func,\n        (particle_state, jnp.array(0), jnp.array(0)),\n        jr.split(key, int(n_mcmc)),\n    )\n\n    final_state, accepted_mcmc, accepted_hmc = final_loop_state\n\n    return final_state, history, accepted_mcmc, accepted_hmc\n</code></pre>"},{"location":"autoapi/gallifrey/inference/rejuvenation/#gallifrey.inference.rejuvenation.rejuvenation_step","title":"<code>rejuvenation_step(key, particle_state, data, kernel_prior, noise_prior, n_hmc, fix_noise, hmc_sampler_factory, verbosity=0)</code>","text":"<p>Performs a single rejuvenation step for a article state.</p> <p>This function implements a single step of the rejuvenation process. It consists of two main moves:     1. Structure Move: Proposes a new kernel structure based on the <code>kernel_prior</code>.     2. Parameter Rejuvenation: If the structure move is accepted, rejuvenates     the kernel parameters and potentially the noise variance using     Hamiltonian Monte Carlo (HMC).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for the rejuvenation step.</p> required <code>particle_state</code> <code>State</code> <p>The current state of the particle to be rejuvenated.</p> required <code>data</code> <code>Dataset</code> <p>The dataset object containing the input and output data.</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior for the kernel structure.</p> required <code>noise_prior</code> <code>Distribution</code> <p>The prior distribution for the noise variance.</p> required <code>n_hmc</code> <code>ScalarInt</code> <p>The number of Hamiltonian Monte Carlo (HMC) steps to perform during parameter rejuvenation.</p> required <code>fix_noise</code> <code>bool</code> <p>A boolean flag indicating whether the noise variance is fixed or trainable.</p> required <code>hmc_sampler_factory</code> <code>Callable</code> <p>A factory function that creates an HMC sampler.</p> required <code>verbosity</code> <code>int</code> <p>The verbosity level. Debug information of the structure move is printed if <code>verbosity &gt; 1</code>.</p> <code>0</code> <p>Returns:</p> Type Description <code>State</code> <p>The rejuvenated particle state after performing the structure move and (potentially) parameter rejuvenation.</p> <code>ScalarBool</code> <p>A boolean flag indicating whether the structure move was accepted.</p> <code>ScalarInt</code> <p>The number of HMC steps accepted during the parameter rejuvenation.</p> Source code in <code>gallifrey/inference/rejuvenation.py</code> <pre><code>@partial(\n    jit,\n    static_argnames=(\n        \"data\",\n        \"kernel_prior\",\n        \"noise_prior\",\n        \"n_hmc\",\n        \"fix_noise\",\n        \"hmc_sampler_factory\",\n        \"verbosity\",\n    ),\n)\ndef rejuvenation_step(\n    key: PRNGKeyArray,\n    particle_state: nnx.State,\n    data: Dataset,\n    kernel_prior: KernelPrior,\n    noise_prior: Distribution,\n    n_hmc: ScalarInt,\n    fix_noise: bool,\n    hmc_sampler_factory: tp.Callable,\n    verbosity: int = 0,\n) -&gt; tuple[nnx.State, ScalarBool, ScalarInt]:\n    \"\"\"\n    Performs a single rejuvenation step for a\n    article state.\n\n    This function implements a single step of the rejuvenation process.\n    It consists of two main moves:\n        1. Structure Move: Proposes a new kernel structure based on the `kernel_prior`.\n        2. Parameter Rejuvenation: If the structure move is accepted, rejuvenates\n        the kernel parameters and potentially the noise variance using\n        Hamiltonian Monte Carlo (HMC).\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for the rejuvenation step.\n    particle_state : nnx.State\n        The current state of the particle to be rejuvenated.\n    data : Dataset\n        The dataset object containing the input and output data.\n    kernel_prior : KernelPrior\n        The kernel prior for the kernel structure.\n    noise_prior : Distribution\n        The prior distribution for the noise variance.\n    n_hmc : ScalarInt\n        The number of Hamiltonian Monte Carlo (HMC) steps to perform during\n        parameter rejuvenation.\n    fix_noise : bool\n        A boolean flag indicating whether the noise variance is fixed or trainable.\n    hmc_sampler_factory : tp.Callable\n        A factory function that creates an HMC sampler.\n    verbosity : int\n        The verbosity level. Debug information of the structure move\n        is printed if `verbosity &gt; 1`.\n\n    Returns\n    -------\n    nnx.State\n        The rejuvenated particle state after performing the structure move\n        and (potentially) parameter rejuvenation.\n    ScalarBool\n        A boolean flag indicating whether the structure move was accepted.\n    ScalarInt\n        The number of HMC steps accepted during the parameter rejuvenation.\n    \"\"\"\n\n    key, structure_key, hmc_key = jr.split(key, 3)\n\n    new_particle_state, accepted = structure_move(\n        structure_key,\n        particle_state,\n        kernel_prior,\n        noise_prior,\n        data,\n        fix_noise=fix_noise,\n        verbosity=verbosity,\n    )\n\n    new_particle_state, n_hmc_accepted = lax.cond(\n        accepted,\n        lambda key: rejuvenate_particle_parameters(\n            key,\n            new_particle_state,\n            data,\n            kernel_prior,\n            noise_prior,\n            n_hmc,\n            hmc_sampler_factory,\n        ),\n        lambda key: (new_particle_state, jnp.array(0)),\n        hmc_key,\n    )\n\n    return new_particle_state, jnp.array(accepted, dtype=jnp.bool), n_hmc_accepted\n</code></pre>"},{"location":"autoapi/gallifrey/inference/smc/","title":"smc","text":""},{"location":"autoapi/gallifrey/inference/smc/#gallifrey.inference.smc.calculate_effective_sample_size","title":"<code>calculate_effective_sample_size(log_weights)</code>","text":"<p>Calculates the effective sample size (ESS) of the particles.</p> <p>The effective sample size is a measure of how diverse the particle weights are, and is used to determine whether resampling is needed. If the ESS is low, it means that many particles have similar weights, and resampling is needed to avoid degeneracy.</p> <p>The ESS is calculated as:</p> <pre><code>ESS = 1 / sum(w^2)\n</code></pre> <p>where w is the normalised(!) weight of each particle.</p> <p>Parameters:</p> Name Type Description Default <code>log_weights</code> <code>Float[ndarray, ' N']</code> <p>The log weights of the particles.</p> required <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The effective sample size of the particles.</p> Source code in <code>gallifrey/inference/smc.py</code> <pre><code>def calculate_effective_sample_size(\n    log_weights: Float[jnp.ndarray, \" N\"]\n) -&gt; ScalarFloat:\n    \"\"\"\n    Calculates the effective sample size (ESS) of the particles.\n\n    The effective sample size is a measure of how diverse the particle weights are,\n    and is used to determine whether resampling is needed. If the ESS is low, it means\n    that many particles have similar weights, and resampling is needed to avoid\n    degeneracy.\n\n    The ESS is calculated as:\n\n        ESS = 1 / sum(w^2)\n\n    where w is the normalised(!) weight of each particle.\n\n    Parameters\n    ----------\n    log_weights : Float[jnp.ndarray, \" N\"]\n        The log weights of the particles.\n\n    Returns\n    -------\n    ScalarFloat\n        The effective sample size of the particles.\n\n    \"\"\"\n\n    weights = jnp.exp(log_weights)\n    ess = 1 / jnp.sum(weights**2)\n    return ess\n</code></pre>"},{"location":"autoapi/gallifrey/inference/smc/#gallifrey.inference.smc.calculate_mll_wrapper","title":"<code>calculate_mll_wrapper(particle_state, data, kernel_prior)</code>","text":"<p>Wapper around calculate_marginal_log_likelihood, which calculates the MLL from a particle state.</p> <p>Parameters:</p> Name Type Description Default <code>particle_state</code> <code>State</code> <p>The particle state.</p> required <code>data</code> <code>Dataset</code> <p>The dataset object, containing the input x and output y.</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior, used to reconstruct the kernel.</p> required <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The marginal log likelihood for the given particle state and subset of the data.</p> Source code in <code>gallifrey/inference/smc.py</code> <pre><code>def calculate_mll_wrapper(\n    particle_state: nnx.State,\n    data: Dataset,\n    kernel_prior: KernelPrior,\n) -&gt; ScalarFloat:\n    \"\"\"\n    Wapper around calculate_marginal_log_likelihood, which\n    calculates the MLL from a particle state.\n\n    Parameters\n    ----------\n    particle_state : nnx.State\n        The particle state.\n    data : Dataset\n        The dataset object, containing the input x and output y.\n    kernel_prior : KernelPrior\n        The kernel prior, used to reconstruct the kernel.\n\n    Returns\n    -------\n    ScalarFloat\n        The marginal log likelihood for the given particle state and\n        subset of the data.\n\n    \"\"\"\n\n    kernel = kernel_prior.reconstruct_kernel(particle_state.kernel)\n    kernel_gram = kernel._gram_train(data.x)\n\n    noise_variance = jnp.array(particle_state.noise_variance.value)  # type: ignore\n\n    mll = calculate_marginal_log_likelihood(\n        kernel_gram,\n        noise_variance,\n        data,\n    )\n    return mll\n</code></pre>"},{"location":"autoapi/gallifrey/inference/smc/#gallifrey.inference.smc.resample_particles","title":"<code>resample_particles(key, particle_states, log_weights, num_particles, resampling_func)</code>","text":"<p>Resamples particles based on the log weights.</p> <p>This function resamples the particles based on the log weights using a resampling function, which takes the log weights and the number of particles and returns the indices of the resampled particles.</p> <p>The resampling function must have the signature:     resampling_func(key, weights, num_particles) -&gt; jnp.ndarray</p> <p>Note that the actual weights must be passed to the resampling function (not the log weights), and that they need to be normalised.</p> <p>We use sampling functions from blackjax. By default we use stratified resampling, but systematic, multinomial, and residual resampling are also available.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key for the resampling function.</p> required <code>particle_states</code> <code>State</code> <p>The current particle states.</p> required <code>log_weights</code> <code>Float[ndarray, ' N']</code> <p>The log weights of the particles.</p> required <code>num_particles</code> <code>ScalarInt</code> <p>The number of particles (should be the same as the length of the log weights, and the number of particles in the particle_states).</p> required <code>resampling_func</code> <code>Callable</code> <p>The resampling function to use.</p> required <p>Returns:</p> Type Description <code>State</code> <p>The resampled particle states.</p> <code>Float[ndarray, ' N']</code> <p>The normalised log weights of the resampled particles. (i.e. log(1/N) for each particle, see Saad2023, algorithm 1 line 9).</p> Source code in <code>gallifrey/inference/smc.py</code> <pre><code>@partial(jit, static_argnames=(\"num_particles\", \"resampling_func\"))\ndef resample_particles(\n    key: PRNGKeyArray,\n    particle_states: nnx.State,\n    log_weights: Float[jnp.ndarray, \" N\"],\n    num_particles: ScalarInt,\n    resampling_func: tp.Callable,\n) -&gt; tuple[nnx.State, Float[jnp.ndarray, \" N\"]]:\n    \"\"\"\n    Resamples particles based on the log weights.\n\n    This function resamples the particles based on the log weights\n    using a resampling function, which takes the log weights and\n    the number of particles and returns the indices of the resampled\n    particles.\n\n    The resampling function must have the signature:\n        resampling_func(key, weights, num_particles) -&gt; jnp.ndarray\n\n    Note that the actual weights must be passed to the resampling function\n    (not the log weights), and that they need to be normalised.\n\n    We use sampling functions from blackjax. By default we use stratified\n    resampling, but systematic, multinomial, and residual resampling are also\n    available.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key for the resampling function.\n    particle_states : nnx.State\n        The current particle states.\n    log_weights : Float[jnp.ndarray, \" N\"]\n        The log weights of the particles.\n    num_particles : ScalarInt\n        The number of particles (should be the same as the length of the log weights,\n        and the number of particles in the particle_states).\n    resampling_func : tp.Callable\n        The resampling function to use.\n\n    Returns\n    -------\n    nnx.State\n        The resampled particle states.\n    Float[jnp.ndarray, \" N\"]\n        The normalised log weights of the resampled particles.\n        (i.e. log(1/N) for each particle, see Saad2023, algorithm 1 line 9).\n    \"\"\"\n\n    # sample indices based on weights\n    sampled_indices = resampling_func(key, jnp.exp(log_weights), num_particles)\n\n    # update particle states based on sampled indices\n    resampled_state = jtu.tree_map(lambda leaf: leaf[sampled_indices], particle_states)\n\n    resampled_log_weights = jnp.log(jnp.ones(num_particles) / num_particles)\n\n    return resampled_state, resampled_log_weights\n</code></pre>"},{"location":"autoapi/gallifrey/inference/smc/#gallifrey.inference.smc.reweight_particles","title":"<code>reweight_particles(smc_state, data, kernel_prior)</code>","text":"<p>Reweights particles based on the marginal log likelihood calculated with new data points.</p> <p>This function updates the log weights of the particles in the SMC state by calculating the marginal log likelihood for each particle using the new subset vs the previous subset, taking the difference in MLL for each particle and adding it to the existing log weights, in essence:</p> <pre><code>w_t = w_{t-1} * (mll_t / mll_{t-1}) (but in log space, so addition instead)\n</code></pre> <p>See Saad2023, algorithm 1 line 5.</p> <p>The weights are returned normalised.</p> <p>Parameters:</p> Name Type Description Default <code>smc_state</code> <code>GPState</code> <p>The SMC state after the previous round.</p> required <code>data</code> <code>Dataset</code> <p>The dataset containing the data at the current time point.</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior object, used to construct the kernel.</p> required <p>Returns:</p> Type Description <code>Float[ndarray, ' N']</code> <p>The updated (normalised) log weights for the particles.</p> Source code in <code>gallifrey/inference/smc.py</code> <pre><code>@partial(jit, static_argnames=(\"data\", \"kernel_prior\"))\ndef reweight_particles(\n    smc_state: GPState,\n    data: Dataset,\n    kernel_prior: KernelPrior,\n) -&gt; Float[jnp.ndarray, \" N\"]:\n    \"\"\"\n    Reweights particles based on the marginal log likelihood\n    calculated with new data points.\n\n    This function updates the log weights of the particles in the SMC state by\n    calculating the marginal log likelihood for each particle using the new subset\n    vs the previous subset, taking the difference in MLL for each particle and adding\n    it to the existing log weights, in essence:\n\n        w_t = w_{t-1} * (mll_t / mll_{t-1}) (but in log space, so addition instead)\n\n    See Saad2023, algorithm 1 line 5.\n\n    The weights are returned normalised.\n\n    Parameters\n    ----------\n    smc_state : GPState\n        The SMC state after the previous round.\n    data : Dataset\n        The dataset containing the data at the current time point.\n    kernel_prior : KernelPrior\n        The kernel prior object, used to construct the kernel.\n\n    Returns\n    -------\n    Float[jnp.ndarray, \" N\"]\n        The updated (normalised) log weights for the particles.\n    \"\"\"\n\n    new_mlls = vmap(calculate_mll_wrapper, in_axes=(0, None, None))(\n        smc_state.particle_states,  # type: ignore\n        data,\n        kernel_prior,\n    )\n\n    mll_ratio = new_mlls - smc_state.marginal_log_likelihoods  # type: ignore\n\n    unnorm_log_weights: jnp.ndarray = smc_state.log_weights + mll_ratio  # type: ignore\n    log_weights = unnorm_log_weights - logsumexp(unnorm_log_weights)\n    return log_weights\n</code></pre>"},{"location":"autoapi/gallifrey/inference/smc/#gallifrey.inference.smc.smc_loop","title":"<code>smc_loop(key, particle_states, annealing_schedule, num_particles, data, kernel_prior, noise_prior, fix_noise, hmc_sampler_factory, n_mcmc, n_hmc, resampling_func=stratified, ess_threshold=0.5, verbosity=0)</code>","text":"<p>Runs the Sequential Monte Carlo (SMC) algorithm.</p> <p>The SMC loop runs a series of SMC rounds using a data annealing schedule, each round includes three steps:</p> <ul> <li> <p>Reweighting: Update particle weights based on new data. The new weight is the old weight times the ratio of the new marginal likelihood evaluated on the new data (so all data up to n_t, where n_t is the number of data points at time t in the annealing schedule) and the old marginal likelihood evaluated on the previous data (up to n_{t-1}). In essence, the new weight reflect how much better the particle fits the new (unseen) data compared to the data it was trained on.</p> </li> <li> <p>Resampling: Adaptively resample particles based on the new weights. This step is performed if the effective sample size (ESS) is below a threshold (0.5 in this implementation). The ESS is a measure of how diverse the particle weights are, and if it is low, it means that many particles have similar weights, and resampling is needed to avoid degeneracy. There are various resampling strategies available, we default to stratified resampling.</p> </li> <li> <p>Rejuvenation: Rejuvenate particles using MCMC and HMC steps. This step is performed on all particles, and consists of two main moves:</p> <ol> <li>Structure Move: Proposes a new kernel structure based on the <code>kernel_prior</code>.</li> <li>Parameter Rejuvenation: If the structure move is accepted, rejuvenates the kernel parameters and potentially the noise variance using Hamiltonian Monte Carlo (HMC).</li> </ol> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key used for sampling.</p> required <code>particle_states</code> <code>State</code> <p>The particle states at the start of the SMC loop.</p> required <code>annealing_schedule</code> <code>Int[ndarray, ' T']</code> <p>The annealing schedule, an array of integers representing the number of data points to use at each time point (e.g. jnp.array([10, 20, 30, 40, 50]) ).</p> required <code>num_particles</code> <code>int</code> <p>The number of particles (must match number of particles in the particle_states).</p> required <code>data</code> <code>Dataset</code> <p>The dataset object containing the input x and output y for all data points.</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior object, used to reconstruct the kernel, and sample new kernel structures.</p> required <code>noise_prior</code> <code>Distribution</code> <p>The prior distribution for the noise variance.</p> required <code>fix_noise</code> <code>bool</code> <p>Whether to fix the noise variance during rejuvenation, or sample it.</p> required <code>hmc_sampler_factory</code> <code>Callable</code> <p>A factory function that creates an HMC sampler instance for the parameter rejuvenation step.</p> required <code>n_mcmc</code> <code>ScalarInt</code> <p>The number of MCMC steps to perform during rejuvenation.</p> required <code>n_hmc</code> <code>ScalarInt</code> <p>The number of HMC steps to perform during rejuvenation.</p> required <code>resampling_func</code> <code>Callable</code> <p>The resampling function to use, by default stratified.</p> <code>stratified</code> <code>ess_threshold</code> <code>float</code> <p>The threshold for the effective sample size (ESS) below which resampling is performed, by default 0.5.</p> <code>0.5</code> <code>verbosity</code> <code>int</code> <p>The verbosity level for the rejuvenation process. Debug information is printed if verbosity &gt; 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>GPState</code> <p>The final SMC state after the SMC loop.</p> <code>list[GPState]</code> <p>The full history of the SMC state at each time point in the annealing schedule.</p> Source code in <code>gallifrey/inference/smc.py</code> <pre><code>def smc_loop(\n    key: PRNGKeyArray,\n    particle_states: nnx.State,\n    annealing_schedule: tuple[int, ...],\n    num_particles: int,\n    data: Dataset,\n    kernel_prior: KernelPrior,\n    noise_prior: Distribution,\n    fix_noise: bool,\n    hmc_sampler_factory: tp.Callable,\n    n_mcmc: ScalarInt,\n    n_hmc: ScalarInt,\n    resampling_func: tp.Callable = stratified,\n    ess_threshold: float = 0.5,\n    verbosity: ScalarInt = 0,\n) -&gt; tuple[GPState, list[GPState]]:\n    \"\"\"\n    Runs the Sequential Monte Carlo (SMC) algorithm.\n\n    The SMC loop runs a series of SMC rounds using a data annealing\n    schedule, each round includes three steps:\n\n    - Reweighting: Update particle weights based on new data.\n    The new weight is the old weight times the ratio of the new\n    marginal likelihood evaluated on the new data (so all data up\n    to n_t, where n_t is the number of data points at time t in\n    the annealing schedule) and the old marginal likelihood evaluated\n    on the previous data (up to n_{t-1}). In essence, the new weight\n    reflect how much better the particle fits the new (unseen) data\n    compared to the data it was trained on.\n\n    - Resampling: Adaptively resample particles based on the new weights.\n    This step is performed if the effective sample size (ESS) is below a\n    threshold (0.5 in this implementation). The ESS is a measure of how\n    diverse the particle weights are, and if it is low, it means that many\n    particles have similar weights, and resampling is needed to avoid\n    degeneracy. There are various resampling strategies available, we\n    default to stratified resampling.\n\n    - Rejuvenation: Rejuvenate particles using MCMC and HMC steps. This\n    step is performed on all particles, and consists of two main moves:\n        1. Structure Move: Proposes a new kernel structure based on the\n        `kernel_prior`.\n        2. Parameter Rejuvenation: If the structure move is accepted,\n        rejuvenates the kernel parameters and potentially the noise variance\n        using Hamiltonian Monte Carlo (HMC).\n\n\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key used for sampling.\n    particle_states : nnx.State\n        The particle states at the start of the SMC loop.\n    annealing_schedule : Int[jnp.ndarray, \" T\"]\n        The annealing schedule, an array of integers representing\n        the number of data points to use at each time point\n        (e.g. jnp.array([10, 20, 30, 40, 50]) ).\n    num_particles : int\n        The number of particles (must match number of particles\n        in the particle_states).\n    data : Dataset\n        The dataset object containing the input x and output y for\n        all data points.\n    kernel_prior : KernelPrior\n        The kernel prior object, used to reconstruct the kernel, and sample\n        new kernel structures.\n    noise_prior : Distribution\n        The prior distribution for the noise variance.\n    fix_noise : bool\n        Whether to fix the noise variance during rejuvenation, or sample it.\n    hmc_sampler_factory : tp.Callable\n        A factory function that creates an HMC sampler instance for the parameter\n        rejuvenation step.\n    n_mcmc : ScalarInt\n        The number of MCMC steps to perform during rejuvenation.\n    n_hmc : ScalarInt\n        The number of HMC steps to perform during rejuvenation.\n    resampling_func : tp.Callable, optional\n        The resampling function to use, by default stratified.\n    ess_threshold : float, optional\n        The threshold for the effective sample size (ESS) below which\n        resampling is performed, by default 0.5.\n    verbosity : int, optional\n        The verbosity level for the rejuvenation process. Debug information\n        is printed if verbosity &gt; 0.\n\n    Returns\n    -------\n    GPState\n        The final SMC state after the SMC loop.\n    list[GPState]\n        The full history of the SMC state at each time point in the\n        annealing schedule.\n\n    \"\"\"\n\n    def smc_round_wrapper(\n        smc_state: GPState,\n        points_to_use: int,\n    ) -&gt; GPState:\n        \"\"\"Wrapper around the smc_round function.\"\"\"\n\n        x_slice = lax.slice(data.x, (0,), (points_to_use,))\n        y_slice = lax.slice(data.y, (0,), (points_to_use,))\n\n        kernel_prior_round = deepcopy(kernel_prior)\n        kernel_prior_round.num_datapoints = points_to_use\n\n        data_subset = Dataset(x=x_slice, y=y_slice)\n\n        return smc_round(\n            smc_state,\n            num_particles,\n            n_mcmc,\n            n_hmc,\n            data_subset,\n            kernel_prior_round,\n            noise_prior,\n            fix_noise,\n            hmc_sampler_factory,\n            resampling_func,\n            ess_threshold,\n            data.x.shape[0],\n            verbosity,\n        )\n\n    # initialise SMC state\n    initial_smc_state = GPState(\n        particle_states=particle_states,\n        log_weights=jnp.log(jnp.ones(num_particles) / num_particles),\n        marginal_log_likelihoods=jnp.zeros(num_particles),\n        num_particles=num_particles,\n        num_data_points=jnp.array(0),\n        resampled=False,\n        mcmc_accepted=jnp.zeros(num_particles, dtype=int),\n        hmc_accepted=jnp.zeros(num_particles, dtype=int),\n        key=key,\n    )\n\n    history = []\n    for i, points_to_use in enumerate(annealing_schedule):\n        if verbosity &gt; 0:\n            print(\n                f\"Running SMC round [{i+1}/{len(annealing_schedule)}] \"\n                f\"with [{points_to_use}/{len(data.x)}] data points.\"\n            )\n\n        final_smc_state = smc_round_wrapper(\n            initial_smc_state,\n            points_to_use,\n        )\n        initial_smc_state = final_smc_state\n        history.append(final_smc_state)\n\n    return final_smc_state, history\n</code></pre>"},{"location":"autoapi/gallifrey/inference/smc/#gallifrey.inference.smc.smc_round","title":"<code>smc_round(smc_state, num_particles, n_mcmc, n_hmc, data_subset, kernel_prior, noise_prior, fix_noise, hmc_sampler_factory, resampling_func, ess_threshold, total_data_points, verbosity=0)</code>","text":"<p>Performs one round of Sequential Monte Carlo (SMC).</p> <p>This function executes a single SMC round, which includes reweighting particles based on new data, resampling particles if the effective sample size is below a threshold, and rejuvenating particles using MCMC and HMC steps.</p> <p>Parameters:</p> Name Type Description Default <code>smc_state</code> <code>GPState</code> <p>The current SMC state.</p> required <code>num_particles</code> <code>ScalarInt</code> <p>The number of particles.</p> required <code>n_mcmc</code> <code>ScalarInt</code> <p>The number of MCMC steps to perform during rejuvenation.</p> required <code>n_hmc</code> <code>ScalarInt</code> <p>The number of HMC steps within each MCMC step during rejuvenation.</p> required <code>data_subset</code> <code>Dataset</code> <p>The data to be considered for this SMC round.</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior object.</p> required <code>noise_prior</code> <code>Distribution</code> <p>The prior distribution for the noise parameter.</p> required <code>fix_noise</code> <code>bool</code> <p>Whether to fix the noise parameter during rejuvenation.</p> required <code>hmc_sampler_factory</code> <code>Callable</code> <p>A factory function that creates an HMC sampler.</p> required <code>resampling_func</code> <code>Callable</code> <p>The resampling function to use.</p> required <code>ess_threshold</code> <code>float</code> <p>The threshold for the effective sample size (ESS) below which resampling is performed.</p> required <code>total_data_points</code> <code>ScalarInt</code> <p>The total number of data points in the dataset.</p> required <code>verbosity</code> <code>ScalarInt</code> <p>The verbosity level for the rejuvenation process. Debug information is printed if verbosity &gt; 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>SMState</code> <p>The updated SMC state after the round.</p> Source code in <code>gallifrey/inference/smc.py</code> <pre><code>def smc_round(\n    smc_state: GPState,\n    num_particles: ScalarInt,\n    n_mcmc: ScalarInt,\n    n_hmc: ScalarInt,\n    data_subset: Dataset,\n    kernel_prior: KernelPrior,\n    noise_prior: Distribution,\n    fix_noise: bool,\n    hmc_sampler_factory: tp.Callable,\n    resampling_func: tp.Callable,\n    ess_threshold: float,\n    total_data_points: ScalarInt,\n    verbosity: ScalarInt = 0,\n) -&gt; GPState:\n    \"\"\"\n    Performs one round of Sequential Monte Carlo (SMC).\n\n    This function executes a single SMC round, which includes reweighting particles\n    based on new data, resampling particles if the effective sample size is below\n    a threshold, and rejuvenating particles using MCMC and HMC steps.\n\n    Parameters\n    ----------\n    smc_state : GPState\n        The current SMC state.\n    num_particles : ScalarInt\n        The number of particles.\n    n_mcmc : ScalarInt\n        The number of MCMC steps to perform during rejuvenation.\n    n_hmc : ScalarInt\n        The number of HMC steps within each MCMC step during rejuvenation.\n    data_subset : Dataset\n        The data to be considered for this SMC round.\n    kernel_prior : KernelPrior\n        The kernel prior object.\n    noise_prior : Distribution\n        The prior distribution for the noise parameter.\n    fix_noise : bool\n        Whether to fix the noise parameter during rejuvenation.\n    hmc_sampler_factory : tp.Callable\n        A factory function that creates an HMC sampler.\n    resampling_func : tp.Callable\n        The resampling function to use.\n    ess_threshold : float\n        The threshold for the effective sample size (ESS) below which\n        resampling is performed.\n    total_data_points : ScalarInt\n        The total number of data points in the dataset.\n    verbosity : ScalarInt, optional\n        The verbosity level for the rejuvenation process. Debug information\n        is printed if verbosity &gt; 0.\n\n    Returns\n    -------\n    SMState\n        The updated SMC state after the round.\n\n    \"\"\"\n\n    key, resample_key, rejuvenate_key, state_key = jr.split(\n        smc_state.key,  # type: ignore\n        4,\n    )\n\n    # reweight\n    new_log_weights = reweight_particles(\n        smc_state,\n        data_subset,\n        kernel_prior,\n    )\n    if verbosity &gt; 0:\n        print(f\"Weights: {jnp.exp(new_log_weights)}\")\n\n    # get number of data points for this round\n    num_data_points = data_subset.x.shape[0]\n\n    # maybe resample, depending on effective sample size (ESS)\n    # (if resample, also reset log weights to uniform)\n    normalised_ess = (\n        calculate_effective_sample_size(new_log_weights)\n        / smc_state.num_particles  # type: ignore\n    )\n    # do not resample if we are at last step\n    do_resample = (normalised_ess &lt; ess_threshold) &amp; (\n        num_data_points &lt; total_data_points\n    )\n\n    resampled_particle_states, new_log_weights, resampled = lax.cond(\n        do_resample,\n        lambda key: (\n            *resample_particles(\n                resample_key,\n                smc_state.particle_states,\n                new_log_weights,\n                num_particles,\n                resampling_func,\n            ),\n            True,\n        ),\n        lambda key: (smc_state.particle_states, new_log_weights, False),\n        key,\n    )\n    if verbosity &gt; 0:\n        print(\n            f\"Resampled: {resampled} (Normalised ESS: \"\n            f\"{float(normalised_ess):.2f})\"  # type: ignore\n        )\n\n    # rejuvenate\n    @jit\n    def wrapper(\n        key: PRNGKeyArray,\n        state: nnx.State,\n    ) -&gt; tuple[nnx.State, nnx.State, ScalarInt, ScalarInt]:\n        \"\"\"Wrapper around the rejuvenate_particle function using\n        GPmodel attributes.\"\"\"\n        return rejuvenate_particle(\n            key,\n            state,\n            data_subset,\n            kernel_prior,\n            noise_prior,\n            n_mcmc=n_mcmc,\n            n_hmc=n_hmc,\n            fix_noise=fix_noise,\n            hmc_sampler_factory=hmc_sampler_factory,\n            verbosity=verbosity,\n        )\n\n    # run rejuvenation on all particles, parallelised\n    final_state, _, accepted_mcmc, accepted_hmc = pmap(wrapper, in_axes=0)(\n        jr.split(rejuvenate_key, num_particles),  # type: ignore\n        resampled_particle_states,\n    )\n\n    if verbosity &gt; 0:\n        for i, acc_mcmc, acc_hmc in zip(\n            range(smc_state.num_particles),  # type: ignore\n            accepted_mcmc,\n            accepted_hmc,\n        ):\n            print(\n                f\"Particle {i+1} | Accepted: MCMC[{acc_mcmc}/{n_mcmc}] \"\n                f\" HMC[{acc_hmc}/{acc_mcmc*n_hmc}]\"\n            )\n        print(\"=\" * 50)\n\n    # calculate annealing MLL\n    new_mlls = jnp.array(\n        jit(\n            vmap(calculate_mll_wrapper, in_axes=(0, None, None)),\n            static_argnames=(\"data\", \"kernel_prior\"),\n        )(\n            final_state,\n            data_subset,\n            kernel_prior,\n        )\n    )\n\n    return GPState(\n        particle_states=final_state,\n        log_weights=new_log_weights,\n        marginal_log_likelihoods=new_mlls,\n        num_particles=smc_state.num_particles,\n        num_data_points=num_data_points,\n        resampled=resampled,\n        mcmc_accepted=accepted_mcmc,\n        hmc_accepted=accepted_hmc,\n        key=state_key,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/inference/state/","title":"state","text":""},{"location":"autoapi/gallifrey/inference/state/#gallifrey.inference.state.GPState","title":"<code>GPState</code>","text":"<p>A dataclass to hold the state of the SMC or MCMC algorithm.</p> <p>Attributes:</p> Name Type Description <code>particle_states</code> <code>State</code> <p>The particle states (batched into a single State object).</p> <code>num_particles</code> <code>Int[ArrayLike, '...']</code> <p>The number of particles.</p> <code>num_data_points</code> <code>Int[ArrayLike, '...']</code> <p>The number of data points used in the SMC round. (All data points are used in the MCMC algorithm.)</p> <code>mcmc_accepted</code> <code>Int[ArrayLike, '...']</code> <p>The number of accepted MCMC steps per particle.</p> <code>hmc_accepted</code> <code>Int[ArrayLike, '...']</code> <p>The number of accepted HMC steps per particle.</p> <code>log_weights</code> <code>Float[ArrayLike, '...']</code> <p>The log weights of the particles (normalised). (Only used in the SMC algorithm, default is None.)</p> <code>marginal_log_likelihoods</code> <code>Float[ArrayLike, '...']</code> <p>The marginal log likelihoods of the particles, at the current point in the anneaing schedule. (Only used in the SMC algorithm, default is None.)</p> <code>resampled</code> <code>Bool[ArrayLike, '...']</code> <p>Whether the particles have been resampled in the current round. (Only used in the SMC algorithm, default is None.)</p> <code>key</code> <code>Optional[PRNGKeyArray | ArrayLike]</code> <p>The random key for this round. (Only used in the SMC algorithm, default is None.)</p> Source code in <code>gallifrey/inference/state.py</code> <pre><code>@struct.dataclass\nclass GPState:\n    \"\"\"\n    A dataclass to hold the state of the SMC or\n    MCMC algorithm.\n\n    Attributes\n    ----------\n    particle_states : nnx.State\n        The particle states (batched into a single State object).\n    num_particles : Int[ArrayLike, \"...\"]\n        The number of particles.\n    num_data_points : Int[ArrayLike, \"...\"]\n        The number of data points used in the SMC round. (All data\n        points are used in the MCMC algorithm.)\n    mcmc_accepted : Int[ArrayLike, \"...\"]\n        The number of accepted MCMC steps per particle.\n    hmc_accepted : Int[ArrayLike, \"...\"]\n        The number of accepted HMC steps per particle.\n    log_weights : Float[ArrayLike, \"...\"]\n        The log weights of the particles (normalised). (Only\n        used in the SMC algorithm, default is None.)\n    marginal_log_likelihoods : Float[ArrayLike, \"...\"]\n        The marginal log likelihoods of the particles, at the current\n        point in the anneaing schedule. (Only used in the SMC\n        algorithm, default is None.)\n    resampled : Bool[ArrayLike, \"...\"]\n        Whether the particles have been resampled in the current round.\n        (Only used in the SMC algorithm, default is None.)\n    key : tp.Optional[PRNGKeyArray | ArrayLike]\n        The random key for this round. (Only used in the SMC\n        algorithm, default is None.)\n\n    \"\"\"\n\n    # overly permissive type hints to allow for checkpointing\n    # TODO: find a better way to handle this\n\n    particle_states: tp.Any\n    num_particles: tp.Any\n    num_data_points: tp.Any  # num data points used in the SMC round\n    mcmc_accepted: tp.Any\n    hmc_accepted: tp.Any\n    log_weights: tp.Any = None\n    marginal_log_likelihoods: tp.Any = None\n    resampled: tp.Any = None\n    key: tp.Any = None\n\n    @classmethod\n    def from_dict(cls, state_dict: dict) -&gt; GPState:\n        \"\"\"\n        Create a GPState object from a dictionary.\n\n        Parameters\n        ----------\n        state_dict : dict\n            The dictionary containing the state. Must contain keys\n            matching the attributes of the GPState class.\n        \"\"\"\n        return GPState(**state_dict)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/state/#gallifrey.inference.state.GPState.from_dict","title":"<code>from_dict(state_dict)</code>  <code>classmethod</code>","text":"<p>Create a GPState object from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict</code> <p>The dictionary containing the state. Must contain keys matching the attributes of the GPState class.</p> required Source code in <code>gallifrey/inference/state.py</code> <pre><code>@classmethod\ndef from_dict(cls, state_dict: dict) -&gt; GPState:\n    \"\"\"\n    Create a GPState object from a dictionary.\n\n    Parameters\n    ----------\n    state_dict : dict\n        The dictionary containing the state. Must contain keys\n        matching the attributes of the GPState class.\n    \"\"\"\n    return GPState(**state_dict)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/","title":"transforms","text":""},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LinearTransform","title":"<code>LinearTransform</code>","text":"<p>               Bases: <code>Transform</code></p> <p>Class for linear transformations.</p> <p>The transformation is defined as y = slope * x + intercept.</p> <p>Attributes:</p> Name Type Description <code>slope</code> <code>float</code> <p>The slope of the linear transformation.</p> <code>intercept</code> <code>float</code> <p>The intercept of the linear transformation.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>class LinearTransform(Transform):\n    \"\"\"\n    Class for linear transformations.\n\n    The transformation is defined as y = slope * x + intercept.\n\n    Attributes\n    ----------\n    slope : float\n        The slope of the linear transformation.\n    intercept : float\n        The intercept of the linear transformation.\n    \"\"\"\n\n    def __init__(self, slope: ScalarFloat, intercept: ScalarFloat):\n        \"\"\"\n        Initializes the LinearTransform object.\n\n        Parameters\n        ----------\n        slope : ScalarFloat\n            The slope of the linear transformation.\n        intercept : ScalarFloat\n            The intercept of the linear transformation.\n        \"\"\"\n        self.slope = jnp.asarray(slope)\n        self.intercept = jnp.asarray(intercept)\n\n    def apply(\n        self,\n        x: Float[ArrayLike, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        \"\"\"\n        Applies the linear transformation to input x.\n\n        Parameters\n        ----------\n        x :  Float[ArrayLike, \"...\"]\n            The input data.\n\n        Returns\n        -------\n         Float[jnp.ndarray, \"...\"]\n            The transformed data.\n\n        \"\"\"\n        return self.slope * x + self.intercept\n\n    def unapply(\n        self,\n        x: Float[ArrayLike, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        \"\"\"\n        Unapplies the linear transformation to input x.\n\n        Parameters\n        ----------\n        x :  Float[ArrayLike, \"...\"]\n            The (reverse) transformed data.\n\n        Returns\n        -------\n         Float[jnp.ndarray, \"...\"]\n            The un-transformed data.\n        \"\"\"\n        return jnp.asarray((x - self.intercept) / self.slope)\n\n    def apply_mean(\n        self,\n        mean_val: Float[ArrayLike, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        \"\"\"\n        Applies the linear transformation to a mean value.\n\n        Parameters\n        ----------\n        mean_val : Float[ArrayLike, \"...\"]\n            The mean value to be transformed.\n\n        Returns\n        -------\n        Float[jnp.ndarray, \"...\"]\n            The transformed mean value.\n        \"\"\"\n        return self.apply(jnp.asarray(mean_val))\n\n    def unapply_mean(\n        self,\n        mean_val: Float[ArrayLike, \"...\"],\n    ) -&gt; Float[ArrayLike, \"...\"]:\n        \"\"\"\n        Unapplies the linear transformation to a mean value.\n\n        Parameters\n        ----------\n        mean_val : Float[ArrayLike, \"...\"]\n            The mean value to be un-transformed.\n\n        Returns\n        -------\n        Float[ArrayLike, \"...\"]\n            The un-transformed mean value.\n        \"\"\"\n        return self.unapply(jnp.asarray(mean_val))\n\n    def apply_var(\n        self,\n        var_val: Float[ArrayLike, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        \"\"\"\n        Applies the linear transformation to a variance value.\n\n        Parameters\n        ----------\n        var_val : Float[ArrayLike, \"...\"]\n            The variance value to be transformed.\n\n        Returns\n        -------\n        Float[jnp.ndarray, \"...\"]\n            The transformed variance value.\n        \"\"\"\n        return jnp.asarray(self.slope**2 * var_val)\n\n    def unapply_var(\n        self,\n        var_val: Float[ArrayLike, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        \"\"\"\n        Unapplies the linear transformation to a variance value.\n\n        Parameters\n        ----------\n        var_val : Float[ArrayLike, \"...\"]\n            The variance value to be un-transformed.\n\n        Returns\n        -------\n        Float[jnp.ndarray, \"...\"]\n            The un-transformed variance value.\n        \"\"\"\n        return jnp.asarray((1 / (self.slope**2)) * var_val)\n\n    def apply_mean_var(\n        self,\n        mean_val: Float[ArrayLike, \"...\"],\n        var_val: Float[ArrayLike, \"...\"],\n    ) -&gt; tuple[Float[jnp.ndarray, \"...\"], Float[jnp.ndarray, \"...\"]]:\n        \"\"\"\n        Applies the linear transformation to mean and variance values.\n\n        Parameters\n        ----------\n        mean_val : Float[ArrayLike, \"...\"]\n            The mean value to be transformed.\n        var_val : Float[ArrayLike, \"...\"]\n            The variance value to be transformed.\n\n        Returns\n        -------\n        tuple[Float[jnp.ndarray, \"...\"], Float[jnp.ndarray, \"...\"]]\n            A tuple containing the transformed mean and variance values.\n\n        \"\"\"\n        m = self.apply_mean(mean_val)\n        v = self.apply_var(var_val)\n        return (m, v)\n\n    def unapply_mean_var(\n        self,\n        mean_val: Float[ArrayLike, \"...\"],\n        var_val: Float[ArrayLike, \"...\"],\n    ) -&gt; tuple[Float[ArrayLike, \"...\"], Float[ArrayLike, \"...\"]]:\n        \"\"\"\n        Unapplies the linear transformation to mean and variance values.\n\n        Parameters\n        ----------\n        mean_val : Float[ArrayLike, \"...\"]\n            The mean value to be un-transformed.\n        var_val : Float[ArrayLike, \"...\"]\n            The variance value to be un-transformed.\n\n        Returns\n        -------\n        tuple[Float[ArrayLike, \"...\"], Float[ArrayLike, \"...\"]]\n            A tuple containing the un-transformed mean and variance values.\n\n        \"\"\"\n        m = self.unapply_mean(mean_val)\n        v = self.unapply_var(var_val)\n        return (m, v)\n\n    @classmethod\n    def from_data_range(\n        cls,\n        data: Float[ArrayLike, \"...\"],\n        lo: ScalarFloat | ScalarInt,\n        hi: ScalarFloat | ScalarInt,\n    ) -&gt; LinearTransform:\n        \"\"\"\n        Creates a LinearTransform instance such that data\n        is scaled to [lo, hi].\n\n        NaN values are ignored in the calculation.\n\n        Parameters\n        ----------\n        data :  Float[ArrayLike, \"...\"]\n            The input data.\n        lo : ScalarFloat | ScalarInt\n            The lower bound of the desired range.\n        hi : ScalarFloat | ScalarInt\n            The upper bound of the desired\n\n        Returns\n        -------\n        LinearTransform\n            A LinearTransform instance with slope and intercept\n            such that data is scaled to [lo, hi].\n\n        Raises\n        ------\n        ValueError\n            If the input data contains less than 2 non-NaN values.\n\n        \"\"\"\n        tnan = jnp.asarray(data)[~jnp.isnan(data)]\n        if len(tnan) &lt; 2:\n            raise ValueError(\"Cannot scale with &lt;2 values.\")\n        tmin = jnp.min(tnan)\n        tmax = jnp.max(tnan)\n        a = hi - lo\n        b = tmax - tmin\n        slope = a / b\n        intercept = -slope * tmin + lo\n        return cls(slope, intercept)\n\n    @classmethod\n    def from_data_width(\n        cls,\n        data: Float[ArrayLike, \"...\"],\n        width: ScalarFloat | ScalarInt,\n    ) -&gt; LinearTransform:\n        \"\"\"\n        Creates a LinearTransform instance such that the width of the data\n        is scaled to the given width, i.e., the data is scaled to\n        [-width/2, width/2].\n\n        NaN values are ignored in the calculation.\n\n        Parameters\n        ----------\n        data :  Float[ArrayLike, \"...\"]\n            The input data.\n        width : ScalarFloat | ScalarInt\n            The desired width of the data.\n\n        Returns\n        -------\n        LinearTransform\n            A LinearTransform instance with slope and intercept\n            such that the data is scaled to [-width/2, width/2].\n\n        Raises\n        ------\n        ValueError\n            If the input data contains less than 2 non-NaN values.\n        \"\"\"\n        tnan = jnp.asarray(data)[~jnp.isnan(data)]\n        if len(tnan) &lt; 2:\n            raise ValueError(\"Cannot scale with &lt;2 values.\")\n\n        a = tnan.max() - tnan.min()\n        slope = width / a\n        intercept = -(jnp.asarray(width) * tnan.mean()) / a\n        return cls(slope, intercept)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LinearTransform.__init__","title":"<code>__init__(slope, intercept)</code>","text":"<p>Initializes the LinearTransform object.</p> <p>Parameters:</p> Name Type Description Default <code>slope</code> <code>ScalarFloat</code> <p>The slope of the linear transformation.</p> required <code>intercept</code> <code>ScalarFloat</code> <p>The intercept of the linear transformation.</p> required Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>def __init__(self, slope: ScalarFloat, intercept: ScalarFloat):\n    \"\"\"\n    Initializes the LinearTransform object.\n\n    Parameters\n    ----------\n    slope : ScalarFloat\n        The slope of the linear transformation.\n    intercept : ScalarFloat\n        The intercept of the linear transformation.\n    \"\"\"\n    self.slope = jnp.asarray(slope)\n    self.intercept = jnp.asarray(intercept)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LinearTransform.apply","title":"<code>apply(x)</code>","text":"<p>Applies the linear transformation to input x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code> Float[ArrayLike, \"...\"]</code> <p>The input data.</p> required <p>Returns:</p> Type Description <code> Float[jnp.ndarray, \"...\"]</code> <p>The transformed data.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>def apply(\n    self,\n    x: Float[ArrayLike, \"...\"],\n) -&gt; Float[jnp.ndarray, \"...\"]:\n    \"\"\"\n    Applies the linear transformation to input x.\n\n    Parameters\n    ----------\n    x :  Float[ArrayLike, \"...\"]\n        The input data.\n\n    Returns\n    -------\n     Float[jnp.ndarray, \"...\"]\n        The transformed data.\n\n    \"\"\"\n    return self.slope * x + self.intercept\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LinearTransform.apply_mean","title":"<code>apply_mean(mean_val)</code>","text":"<p>Applies the linear transformation to a mean value.</p> <p>Parameters:</p> Name Type Description Default <code>mean_val</code> <code>Float[ArrayLike, '...']</code> <p>The mean value to be transformed.</p> required <p>Returns:</p> Type Description <code>Float[ndarray, '...']</code> <p>The transformed mean value.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>def apply_mean(\n    self,\n    mean_val: Float[ArrayLike, \"...\"],\n) -&gt; Float[jnp.ndarray, \"...\"]:\n    \"\"\"\n    Applies the linear transformation to a mean value.\n\n    Parameters\n    ----------\n    mean_val : Float[ArrayLike, \"...\"]\n        The mean value to be transformed.\n\n    Returns\n    -------\n    Float[jnp.ndarray, \"...\"]\n        The transformed mean value.\n    \"\"\"\n    return self.apply(jnp.asarray(mean_val))\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LinearTransform.apply_mean_var","title":"<code>apply_mean_var(mean_val, var_val)</code>","text":"<p>Applies the linear transformation to mean and variance values.</p> <p>Parameters:</p> Name Type Description Default <code>mean_val</code> <code>Float[ArrayLike, '...']</code> <p>The mean value to be transformed.</p> required <code>var_val</code> <code>Float[ArrayLike, '...']</code> <p>The variance value to be transformed.</p> required <p>Returns:</p> Type Description <code>tuple[Float[ndarray, '...'], Float[ndarray, '...']]</code> <p>A tuple containing the transformed mean and variance values.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>def apply_mean_var(\n    self,\n    mean_val: Float[ArrayLike, \"...\"],\n    var_val: Float[ArrayLike, \"...\"],\n) -&gt; tuple[Float[jnp.ndarray, \"...\"], Float[jnp.ndarray, \"...\"]]:\n    \"\"\"\n    Applies the linear transformation to mean and variance values.\n\n    Parameters\n    ----------\n    mean_val : Float[ArrayLike, \"...\"]\n        The mean value to be transformed.\n    var_val : Float[ArrayLike, \"...\"]\n        The variance value to be transformed.\n\n    Returns\n    -------\n    tuple[Float[jnp.ndarray, \"...\"], Float[jnp.ndarray, \"...\"]]\n        A tuple containing the transformed mean and variance values.\n\n    \"\"\"\n    m = self.apply_mean(mean_val)\n    v = self.apply_var(var_val)\n    return (m, v)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LinearTransform.apply_var","title":"<code>apply_var(var_val)</code>","text":"<p>Applies the linear transformation to a variance value.</p> <p>Parameters:</p> Name Type Description Default <code>var_val</code> <code>Float[ArrayLike, '...']</code> <p>The variance value to be transformed.</p> required <p>Returns:</p> Type Description <code>Float[ndarray, '...']</code> <p>The transformed variance value.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>def apply_var(\n    self,\n    var_val: Float[ArrayLike, \"...\"],\n) -&gt; Float[jnp.ndarray, \"...\"]:\n    \"\"\"\n    Applies the linear transformation to a variance value.\n\n    Parameters\n    ----------\n    var_val : Float[ArrayLike, \"...\"]\n        The variance value to be transformed.\n\n    Returns\n    -------\n    Float[jnp.ndarray, \"...\"]\n        The transformed variance value.\n    \"\"\"\n    return jnp.asarray(self.slope**2 * var_val)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LinearTransform.from_data_range","title":"<code>from_data_range(data, lo, hi)</code>  <code>classmethod</code>","text":"<p>Creates a LinearTransform instance such that data is scaled to [lo, hi].</p> <p>NaN values are ignored in the calculation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code> Float[ArrayLike, \"...\"]</code> <p>The input data.</p> required <code>lo</code> <code>ScalarFloat | ScalarInt</code> <p>The lower bound of the desired range.</p> required <code>hi</code> <code>ScalarFloat | ScalarInt</code> <p>The upper bound of the desired</p> required <p>Returns:</p> Type Description <code>LinearTransform</code> <p>A LinearTransform instance with slope and intercept such that data is scaled to [lo, hi].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input data contains less than 2 non-NaN values.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>@classmethod\ndef from_data_range(\n    cls,\n    data: Float[ArrayLike, \"...\"],\n    lo: ScalarFloat | ScalarInt,\n    hi: ScalarFloat | ScalarInt,\n) -&gt; LinearTransform:\n    \"\"\"\n    Creates a LinearTransform instance such that data\n    is scaled to [lo, hi].\n\n    NaN values are ignored in the calculation.\n\n    Parameters\n    ----------\n    data :  Float[ArrayLike, \"...\"]\n        The input data.\n    lo : ScalarFloat | ScalarInt\n        The lower bound of the desired range.\n    hi : ScalarFloat | ScalarInt\n        The upper bound of the desired\n\n    Returns\n    -------\n    LinearTransform\n        A LinearTransform instance with slope and intercept\n        such that data is scaled to [lo, hi].\n\n    Raises\n    ------\n    ValueError\n        If the input data contains less than 2 non-NaN values.\n\n    \"\"\"\n    tnan = jnp.asarray(data)[~jnp.isnan(data)]\n    if len(tnan) &lt; 2:\n        raise ValueError(\"Cannot scale with &lt;2 values.\")\n    tmin = jnp.min(tnan)\n    tmax = jnp.max(tnan)\n    a = hi - lo\n    b = tmax - tmin\n    slope = a / b\n    intercept = -slope * tmin + lo\n    return cls(slope, intercept)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LinearTransform.from_data_width","title":"<code>from_data_width(data, width)</code>  <code>classmethod</code>","text":"<p>Creates a LinearTransform instance such that the width of the data is scaled to the given width, i.e., the data is scaled to [-width/2, width/2].</p> <p>NaN values are ignored in the calculation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code> Float[ArrayLike, \"...\"]</code> <p>The input data.</p> required <code>width</code> <code>ScalarFloat | ScalarInt</code> <p>The desired width of the data.</p> required <p>Returns:</p> Type Description <code>LinearTransform</code> <p>A LinearTransform instance with slope and intercept such that the data is scaled to [-width/2, width/2].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input data contains less than 2 non-NaN values.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>@classmethod\ndef from_data_width(\n    cls,\n    data: Float[ArrayLike, \"...\"],\n    width: ScalarFloat | ScalarInt,\n) -&gt; LinearTransform:\n    \"\"\"\n    Creates a LinearTransform instance such that the width of the data\n    is scaled to the given width, i.e., the data is scaled to\n    [-width/2, width/2].\n\n    NaN values are ignored in the calculation.\n\n    Parameters\n    ----------\n    data :  Float[ArrayLike, \"...\"]\n        The input data.\n    width : ScalarFloat | ScalarInt\n        The desired width of the data.\n\n    Returns\n    -------\n    LinearTransform\n        A LinearTransform instance with slope and intercept\n        such that the data is scaled to [-width/2, width/2].\n\n    Raises\n    ------\n    ValueError\n        If the input data contains less than 2 non-NaN values.\n    \"\"\"\n    tnan = jnp.asarray(data)[~jnp.isnan(data)]\n    if len(tnan) &lt; 2:\n        raise ValueError(\"Cannot scale with &lt;2 values.\")\n\n    a = tnan.max() - tnan.min()\n    slope = width / a\n    intercept = -(jnp.asarray(width) * tnan.mean()) / a\n    return cls(slope, intercept)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LinearTransform.unapply","title":"<code>unapply(x)</code>","text":"<p>Unapplies the linear transformation to input x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code> Float[ArrayLike, \"...\"]</code> <p>The (reverse) transformed data.</p> required <p>Returns:</p> Type Description <code> Float[jnp.ndarray, \"...\"]</code> <p>The un-transformed data.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>def unapply(\n    self,\n    x: Float[ArrayLike, \"...\"],\n) -&gt; Float[jnp.ndarray, \"...\"]:\n    \"\"\"\n    Unapplies the linear transformation to input x.\n\n    Parameters\n    ----------\n    x :  Float[ArrayLike, \"...\"]\n        The (reverse) transformed data.\n\n    Returns\n    -------\n     Float[jnp.ndarray, \"...\"]\n        The un-transformed data.\n    \"\"\"\n    return jnp.asarray((x - self.intercept) / self.slope)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LinearTransform.unapply_mean","title":"<code>unapply_mean(mean_val)</code>","text":"<p>Unapplies the linear transformation to a mean value.</p> <p>Parameters:</p> Name Type Description Default <code>mean_val</code> <code>Float[ArrayLike, '...']</code> <p>The mean value to be un-transformed.</p> required <p>Returns:</p> Type Description <code>Float[ArrayLike, '...']</code> <p>The un-transformed mean value.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>def unapply_mean(\n    self,\n    mean_val: Float[ArrayLike, \"...\"],\n) -&gt; Float[ArrayLike, \"...\"]:\n    \"\"\"\n    Unapplies the linear transformation to a mean value.\n\n    Parameters\n    ----------\n    mean_val : Float[ArrayLike, \"...\"]\n        The mean value to be un-transformed.\n\n    Returns\n    -------\n    Float[ArrayLike, \"...\"]\n        The un-transformed mean value.\n    \"\"\"\n    return self.unapply(jnp.asarray(mean_val))\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LinearTransform.unapply_mean_var","title":"<code>unapply_mean_var(mean_val, var_val)</code>","text":"<p>Unapplies the linear transformation to mean and variance values.</p> <p>Parameters:</p> Name Type Description Default <code>mean_val</code> <code>Float[ArrayLike, '...']</code> <p>The mean value to be un-transformed.</p> required <code>var_val</code> <code>Float[ArrayLike, '...']</code> <p>The variance value to be un-transformed.</p> required <p>Returns:</p> Type Description <code>tuple[Float[ArrayLike, '...'], Float[ArrayLike, '...']]</code> <p>A tuple containing the un-transformed mean and variance values.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>def unapply_mean_var(\n    self,\n    mean_val: Float[ArrayLike, \"...\"],\n    var_val: Float[ArrayLike, \"...\"],\n) -&gt; tuple[Float[ArrayLike, \"...\"], Float[ArrayLike, \"...\"]]:\n    \"\"\"\n    Unapplies the linear transformation to mean and variance values.\n\n    Parameters\n    ----------\n    mean_val : Float[ArrayLike, \"...\"]\n        The mean value to be un-transformed.\n    var_val : Float[ArrayLike, \"...\"]\n        The variance value to be un-transformed.\n\n    Returns\n    -------\n    tuple[Float[ArrayLike, \"...\"], Float[ArrayLike, \"...\"]]\n        A tuple containing the un-transformed mean and variance values.\n\n    \"\"\"\n    m = self.unapply_mean(mean_val)\n    v = self.unapply_var(var_val)\n    return (m, v)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LinearTransform.unapply_var","title":"<code>unapply_var(var_val)</code>","text":"<p>Unapplies the linear transformation to a variance value.</p> <p>Parameters:</p> Name Type Description Default <code>var_val</code> <code>Float[ArrayLike, '...']</code> <p>The variance value to be un-transformed.</p> required <p>Returns:</p> Type Description <code>Float[ndarray, '...']</code> <p>The un-transformed variance value.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>def unapply_var(\n    self,\n    var_val: Float[ArrayLike, \"...\"],\n) -&gt; Float[jnp.ndarray, \"...\"]:\n    \"\"\"\n    Unapplies the linear transformation to a variance value.\n\n    Parameters\n    ----------\n    var_val : Float[ArrayLike, \"...\"]\n        The variance value to be un-transformed.\n\n    Returns\n    -------\n    Float[jnp.ndarray, \"...\"]\n        The un-transformed variance value.\n    \"\"\"\n    return jnp.asarray((1 / (self.slope**2)) * var_val)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LogTransform","title":"<code>LogTransform</code>","text":"<p>               Bases: <code>Transform</code></p> <p>Class for log transformations.</p> <p>The transformation is defined as y = log(x).</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>class LogTransform(Transform):\n    \"\"\"\n    Class for log transformations.\n\n    The transformation is defined as y = log(x).\n\n    \"\"\"\n\n    def apply(\n        self,\n        x: Float[ArrayLike, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        \"\"\"\n        Applies the log transformation to input x.\n\n        Parameters\n        ----------\n        x : Float[ArrayLike, \"...\"]\n            The input data.\n\n        Returns\n        -------\n        Float[jnp.ndarray, \"...\"]\n            The transformed data.\n\n        \"\"\"\n\n        return jnp.log(x)\n\n    def unapply(\n        self,\n        x: Float[ArrayLike, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        \"\"\"\n        Unapplies the log transformation to input x.\n\n        Parameters\n        ----------\n        x : Float[ArrayLike, \"...\"]\n            The (reverse) transformed data.\n\n        Returns\n        -------\n         Float[jnp.ndarray, \"...\"]\n            The un-transformed data.\n        \"\"\"\n        return jnp.exp(x)\n\n    def unapply_mean_var(\n        self,\n        mean_val: Float[ArrayLike, \"...\"],\n        var_val: Float[ArrayLike, \"...\"],\n    ) -&gt; tuple[Float[jnp.ndarray, \"...\"], Float[jnp.ndarray, \"...\"]]:\n        \"\"\"\n        Unapplies the log transformation to mean and variance values.\n\n        Parameters\n        ----------\n        mean_val : Float[ArrayLike, \"...\"]\n            The mean value to be un-transformed.\n        var_val : Float[ArrayLike, \"...\"]\n            The variance value to be un-transformed.\n\n        Returns\n        -------\n        tuple[Float[jnp.ndarray, \"...\"], Float[jnp.ndarray, \"...\"]]\n            A tuple containing the un-transformed mean and variance values.\n        \"\"\"\n        m = jnp.exp(mean_val + var_val / 2)\n        v = (jnp.exp(var_val) - 1) * jnp.exp(2 * mean_val + var_val)\n        return (m, v)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LogTransform.apply","title":"<code>apply(x)</code>","text":"<p>Applies the log transformation to input x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[ArrayLike, '...']</code> <p>The input data.</p> required <p>Returns:</p> Type Description <code>Float[ndarray, '...']</code> <p>The transformed data.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>def apply(\n    self,\n    x: Float[ArrayLike, \"...\"],\n) -&gt; Float[jnp.ndarray, \"...\"]:\n    \"\"\"\n    Applies the log transformation to input x.\n\n    Parameters\n    ----------\n    x : Float[ArrayLike, \"...\"]\n        The input data.\n\n    Returns\n    -------\n    Float[jnp.ndarray, \"...\"]\n        The transformed data.\n\n    \"\"\"\n\n    return jnp.log(x)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LogTransform.unapply","title":"<code>unapply(x)</code>","text":"<p>Unapplies the log transformation to input x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[ArrayLike, '...']</code> <p>The (reverse) transformed data.</p> required <p>Returns:</p> Type Description <code> Float[jnp.ndarray, \"...\"]</code> <p>The un-transformed data.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>def unapply(\n    self,\n    x: Float[ArrayLike, \"...\"],\n) -&gt; Float[jnp.ndarray, \"...\"]:\n    \"\"\"\n    Unapplies the log transformation to input x.\n\n    Parameters\n    ----------\n    x : Float[ArrayLike, \"...\"]\n        The (reverse) transformed data.\n\n    Returns\n    -------\n     Float[jnp.ndarray, \"...\"]\n        The un-transformed data.\n    \"\"\"\n    return jnp.exp(x)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.LogTransform.unapply_mean_var","title":"<code>unapply_mean_var(mean_val, var_val)</code>","text":"<p>Unapplies the log transformation to mean and variance values.</p> <p>Parameters:</p> Name Type Description Default <code>mean_val</code> <code>Float[ArrayLike, '...']</code> <p>The mean value to be un-transformed.</p> required <code>var_val</code> <code>Float[ArrayLike, '...']</code> <p>The variance value to be un-transformed.</p> required <p>Returns:</p> Type Description <code>tuple[Float[ndarray, '...'], Float[ndarray, '...']]</code> <p>A tuple containing the un-transformed mean and variance values.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>def unapply_mean_var(\n    self,\n    mean_val: Float[ArrayLike, \"...\"],\n    var_val: Float[ArrayLike, \"...\"],\n) -&gt; tuple[Float[jnp.ndarray, \"...\"], Float[jnp.ndarray, \"...\"]]:\n    \"\"\"\n    Unapplies the log transformation to mean and variance values.\n\n    Parameters\n    ----------\n    mean_val : Float[ArrayLike, \"...\"]\n        The mean value to be un-transformed.\n    var_val : Float[ArrayLike, \"...\"]\n        The variance value to be un-transformed.\n\n    Returns\n    -------\n    tuple[Float[jnp.ndarray, \"...\"], Float[jnp.ndarray, \"...\"]]\n        A tuple containing the un-transformed mean and variance values.\n    \"\"\"\n    m = jnp.exp(mean_val + var_val / 2)\n    v = (jnp.exp(var_val) - 1) * jnp.exp(2 * mean_val + var_val)\n    return (m, v)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.Transform","title":"<code>Transform</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data transformations.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>class Transform(ABC):\n    \"\"\"Abstract base class for data transformations.\"\"\"\n\n    def __call__(\n        self,\n        x: Float[ArrayLike, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        \"\"\"Applies the transformation to input.\"\"\"\n        return self.apply(x)\n\n    @abstractmethod\n    def apply(\n        self,\n        x: Float[ArrayLike, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        \"\"\"Applies the transformation to input.\"\"\"\n        pass\n\n    @abstractmethod\n    def unapply(\n        self,\n        x: Float[ArrayLike, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        \"\"\"Unapplies the transformation to input.\"\"\"\n        pass\n\n    @abstractmethod\n    def apply_var(\n        self,\n        var_val: Float[ArrayLike, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        \"\"\"Applies the transformation to a variance value.\"\"\"\n        pass\n\n    @abstractmethod\n    def unapply_var(\n        self,\n        var_val: Float[ArrayLike, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        \"\"\"Unapplies the transformation to a variance value.\"\"\"\n        pass\n\n    @classmethod\n    @abstractmethod\n    def from_data_range(\n        cls,\n        data: Float[ArrayLike, \"...\"],\n        lo: ScalarFloat | ScalarInt,\n        hi: ScalarFloat | ScalarInt,\n    ) -&gt; Transform:\n        \"\"\"Creates a Transform instance such that data is scaled to [lo, hi].\"\"\"\n        pass\n\n    @classmethod\n    @abstractmethod\n    def from_data_width(\n        cls,\n        data: Float[ArrayLike, \"...\"],\n        width: ScalarFloat | ScalarInt,\n    ) -&gt; Transform:\n        \"\"\"Creates a Transform instance such that the width of the data\n        is scaled to the given width.\"\"\"\n        pass\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.Transform.__call__","title":"<code>__call__(x)</code>","text":"<p>Applies the transformation to input.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>def __call__(\n    self,\n    x: Float[ArrayLike, \"...\"],\n) -&gt; Float[jnp.ndarray, \"...\"]:\n    \"\"\"Applies the transformation to input.\"\"\"\n    return self.apply(x)\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.Transform.apply","title":"<code>apply(x)</code>  <code>abstractmethod</code>","text":"<p>Applies the transformation to input.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>@abstractmethod\ndef apply(\n    self,\n    x: Float[ArrayLike, \"...\"],\n) -&gt; Float[jnp.ndarray, \"...\"]:\n    \"\"\"Applies the transformation to input.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.Transform.apply_var","title":"<code>apply_var(var_val)</code>  <code>abstractmethod</code>","text":"<p>Applies the transformation to a variance value.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>@abstractmethod\ndef apply_var(\n    self,\n    var_val: Float[ArrayLike, \"...\"],\n) -&gt; Float[jnp.ndarray, \"...\"]:\n    \"\"\"Applies the transformation to a variance value.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.Transform.from_data_range","title":"<code>from_data_range(data, lo, hi)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Creates a Transform instance such that data is scaled to [lo, hi].</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_data_range(\n    cls,\n    data: Float[ArrayLike, \"...\"],\n    lo: ScalarFloat | ScalarInt,\n    hi: ScalarFloat | ScalarInt,\n) -&gt; Transform:\n    \"\"\"Creates a Transform instance such that data is scaled to [lo, hi].\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.Transform.from_data_width","title":"<code>from_data_width(data, width)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Creates a Transform instance such that the width of the data is scaled to the given width.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_data_width(\n    cls,\n    data: Float[ArrayLike, \"...\"],\n    width: ScalarFloat | ScalarInt,\n) -&gt; Transform:\n    \"\"\"Creates a Transform instance such that the width of the data\n    is scaled to the given width.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.Transform.unapply","title":"<code>unapply(x)</code>  <code>abstractmethod</code>","text":"<p>Unapplies the transformation to input.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>@abstractmethod\ndef unapply(\n    self,\n    x: Float[ArrayLike, \"...\"],\n) -&gt; Float[jnp.ndarray, \"...\"]:\n    \"\"\"Unapplies the transformation to input.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/gallifrey/inference/transforms/#gallifrey.inference.transforms.Transform.unapply_var","title":"<code>unapply_var(var_val)</code>  <code>abstractmethod</code>","text":"<p>Unapplies the transformation to a variance value.</p> Source code in <code>gallifrey/inference/transforms.py</code> <pre><code>@abstractmethod\ndef unapply_var(\n    self,\n    var_val: Float[ArrayLike, \"...\"],\n) -&gt; Float[jnp.ndarray, \"...\"]:\n    \"\"\"Unapplies the transformation to a variance value.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/","title":"kernels","text":""},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.ConstantAtom","title":"<code>ConstantAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Constant atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class ConstantAtom(AbstractAtom):\n    \"\"\"Constant atom.\"\"\"\n\n    name = \"Constant\"\n    num_parameter = 1\n    parameter_support = [\"real\"]\n    parameter_names = [\"constant\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        constant = params[0]\n        return jnp.asarray(constant).astype(jnp.asarray(x).dtype).squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.KernelLibrary","title":"<code>KernelLibrary</code>","text":"<p>A class to store the kernel library, which contains information about what atoms and operators are available for constructing the tree kernel.</p> <p>Attributes:</p> Name Type Description <code>atoms</code> <code>Optional[list[AbstractAtom]]</code> <p>A list of atoms that are used to construct the tree kernel. If None, the default atoms are used. Atoms should inherit from the AbstractAtom class in gallifrey.kernels.atoms.</p> <code>operators</code> <code>Optional[list[AbstractOperator]]</code> <p>A list of operators that are used to combine the atoms in the tree kernel. If None, the default operators are used. Operators should inherit from the AbstractOperator class in gallifrey.kernels.atoms.</p> <code>num_atoms</code> <code>int</code> <p>The number of atoms in the library.</p> <code>num_operators</code> <code>int</code> <p>The number of operators in the library.</p> <code>library</code> <code>list[Union[AbstractAtom, AbstractOperator]]</code> <p>A list combining the atoms and operators in the library.</p> <code>is_operator</code> <code>ndarray</code> <p>A boolean array to indicate if an entry in the library is an operator or not.</p> <code>max_atom_parameters</code> <code>int</code> <p>The maximum number of parameters any atom or operator in the library takes as input.</p> <code>prior_transforms</code> <code>Optional[dict[str, Bijector]]</code> <p>A dictionary that maps the parameter tags to the corresponding transform functions. Transformation functions must transform parameter from a standard normal distribution to the desired (prior) distribution, and should be implemented via tensorflow_probability bijectors. The default transformations are used if None. The default transformations are: - \"positive\": Log-normal transformation(mu=0.0, sigma=1.0) - \"real\": Log-normal transformation(mu=0.0, sigma=1.0) - \"sigmoid\": Logit-normal transform(scale=1.0, mu=0.0, sigma=1.0) NOTE: To make functions that use this dict jittable, the prior_transforms is converted to a FrozenDict, to make it hashable. NOTE: The \"none\" key is reserved for internal use and should not be used in the prior_transforms dictionary.</p> <code>support_tag_mapping</code> <code>dict[str, int]</code> <p>A dictionary that maps the support tags to integer values.</p> <code>support_mapping_array</code> <code>ndarray</code> <p>An array that contains the support tags as integers for each parameter in the library.</p> <code>support_transforms</code> <code>FrozenDict[str, Transformation]</code> <p>A (frozen) dict that maps the parameter tags to the corresponding transform functions. Transformation functions must transform parameter from a constrained space to an unconstrained space. This is used for sampling and optimization. NOTE: This should ideally never be touched, unless you know what you are doing.</p> Source code in <code>gallifrey/kernels/library.py</code> <pre><code>class KernelLibrary:\n    \"\"\"\n    A class to store the kernel library, which contains information\n    about what atoms and operators are available for constructing\n    the tree kernel.\n\n    Attributes\n    ----------\n    atoms : tp.Optional[list[AbstractAtom]]\n        A list of atoms that are used to construct the tree kernel.\n        If None, the default atoms are used. Atoms should inherit\n        from the AbstractAtom class in gallifrey.kernels.atoms.\n    operators : tp.Optional[list[AbstractOperator]]\n        A list of operators that are used to combine the atoms in the\n        tree kernel. If None, the default operators are used. Operators\n        should inherit from the AbstractOperator class in\n        gallifrey.kernels.atoms.\n    num_atoms : int\n        The number of atoms in the library.\n    num_operators : int\n        The number of operators in the library.\n    library : list[tp.Union[AbstractAtom, AbstractOperator]]\n        A list combining the atoms and operators in the library.\n    is_operator : jnp.ndarray\n        A boolean array to indicate if an entry in the library is an operator\n        or not.\n    max_atom_parameters : int\n        The maximum number of parameters any atom or operator in the library\n        takes as input.\n    prior_transforms : tp.Optional[dict[str, tfb.Bijector]]\n        A dictionary that maps the parameter tags to the corresponding\n        transform functions. Transformation functions must transform\n        parameter from a standard normal distribution to the desired\n        (prior) distribution, and should be implemented via\n        tensorflow_probability bijectors. The default transformations\n        are used if None. The default transformations are:\n        - \"positive\": Log-normal transformation(mu=0.0, sigma=1.0)\n        - \"real\": Log-normal transformation(mu=0.0, sigma=1.0)\n        - \"sigmoid\": Logit-normal transform(scale=1.0, mu=0.0, sigma=1.0)\n        NOTE: To make functions that use this dict jittable, the prior_transforms\n        is converted to a FrozenDict, to make it hashable.\n        NOTE: The \"none\" key is reserved for internal use and should not be\n        used in the prior_transforms dictionary.\n    support_tag_mapping : dict[str, int]\n        A dictionary that maps the support tags to integer values.\n    support_mapping_array : jnp.ndarray\n        An array that contains the support tags as integers for each parameter\n        in the library.\n    support_transforms : FrozenDict[str, Transformation]\n        A (frozen) dict that maps the parameter tags to the corresponding transform\n        functions. Transformation functions must transform parameter from a constrained\n        space to an unconstrained space. This is used for sampling and optimization.\n        NOTE: This should ideally never be touched, unless you know what you are doing.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        atoms: tp.Optional[list[AbstractAtom]] = None,\n        operators: tp.Optional[list[AbstractOperator]] = None,\n        prior_transforms: tp.Optional[dict[str, tfb.Bijector]] = None,\n    ):\n        \"\"\"\n        Initialize the KernelLibrary. If no atoms or operators are provided,\n        a default library is used.\n\n        Parameters\n        ----------\n        atoms : tp.Optional[list[AbstractAtom]], optional\n            A list of atoms that are used to construct the tree kernel.\n            If None, the default atoms are used. Atoms should inherit\n            from the AbstractAtom class in gallifrey.kernels.atoms, by default None.\n        operators : tp.Optional[list[AbstractOperator]], optional\n            A list of operators that are used to combine the atoms in the\n            tree kernel. If None, the default operators are used. Operators\n            should inherit from the AbstractOperator class in\n            gallifrey.kernels.atoms, by default None.\n        prior_transforms : tp.Optional[dict[str, Transformation]], optional\n            A dictionary that maps the parameter tags to the corresponding transform\n            functions. Transformation functions must transform parameter from a standard\n            normal distribution to the desired (prior) distribution, and should be\n            implemented via tensorflow_probability bijectors.\n            The default transformations are used if None, by default None. The default\n            transformations are:\n            - \"positive\": Log-normal transformation(mu=0.0, sigma=1.0)\n            - \"real\": Log-normal transformation(mu=0.0, sigma=1.0)\n            - \"sigmoid\": Logit-normal transform(scale=1.0, mu=0.0, sigma=1.0)\n            NOTE: To make 'transformation' function (see gallifrey.kernels.prior)\n            jitable, the prior_transforms must be hashable. This is why we use\n            FrozenDict instead of dict.\n\n        \"\"\"\n        self.atoms: list[AbstractAtom] = (\n            atoms\n            if atoms is not None\n            else [\n                LinearAtom(),\n                PeriodicAtom(),\n                RBFAtom(),\n            ]\n        )\n\n        self.operators: list[AbstractOperator] = (\n            operators\n            if operators is not None\n            else [\n                SumOperator(),\n                ProductOperator(),\n            ]\n        )\n\n        self.num_atoms = len(self.atoms)\n        self.num_operators = len(self.operators)\n\n        # construct the library by combining the atoms and operators\n        self.library = self.atoms + self.operators\n\n        # create boolean array to indicate if entry is operator or not\n        self.is_operator = jnp.array(\n            [False] * len(self.atoms) + [True] * len(self.operators)\n        )\n\n        # get the maximum number of parameters any atom or operator takes\n        self.max_atom_parameters = max([item.num_parameter for item in self.atoms])\n\n        # transformation functions from normal distribution to prior distribution\n        # NOTE: names are inherited from GPJax, but we don't use the same\n        # transformations (which is why sigmoid is the name for the logit-normal)\n        if prior_transforms is None:\n            self.prior_transforms: FrozenDict[str, tfb.Bijector] = FrozenDict(\n                {\n                    # this is the transformation y = exp(mu + sigma * z),\n                    # with mu = 0 and sigma = 1,\n                    # if z ~ normal(0, 1) then y ~ log-normal(mu, sigma)\n                    \"real\": tfb.Chain(\n                        [\n                            tfb.Exp(),\n                            tfb.Shift(jnp.array(0.0)),\n                            tfb.Scale(jnp.array(1.0)),\n                        ]\n                    ),\n                    \"positive\": tfb.Chain(\n                        [\n                            tfb.Exp(),\n                            tfb.Shift(jnp.array(0.0)),\n                            tfb.Scale(jnp.array(1.0)),\n                        ]\n                    ),\n                    # this is the transformation y = 1/(1 + exp(-(mu + sigma * z))),\n                    # with mu = 0 and sigma = 1,\n                    # if z ~ normal(0, 1) then y ~ logit-normal(mu, sigma)\n                    \"sigmoid\": tfb.Chain(\n                        [\n                            tfb.Sigmoid(\n                                low=jnp.array(0.0),\n                                high=jnp.array(0.95),  # to avoid numerical issues\n                            ),\n                            tfb.Shift(jnp.array(0.0)),\n                            tfb.Scale(jnp.array(0.0)),\n                        ]\n                    ),\n                    # identity transformation for parameter that do not fall under\n                    # the above categories\n                    \"none\": tfb.Identity(),\n                }\n            )\n\n        else:\n            if \"none\" in prior_transforms.keys():\n                raise ValueError(\n                    \"'prior_transforms' should not contain the key 'none', \"\n                    \"as it is reserved for internal use.\"\n                )\n            prior_transforms[\"none\"] = tfb.Identity()\n            self.prior_transforms = FrozenDict(prior_transforms)\n\n        self.support_mapping_array, self.support_tag_mapping = (\n            self.get_support_mapping()\n        )\n\n        # besides the prior transforms, we also perform transforms\n        # between a constrained and unconstrained space for fitting/sampling,\n        # this probably should never be touched unless you implement a new\n        # kernel with new constraints\n        self.support_transforms: FrozenDict[str, tfb.Bijector] = FrozenDict(\n            {\n                \"positive\": tfb.Softplus(),\n                \"real\": tfb.Identity(),\n                \"sigmoid\": tfb.Sigmoid(low=0.0, high=0.95),\n                \"lower_triangular\": tfb.FillTriangular(),\n                \"none\": tfb.Identity(),\n            }\n        )\n\n        self._check_tags()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Get the (technical) string representation of the KernelLibrary.\n\n        Returns\n        -------\n        str\n            The (technical) string representation of the KernelLibrary.\n        \"\"\"\n\n        return (\n            f\"KernelLibrary(\\n  Atoms={self.atoms},\\n   \"\n            f\"operators={self.operators}\\n)\"\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Get the (simplified) string representation of the KernelLibrary.\n\n        Returns\n        -------\n        str\n            The (simplified) string representation of the KernelLibrary.\n        \"\"\"\n        try:\n            atom_names: list[tp.Any] = [atom.name for atom in self.atoms]\n        except Exception:\n            atom_names = self.atoms\n        try:\n            operator_names: list[tp.Any] = [\n                operator.name for operator in self.operators\n            ]\n        except Exception:\n            operator_names = self.operators\n\n        return (\n            f\"KernelLibrary(\\n  Atoms={atom_names},\\n   \"\n            f\"operators={operator_names}\\n)\"\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Get the number of items in the library.\n\n        Returns\n        -------\n        int\n            The number of items in the library.\n        \"\"\"\n        return len(self.library)\n\n    def get_support_mapping(self) -&gt; tuple[jnp.ndarray, dict[str, int]]:\n        \"\"\"\n        Create a mapping between support tags and the integer values, then\n        create an array that contains the support tags for each parameter\n        in the library.\n        (Used for jittable sampling and parameter transformations)\n\n        Returns\n        -------\n        support_mapping_array : jnp.ndarray\n            An array that contains the support tags for each parameter\n            in the library, with shape (len(self), self.max_atom_parameters),\n            enocded as integers.\n        support_tag_mapping : dict[str, int]\n            A dictionary that maps the support tags to the corresponding\n            integer values.\n        \"\"\"\n\n        # create dict that maps string tags to integer values\n        support_tag_mapping = {\n            support_tag: i for i, support_tag in enumerate(self.prior_transforms.keys())\n        }\n        support_tag_mapping[\"none\"] = -1\n\n        # create array that contains the support tag integers for each parameter\n        # in the library\n        support_mapping_array = jnp.full(\n            (len(self), self.max_atom_parameters), support_tag_mapping[\"none\"]\n        )\n\n        for i in range(len(self)):\n            for j in range(self.library[i].num_parameter):\n                support_mapping_array = support_mapping_array.at[i, j].set(\n                    support_tag_mapping[self.library[i].parameter_support[j]]\n                )\n        return support_mapping_array, support_tag_mapping\n\n    def _check_tags(self) -&gt; None:\n        \"\"\"\n        Check if the parameters of the atoms have constraints that are not yet\n        implemented in the library. If so, raise a warning.\n\n        \"\"\"\n        parameter_support_tags = {\n            atom.name: atom.parameter_support for atom in self.atoms\n        }\n\n        for atom_name, tags in parameter_support_tags.items():\n            if not all(tag in self.prior_transforms.keys() for tag in tags):\n                warnings.warn(\n                    f\"Atom {atom_name!r} has parameter tags that are not \"\n                    \"in the prior_transforms dictionary. Tag will be ignored. \"\n                    \"This will lead to problems when performing parameter \"\n                    \"transformations. Add the missing tags to the prior_transforms\"\n                    \"by manually passing the prior_transforms dictionary to the \"\n                    \"KernelLibrary constructor.\"\n                )\n\n        if not all(tag in self.support_transforms.keys() for tag in tags):\n            warnings.warn(\n                f\"Kernel {atom_name!r} has parameter tags that are not \"\n                \"in the support_transforms dictionary. This will lead to problems \"\n                \"in the sampling/optmization if used for a KernelTree object. \"\n                \"Add the missing tags to the by manually overriding the \"\n                \"support_transforms dictionary to the KernelLibrary constructor.\\n\"\n                \"THIS IS VERY EXPERIMENTAL AND NOT RECOMMENDED.\"\n            )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.KernelLibrary.__init__","title":"<code>__init__(atoms=None, operators=None, prior_transforms=None)</code>","text":"<p>Initialize the KernelLibrary. If no atoms or operators are provided, a default library is used.</p> <p>Parameters:</p> Name Type Description Default <code>atoms</code> <code>Optional[list[AbstractAtom]]</code> <p>A list of atoms that are used to construct the tree kernel. If None, the default atoms are used. Atoms should inherit from the AbstractAtom class in gallifrey.kernels.atoms, by default None.</p> <code>None</code> <code>operators</code> <code>Optional[list[AbstractOperator]]</code> <p>A list of operators that are used to combine the atoms in the tree kernel. If None, the default operators are used. Operators should inherit from the AbstractOperator class in gallifrey.kernels.atoms, by default None.</p> <code>None</code> <code>prior_transforms</code> <code>Optional[dict[str, Transformation]]</code> <p>A dictionary that maps the parameter tags to the corresponding transform functions. Transformation functions must transform parameter from a standard normal distribution to the desired (prior) distribution, and should be implemented via tensorflow_probability bijectors. The default transformations are used if None, by default None. The default transformations are: - \"positive\": Log-normal transformation(mu=0.0, sigma=1.0) - \"real\": Log-normal transformation(mu=0.0, sigma=1.0) - \"sigmoid\": Logit-normal transform(scale=1.0, mu=0.0, sigma=1.0) NOTE: To make 'transformation' function (see gallifrey.kernels.prior) jitable, the prior_transforms must be hashable. This is why we use FrozenDict instead of dict.</p> <code>None</code> Source code in <code>gallifrey/kernels/library.py</code> <pre><code>def __init__(\n    self,\n    atoms: tp.Optional[list[AbstractAtom]] = None,\n    operators: tp.Optional[list[AbstractOperator]] = None,\n    prior_transforms: tp.Optional[dict[str, tfb.Bijector]] = None,\n):\n    \"\"\"\n    Initialize the KernelLibrary. If no atoms or operators are provided,\n    a default library is used.\n\n    Parameters\n    ----------\n    atoms : tp.Optional[list[AbstractAtom]], optional\n        A list of atoms that are used to construct the tree kernel.\n        If None, the default atoms are used. Atoms should inherit\n        from the AbstractAtom class in gallifrey.kernels.atoms, by default None.\n    operators : tp.Optional[list[AbstractOperator]], optional\n        A list of operators that are used to combine the atoms in the\n        tree kernel. If None, the default operators are used. Operators\n        should inherit from the AbstractOperator class in\n        gallifrey.kernels.atoms, by default None.\n    prior_transforms : tp.Optional[dict[str, Transformation]], optional\n        A dictionary that maps the parameter tags to the corresponding transform\n        functions. Transformation functions must transform parameter from a standard\n        normal distribution to the desired (prior) distribution, and should be\n        implemented via tensorflow_probability bijectors.\n        The default transformations are used if None, by default None. The default\n        transformations are:\n        - \"positive\": Log-normal transformation(mu=0.0, sigma=1.0)\n        - \"real\": Log-normal transformation(mu=0.0, sigma=1.0)\n        - \"sigmoid\": Logit-normal transform(scale=1.0, mu=0.0, sigma=1.0)\n        NOTE: To make 'transformation' function (see gallifrey.kernels.prior)\n        jitable, the prior_transforms must be hashable. This is why we use\n        FrozenDict instead of dict.\n\n    \"\"\"\n    self.atoms: list[AbstractAtom] = (\n        atoms\n        if atoms is not None\n        else [\n            LinearAtom(),\n            PeriodicAtom(),\n            RBFAtom(),\n        ]\n    )\n\n    self.operators: list[AbstractOperator] = (\n        operators\n        if operators is not None\n        else [\n            SumOperator(),\n            ProductOperator(),\n        ]\n    )\n\n    self.num_atoms = len(self.atoms)\n    self.num_operators = len(self.operators)\n\n    # construct the library by combining the atoms and operators\n    self.library = self.atoms + self.operators\n\n    # create boolean array to indicate if entry is operator or not\n    self.is_operator = jnp.array(\n        [False] * len(self.atoms) + [True] * len(self.operators)\n    )\n\n    # get the maximum number of parameters any atom or operator takes\n    self.max_atom_parameters = max([item.num_parameter for item in self.atoms])\n\n    # transformation functions from normal distribution to prior distribution\n    # NOTE: names are inherited from GPJax, but we don't use the same\n    # transformations (which is why sigmoid is the name for the logit-normal)\n    if prior_transforms is None:\n        self.prior_transforms: FrozenDict[str, tfb.Bijector] = FrozenDict(\n            {\n                # this is the transformation y = exp(mu + sigma * z),\n                # with mu = 0 and sigma = 1,\n                # if z ~ normal(0, 1) then y ~ log-normal(mu, sigma)\n                \"real\": tfb.Chain(\n                    [\n                        tfb.Exp(),\n                        tfb.Shift(jnp.array(0.0)),\n                        tfb.Scale(jnp.array(1.0)),\n                    ]\n                ),\n                \"positive\": tfb.Chain(\n                    [\n                        tfb.Exp(),\n                        tfb.Shift(jnp.array(0.0)),\n                        tfb.Scale(jnp.array(1.0)),\n                    ]\n                ),\n                # this is the transformation y = 1/(1 + exp(-(mu + sigma * z))),\n                # with mu = 0 and sigma = 1,\n                # if z ~ normal(0, 1) then y ~ logit-normal(mu, sigma)\n                \"sigmoid\": tfb.Chain(\n                    [\n                        tfb.Sigmoid(\n                            low=jnp.array(0.0),\n                            high=jnp.array(0.95),  # to avoid numerical issues\n                        ),\n                        tfb.Shift(jnp.array(0.0)),\n                        tfb.Scale(jnp.array(0.0)),\n                    ]\n                ),\n                # identity transformation for parameter that do not fall under\n                # the above categories\n                \"none\": tfb.Identity(),\n            }\n        )\n\n    else:\n        if \"none\" in prior_transforms.keys():\n            raise ValueError(\n                \"'prior_transforms' should not contain the key 'none', \"\n                \"as it is reserved for internal use.\"\n            )\n        prior_transforms[\"none\"] = tfb.Identity()\n        self.prior_transforms = FrozenDict(prior_transforms)\n\n    self.support_mapping_array, self.support_tag_mapping = (\n        self.get_support_mapping()\n    )\n\n    # besides the prior transforms, we also perform transforms\n    # between a constrained and unconstrained space for fitting/sampling,\n    # this probably should never be touched unless you implement a new\n    # kernel with new constraints\n    self.support_transforms: FrozenDict[str, tfb.Bijector] = FrozenDict(\n        {\n            \"positive\": tfb.Softplus(),\n            \"real\": tfb.Identity(),\n            \"sigmoid\": tfb.Sigmoid(low=0.0, high=0.95),\n            \"lower_triangular\": tfb.FillTriangular(),\n            \"none\": tfb.Identity(),\n        }\n    )\n\n    self._check_tags()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.KernelLibrary.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of items in the library.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of items in the library.</p> Source code in <code>gallifrey/kernels/library.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Get the number of items in the library.\n\n    Returns\n    -------\n    int\n        The number of items in the library.\n    \"\"\"\n    return len(self.library)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.KernelLibrary.__repr__","title":"<code>__repr__()</code>","text":"<p>Get the (technical) string representation of the KernelLibrary.</p> <p>Returns:</p> Type Description <code>str</code> <p>The (technical) string representation of the KernelLibrary.</p> Source code in <code>gallifrey/kernels/library.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Get the (technical) string representation of the KernelLibrary.\n\n    Returns\n    -------\n    str\n        The (technical) string representation of the KernelLibrary.\n    \"\"\"\n\n    return (\n        f\"KernelLibrary(\\n  Atoms={self.atoms},\\n   \"\n        f\"operators={self.operators}\\n)\"\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.KernelLibrary.__str__","title":"<code>__str__()</code>","text":"<p>Get the (simplified) string representation of the KernelLibrary.</p> <p>Returns:</p> Type Description <code>str</code> <p>The (simplified) string representation of the KernelLibrary.</p> Source code in <code>gallifrey/kernels/library.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Get the (simplified) string representation of the KernelLibrary.\n\n    Returns\n    -------\n    str\n        The (simplified) string representation of the KernelLibrary.\n    \"\"\"\n    try:\n        atom_names: list[tp.Any] = [atom.name for atom in self.atoms]\n    except Exception:\n        atom_names = self.atoms\n    try:\n        operator_names: list[tp.Any] = [\n            operator.name for operator in self.operators\n        ]\n    except Exception:\n        operator_names = self.operators\n\n    return (\n        f\"KernelLibrary(\\n  Atoms={atom_names},\\n   \"\n        f\"operators={operator_names}\\n)\"\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.KernelLibrary.get_support_mapping","title":"<code>get_support_mapping()</code>","text":"<p>Create a mapping between support tags and the integer values, then create an array that contains the support tags for each parameter in the library. (Used for jittable sampling and parameter transformations)</p> <p>Returns:</p> Name Type Description <code>support_mapping_array</code> <code>ndarray</code> <p>An array that contains the support tags for each parameter in the library, with shape (len(self), self.max_atom_parameters), enocded as integers.</p> <code>support_tag_mapping</code> <code>dict[str, int]</code> <p>A dictionary that maps the support tags to the corresponding integer values.</p> Source code in <code>gallifrey/kernels/library.py</code> <pre><code>def get_support_mapping(self) -&gt; tuple[jnp.ndarray, dict[str, int]]:\n    \"\"\"\n    Create a mapping between support tags and the integer values, then\n    create an array that contains the support tags for each parameter\n    in the library.\n    (Used for jittable sampling and parameter transformations)\n\n    Returns\n    -------\n    support_mapping_array : jnp.ndarray\n        An array that contains the support tags for each parameter\n        in the library, with shape (len(self), self.max_atom_parameters),\n        enocded as integers.\n    support_tag_mapping : dict[str, int]\n        A dictionary that maps the support tags to the corresponding\n        integer values.\n    \"\"\"\n\n    # create dict that maps string tags to integer values\n    support_tag_mapping = {\n        support_tag: i for i, support_tag in enumerate(self.prior_transforms.keys())\n    }\n    support_tag_mapping[\"none\"] = -1\n\n    # create array that contains the support tag integers for each parameter\n    # in the library\n    support_mapping_array = jnp.full(\n        (len(self), self.max_atom_parameters), support_tag_mapping[\"none\"]\n    )\n\n    for i in range(len(self)):\n        for j in range(self.library[i].num_parameter):\n            support_mapping_array = support_mapping_array.at[i, j].set(\n                support_tag_mapping[self.library[i].parameter_support[j]]\n            )\n    return support_mapping_array, support_tag_mapping\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.KernelPrior","title":"<code>KernelPrior</code>","text":"<p>A prior distribution over kernel structures and parameters.</p> <p>Attributes:</p> Name Type Description <code>kernel_library</code> <code>KernelLibrary</code> <p>An instance of the KernelLibrary class, containing the kernel classes and operators, and transformation functions.</p> <code>kernel_structure_prior</code> <code>TreeStructurePrior</code> <p>A prior distribution over kernel structures.</p> <code>parameter_prior</code> <code>ParameterPrior</code> <p>A prior distribution over kernel parameters.</p> <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the nested kernel structure tree.</p> <code>num_datapoints</code> <code>ScalarInt</code> <p>The number of data points in the dataset.</p> <code>max_kernel_parameter</code> <code>int</code> <p>The maximum number of kernel parameters (max_atom_parameters * max_leaves).</p> <code>graphdef</code> <code>GraphDef</code> <p>The graph definition of the kernel. Used together with the kernel state to create a TreeKernel object.</p> <code>kernels</code> <code>List[Type[TreeKernel]]</code> <p>A list of kernel classes. (inherited from the kernel_library, see gallifrey.kernels.library.KernelLibrary)</p> <code>operators</code> <code>List[Callable]</code> <p>A list of operators. (inherited from the kernel_library, see gallifrey.kernels.library.KernelLibrary)</p> <code>is_operator</code> <code>Bool[ndarray, ' D']</code> <p>An array that indicates whether each kernel in the library is an operator. (inherited from the kernel_library, see gallifrey.kernels.library.KernelLibrary)</p> <code>probs</code> <code>Float[ndarray, ' D']</code> <p>The probabilities of sampling each kernel (or operator) in the library. (inherited from the kernel_structure_prior,   see gallifrey.kernels.prior.TreeStructurePrior)</p> <code>prior_transforms</code> <code>FrozenDict[str, Bijector]</code> <p>A (frozen) dictionary containing bijectors for transforming the distribution of the sampled parameters from a standard normal to the desired prior distribution. (inherited from the kernel_library, see gallifrey.kernels.library.KernelLibrary)</p> <code>support_bijectors</code> <code>tuple[Bijector, ...]</code> <p>A tuple of bijectors for transforming the parameters from a constrained support space to an unconstrained space. Primarely used for the numerically stable optimization and sampling of the parameters (in a jit-compatible way).</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>class KernelPrior:\n    \"\"\"\n    A prior distribution over kernel structures and parameters.\n\n    Attributes\n    ----------\n    kernel_library : KernelLibrary\n        An instance of the KernelLibrary class, containing\n        the kernel classes and operators, and transformation functions.\n    kernel_structure_prior : TreeStructurePrior\n        A prior distribution over kernel structures.\n    parameter_prior : ParameterPrior\n        A prior distribution over kernel parameters.\n    max_depth : ScalarInt\n        The maximum depth of the nested kernel structure tree.\n    num_datapoints : ScalarInt\n        The number of data points in the dataset.\n    max_kernel_parameter : int\n        The maximum number of kernel parameters (max_atom_parameters\n        * max_leaves).\n\n    graphdef : nnx.GraphDef\n        The graph definition of the kernel. Used together with the\n        kernel state to create a TreeKernel object.\n\n    kernels : tp.List[tp.Type[TreeKernel]]\n        A list of kernel classes. (inherited from the kernel_library,\n        see gallifrey.kernels.library.KernelLibrary)\n    operators : tp.List[tp.Callable]\n        A list of operators. (inherited from the kernel_library,\n        see gallifrey.kernels.library.KernelLibrary)\n    is_operator : Bool[jnp.ndarray, \" D\"]\n        An array that indicates whether each kernel in the library is an operator.\n        (inherited from the kernel_library, see gallifrey.kernels.library.KernelLibrary)\n    probs :  Float[jnp.ndarray, \" D\"]\n        The probabilities of sampling each kernel (or operator) in the library.\n        (inherited from the kernel_structure_prior,\n          see gallifrey.kernels.prior.TreeStructurePrior)\n    prior_transforms : FrozenDict[str, tfb.Bijector]\n        A (frozen) dictionary containing bijectors for transforming the\n        distribution of the sampled parameters from a standard\n        normal to the desired prior distribution. (inherited from the kernel_library,\n        see gallifrey.kernels.library.KernelLibrary)\n    support_bijectors : tuple[tfb.Bijector, ...]\n        A tuple of bijectors for transforming the parameters from a constrained\n        support space to an unconstrained space. Primarely used for the\n        numerically stable optimization and sampling of the parameters (in a\n        jit-compatible way).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel_library: KernelLibrary,\n        max_depth: int,\n        num_datapoints: int,\n        probs: tp.Optional[Float[jnp.ndarray, \" D\"]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the KernelPrior class.\n\n        Parameters\n        ----------\n        kernel_library : KernelLibrary\n            An instance of the KernelLibrary class, containing\n            the kernel classes and operators, and transformation functions.\n            The TreeStructurePrior and ParameterPrior classes are initialized\n            using the kernel_library.\n        max_depth : int\n            The maximum depth of the nested kernel structure tree.\n        num_datapoints : int\n            The number of data points in the dataset.\n        probs : tp.Optional[ Float[jnp.ndarray, \" D\"]], optional\n            The probabilities of sampling each kernel (or operator) in the library.\n            The array must have the same length as the the arrays in the kernel library.\n            By default None, which will use a uniform distribution.\n        \"\"\"\n        self.kernel_library = kernel_library\n\n        self.kernel_structure_prior = TreeStructurePrior(\n            self.kernel_library,\n            max_depth,\n            probs,\n        )\n\n        self.num_datapoints = num_datapoints\n\n        self.parameter_prior = ParameterPrior(self.kernel_library, max_depth)\n\n        self.graphdef = self._get_graphdef()\n\n        self.max_kernel_parameter = (\n            kernel_library.max_atom_parameters * calculate_max_leaves(self.max_depth)\n        )\n\n        self.support_bijectors = tuple(\n            [\n                self.kernel_library.support_transforms[tag]\n                for tag in self.kernel_library.support_tag_mapping\n            ]\n        )\n\n    def sample(self, key: PRNGKeyArray) -&gt; nnx.State:\n        \"\"\"\n        Sample a kernel structure and its parameters from the prior distribution.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for sampling.\n\n        Returns\n        -------\n        nnx.State\n        The state of the sampled kernel, combine with the graphdef\n        to create a TreeKernel object.\n\n        \"\"\"\n        tree_key, parameter_key = jr.split(key)\n        kernel_structure = self.kernel_structure_prior.sample(\n            tree_key,\n            self.kernel_structure_prior.max_depth,\n        )\n\n        kernel = TreeKernel(\n            kernel_structure,\n            self.kernel_library,\n            self.max_depth,\n            self.num_datapoints,\n        )\n\n        _, state = nnx.split(kernel)\n\n        kernel_state, _ = self.parameter_prior.sample(\n            parameter_key,\n            nnx.State(state),\n        )\n        return kernel_state\n\n    def sample_kernel(self, key: PRNGKeyArray) -&gt; TreeKernel:\n        \"\"\"\n        Sample a kernel state using self.sample and merge\n        it with the graphdef and static_state to create a\n        TreeKernel.\n\n        This function is convenient for sampling a kernel\n        directly, but not vmap or jittable (at least not\n        using the jax commands, potentially using the\n        nnx ones.)\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for sampling.\n\n        Returns\n        -------\n        TreeKernel\n            The sampled kernel.\n        \"\"\"\n\n        kernel_state = self.sample(key)\n        return nnx.merge(self.graphdef, kernel_state)\n\n    def reconstruct_kernel(\n        self,\n        kernel_state: nnx.State,\n    ) -&gt; TreeKernel:\n        \"\"\"\n        Create new TreeKernel from kernel state using the graphdef,\n        and reset some kernel attributes. In principle,\n        this is unnecessary since the values are already\n        set in the kernel state, but we need to make it\n        explicit for the jit compilation.\n\n        Parameters\n        ----------\n        kernel_state : nnx.State\n            The kernel state to be used to create the kernel.\n\n        Returns\n        -------\n        TreeKernel\n            The TreeKernel instance.\n        \"\"\"\n\n        kernel = nnx.merge(self.graphdef, kernel_state)\n\n        max_depth = self.max_depth\n        max_nodes = calculate_max_nodes(max_depth)\n        max_leaves = calculate_max_leaves(max_depth)\n        max_stack = calculate_max_stack_size(max_depth)\n        kernel.max_depth = kernel.max_depth.replace(max_depth)\n        kernel.max_nodes = kernel.max_nodes.replace(max_nodes)\n        kernel.max_leaves = kernel.max_leaves.replace(max_leaves)\n        kernel.max_stack = kernel.max_stack.replace(max_stack)\n        kernel.num_atoms = kernel.num_atoms.replace(self.kernel_library.num_atoms)\n        kernel.num_datapoints = kernel.num_datapoints.replace(self.num_datapoints)\n        return kernel\n\n    def _get_graphdef(\n        self,\n    ) -&gt; nnx.GraphDef:\n        \"\"\"\n        Create a random kernel and return the graphdef.\n\n        This function is used to create a graphdef for the\n        kernel (which is set as attribute in the __init__).\n        The graphdef should be the same for all possible\n        kernels, so we can reuse it whenever we need to\n        create a KernelTree object from a kernel state.\n\n        Returns\n        -------\n        nnx.GraphDef\n            The graph definition of the kernel.\n        \"\"\"\n\n        key = jr.PRNGKey(42)\n\n        kernel_structure = self.kernel_structure_prior.sample(\n            key,\n            self.kernel_structure_prior.max_depth,\n        )\n\n        kernel = TreeKernel(\n            kernel_structure,\n            self.kernel_library,\n            self.max_depth,\n            self.num_datapoints,\n        )\n\n        graphdef, _ = nnx.split(kernel)\n        return graphdef\n\n    @property\n    def atoms(self) -&gt; tp.List[AbstractAtom]:\n        return self.kernel_library.atoms\n\n    @property\n    def operators(self) -&gt; tp.List[AbstractOperator]:\n        return self.kernel_library.operators\n\n    @property\n    def is_operator(self) -&gt; Bool[jnp.ndarray, \" D\"]:\n        return self.kernel_library.is_operator\n\n    @property\n    def probs(self) -&gt; Float[jnp.ndarray, \" D\"]:\n        return self.kernel_structure_prior.probs\n\n    @property\n    def max_depth(self) -&gt; int:\n        return self.kernel_structure_prior.max_depth\n\n    @property\n    def prior_transforms(self) -&gt; FrozenDict[str, tfb.Bijector]:\n        return self.kernel_library.prior_transforms\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.KernelPrior.__init__","title":"<code>__init__(kernel_library, max_depth, num_datapoints, probs=None)</code>","text":"<p>Initialize the KernelPrior class.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_library</code> <code>KernelLibrary</code> <p>An instance of the KernelLibrary class, containing the kernel classes and operators, and transformation functions. The TreeStructurePrior and ParameterPrior classes are initialized using the kernel_library.</p> required <code>max_depth</code> <code>int</code> <p>The maximum depth of the nested kernel structure tree.</p> required <code>num_datapoints</code> <code>int</code> <p>The number of data points in the dataset.</p> required <code>probs</code> <code>Optional[Float[ndarray, ' D']]</code> <p>The probabilities of sampling each kernel (or operator) in the library. The array must have the same length as the the arrays in the kernel library. By default None, which will use a uniform distribution.</p> <code>None</code> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def __init__(\n    self,\n    kernel_library: KernelLibrary,\n    max_depth: int,\n    num_datapoints: int,\n    probs: tp.Optional[Float[jnp.ndarray, \" D\"]] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the KernelPrior class.\n\n    Parameters\n    ----------\n    kernel_library : KernelLibrary\n        An instance of the KernelLibrary class, containing\n        the kernel classes and operators, and transformation functions.\n        The TreeStructurePrior and ParameterPrior classes are initialized\n        using the kernel_library.\n    max_depth : int\n        The maximum depth of the nested kernel structure tree.\n    num_datapoints : int\n        The number of data points in the dataset.\n    probs : tp.Optional[ Float[jnp.ndarray, \" D\"]], optional\n        The probabilities of sampling each kernel (or operator) in the library.\n        The array must have the same length as the the arrays in the kernel library.\n        By default None, which will use a uniform distribution.\n    \"\"\"\n    self.kernel_library = kernel_library\n\n    self.kernel_structure_prior = TreeStructurePrior(\n        self.kernel_library,\n        max_depth,\n        probs,\n    )\n\n    self.num_datapoints = num_datapoints\n\n    self.parameter_prior = ParameterPrior(self.kernel_library, max_depth)\n\n    self.graphdef = self._get_graphdef()\n\n    self.max_kernel_parameter = (\n        kernel_library.max_atom_parameters * calculate_max_leaves(self.max_depth)\n    )\n\n    self.support_bijectors = tuple(\n        [\n            self.kernel_library.support_transforms[tag]\n            for tag in self.kernel_library.support_tag_mapping\n        ]\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.KernelPrior.reconstruct_kernel","title":"<code>reconstruct_kernel(kernel_state)</code>","text":"<p>Create new TreeKernel from kernel state using the graphdef, and reset some kernel attributes. In principle, this is unnecessary since the values are already set in the kernel state, but we need to make it explicit for the jit compilation.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_state</code> <code>State</code> <p>The kernel state to be used to create the kernel.</p> required <p>Returns:</p> Type Description <code>TreeKernel</code> <p>The TreeKernel instance.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def reconstruct_kernel(\n    self,\n    kernel_state: nnx.State,\n) -&gt; TreeKernel:\n    \"\"\"\n    Create new TreeKernel from kernel state using the graphdef,\n    and reset some kernel attributes. In principle,\n    this is unnecessary since the values are already\n    set in the kernel state, but we need to make it\n    explicit for the jit compilation.\n\n    Parameters\n    ----------\n    kernel_state : nnx.State\n        The kernel state to be used to create the kernel.\n\n    Returns\n    -------\n    TreeKernel\n        The TreeKernel instance.\n    \"\"\"\n\n    kernel = nnx.merge(self.graphdef, kernel_state)\n\n    max_depth = self.max_depth\n    max_nodes = calculate_max_nodes(max_depth)\n    max_leaves = calculate_max_leaves(max_depth)\n    max_stack = calculate_max_stack_size(max_depth)\n    kernel.max_depth = kernel.max_depth.replace(max_depth)\n    kernel.max_nodes = kernel.max_nodes.replace(max_nodes)\n    kernel.max_leaves = kernel.max_leaves.replace(max_leaves)\n    kernel.max_stack = kernel.max_stack.replace(max_stack)\n    kernel.num_atoms = kernel.num_atoms.replace(self.kernel_library.num_atoms)\n    kernel.num_datapoints = kernel.num_datapoints.replace(self.num_datapoints)\n    return kernel\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.KernelPrior.sample","title":"<code>sample(key)</code>","text":"<p>Sample a kernel structure and its parameters from the prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for sampling.</p> required <p>Returns:</p> Type Description <code>State</code> <code>The state of the sampled kernel, combine with the graphdef</code> <code>to create a TreeKernel object.</code> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def sample(self, key: PRNGKeyArray) -&gt; nnx.State:\n    \"\"\"\n    Sample a kernel structure and its parameters from the prior distribution.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for sampling.\n\n    Returns\n    -------\n    nnx.State\n    The state of the sampled kernel, combine with the graphdef\n    to create a TreeKernel object.\n\n    \"\"\"\n    tree_key, parameter_key = jr.split(key)\n    kernel_structure = self.kernel_structure_prior.sample(\n        tree_key,\n        self.kernel_structure_prior.max_depth,\n    )\n\n    kernel = TreeKernel(\n        kernel_structure,\n        self.kernel_library,\n        self.max_depth,\n        self.num_datapoints,\n    )\n\n    _, state = nnx.split(kernel)\n\n    kernel_state, _ = self.parameter_prior.sample(\n        parameter_key,\n        nnx.State(state),\n    )\n    return kernel_state\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.KernelPrior.sample_kernel","title":"<code>sample_kernel(key)</code>","text":"<p>Sample a kernel state using self.sample and merge it with the graphdef and static_state to create a TreeKernel.</p> <p>This function is convenient for sampling a kernel directly, but not vmap or jittable (at least not using the jax commands, potentially using the nnx ones.)</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for sampling.</p> required <p>Returns:</p> Type Description <code>TreeKernel</code> <p>The sampled kernel.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def sample_kernel(self, key: PRNGKeyArray) -&gt; TreeKernel:\n    \"\"\"\n    Sample a kernel state using self.sample and merge\n    it with the graphdef and static_state to create a\n    TreeKernel.\n\n    This function is convenient for sampling a kernel\n    directly, but not vmap or jittable (at least not\n    using the jax commands, potentially using the\n    nnx ones.)\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for sampling.\n\n    Returns\n    -------\n    TreeKernel\n        The sampled kernel.\n    \"\"\"\n\n    kernel_state = self.sample(key)\n    return nnx.merge(self.graphdef, kernel_state)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.LinearAtom","title":"<code>LinearAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>(Non-stationary) linear atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class LinearAtom(AbstractAtom):\n    \"\"\"(Non-stationary) linear atom.\"\"\"\n\n    name = \"Linear\"\n    num_parameter = 1\n    parameter_support = [\"positive\"]\n    parameter_names = [\"variance\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        variance = params[0]\n        k = variance * (x * y)\n        return jnp.asarray(k).squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.LinearWithShiftAtom","title":"<code>LinearWithShiftAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>(Non-stationary) linear atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class LinearWithShiftAtom(AbstractAtom):\n    \"\"\"(Non-stationary) linear atom.\"\"\"\n\n    name = \"Linear\"\n    num_parameter = 3\n    parameter_support = [\"positive\", \"positive\", \"real\"]\n    parameter_names = [\"bias\", \"variance\", \"shift\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        bias = params[0]\n        variance = params[1]\n        shift = params[2]\n        k = bias + variance * ((x - shift) * (y - shift))\n        return jnp.asarray(k).squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.Matern12Atom","title":"<code>Matern12Atom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Matern 1/2 atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class Matern12Atom(AbstractAtom):\n    \"\"\"Matern 1/2 atom.\"\"\"\n\n    name = \"Matern12\"\n    num_parameter = 2\n    parameter_support = [\"positive\", \"positive\"]\n    parameter_names = [\"lengthscale\", \"variance\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        lengthscale = params[0]\n        variance = params[1]\n        tau = jnp.abs(x - y) / lengthscale\n        k = variance * jnp.exp(-tau)\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.Matern32Atom","title":"<code>Matern32Atom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Matern 3/2 atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class Matern32Atom(AbstractAtom):\n    \"\"\"Matern 3/2 atom.\"\"\"\n\n    name = \"Matern32\"\n    num_parameter = 2\n    parameter_support = [\"positive\", \"positive\"]\n    parameter_names = [\"lengthscale\", \"variance\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        lengthscale = params[0]\n        variance = params[1]\n        tau = jnp.sqrt(3.0) * jnp.abs(x - y) / lengthscale\n        k = variance * (1.0 + tau) * jnp.exp(-tau)\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.Matern52Atom","title":"<code>Matern52Atom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Matern 5/2 atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class Matern52Atom(AbstractAtom):\n    \"\"\"Matern 5/2 atom.\"\"\"\n\n    name = \"Matern52\"\n    num_parameter = 2\n    parameter_support = [\"positive\", \"positive\"]\n    parameter_names = [\"lengthscale\", \"variance\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        lengthscale = params[0]\n        variance = params[1]\n        tau = jnp.sqrt(5.0) * jnp.abs(x - y) / lengthscale\n        k = variance * (1.0 + tau + jnp.square(tau) / 3.0) * jnp.exp(-tau)\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.ParameterPrior","title":"<code>ParameterPrior</code>","text":"<p>This class defines a prior distribution over kernel parameters.</p> <p>Attributes:</p> Name Type Description <code>num_parameter_array</code> <code>Int[ndarray, ' D']</code> <p>An array that contains the number of parameters for each atom in the kernel structure. Needs to be in same order as the atom library.</p> <code>max_atom_parameters</code> <code>int</code> <p>The maximum number of parameters for an atom in the kernel structure.</p> <code>max_leaves</code> <code>int</code> <p>The maximum number of leaves in the tree.</p> <code>max_nodes</code> <code>int</code> <p>The maximum number of nodes in the kernel structure.</p> <code>support_mapping_array</code> <code>Int[ndarray, ' D']</code> <p>An array that maps the support of the parameters to the corresponding bijector index.</p> <code>forward_bijectors</code> <code>tuple[Callable]</code> <p>A tuple of bijectors to transform the sampled parameters from a standard normal to the desired prior distribution.</p> <code>inverse_bijectors</code> <code>tuple[Callable]</code> <p>A tuple of bijectors to transform the sampled parameters from the prior distribution back to a standard normal.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>class ParameterPrior:\n    \"\"\"\n    This class defines a prior distribution over kernel parameters.\n\n    Attributes\n    ----------\n    num_parameter_array : Int[jnp.ndarray, \" D\"]\n        An array that contains the number of parameters for each\n        atom in the kernel structure. Needs to be in same order\n        as the atom library.\n    max_atom_parameters : int\n        The maximum number of parameters for an atom in the kernel structure.\n    max_leaves : int\n        The maximum number of leaves in the tree.\n    max_nodes : int\n        The maximum number of nodes in the kernel structure.\n    support_mapping_array : Int[jnp.ndarray, \" D\"]\n        An array that maps the support of the parameters to the\n        corresponding bijector index.\n    forward_bijectors : tuple[tfb.Callable]\n        A tuple of bijectors to transform the sampled parameters from\n        a standard normal to the desired prior distribution.\n    inverse_bijectors : tuple[tfb.Callable]\n        A tuple of bijectors to transform the sampled parameters from\n        the prior distribution back to a standard normal.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel_library: KernelLibrary,\n        max_depth: int,\n    ):\n        \"\"\"\n        Initialize the ParameterPrior class.\n\n        Parameters\n        ----------\n        kernel_library : KernelLibrary\n            An instance of the KernelLibrary class, containing\n            the kernel classes and operators, and transformation functions.\n            The prior_transforms dictionary is used to transform the\n            sampled parameters to the desired prior distribution.\n        max_depth : int\n            The maximum depth of the kernel tree.\n\n        \"\"\"\n        self.num_parameter_array = jnp.array(\n            [atom.num_parameter for atom in kernel_library.library]\n        )\n\n        self.max_atom_parameters = kernel_library.max_atom_parameters\n        self.max_leaves = int(calculate_max_leaves(max_depth))\n        self.max_nodes = int(calculate_max_nodes(max_depth))\n\n        self.support_mapping_array = kernel_library.support_mapping_array\n\n        # create tuples of callables for forward and inverse bijectors\n        self.forward_bijectors = tuple(\n            [\n                kernel_library.prior_transforms[tag].forward  # type: ignore\n                for tag in kernel_library.support_tag_mapping\n            ]\n        )\n\n        self.inverse_bijectors = tuple(\n            [\n                kernel_library.prior_transforms[tag].inverse  # type: ignore\n                for tag in kernel_library.support_tag_mapping\n            ]\n        )\n\n    def sample(\n        self,\n        key: PRNGKeyArray,\n        state: nnx.State,\n    ) -&gt; tuple[nnx.State, ScalarFloat]:\n        \"\"\"\n        Sample kernel parameter and assign it to the kernel. Also\n        returns the log probability of the sampled parameters.\n\n\n        The kernel parameter are sampled from a standard normal and\n        transformed to follow their corresponding prior distributions\n        defined by the parameter_transforms dictionary.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for sampling.\n        state : nnx.State\n            The original kernel state to be filled with new parameters.\n\n        Returns\n        -------\n        nnx.State\n            The state with the sampled parameters.\n        ScalarFloat\n            The log probability of the sampled parameters.\n\n        \"\"\"\n        return self.sample_subset(\n            key,\n            state,\n            jnp.arange(self.max_nodes),  # all nodes are considered\n        )\n\n    def sample_subset(\n        self,\n        key: PRNGKeyArray,\n        state: nnx.State,\n        considered_nodes: Int[jnp.ndarray, \" D\"],\n    ) -&gt; tuple[nnx.State, ScalarFloat]:\n        \"\"\"\n        Sample kernel parameter and assign it to the kernel.\n        Same as 'sample' method but with additional\n        parameter 'considered_nodes', which can be used\n        to only sample parameters for a subset of nodes.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for sampling.\n        state : nnx.State\n            The original kernel state to be filled with new parameters.\n        considered_nodes : Int[jnp.ndarray, \" D\"]\n            An array that contains the indices of the nodes that\n            are supposed to be sampled. (Padded with -1 if\n            necessary.)\n\n        Returns\n        -------\n        nnx.State\n            The state with the sampled parameters.\n        ScalarFloat\n            The log probability of the sampled parameters.\n        \"\"\"\n\n        new_state, log_prob = sample_parameters(\n            key,\n            state,\n            considered_nodes,\n            self.num_parameter_array,\n            self.max_leaves,\n            self.max_atom_parameters,\n            self.support_mapping_array,\n            self.forward_bijectors,\n        )\n        return new_state, log_prob\n\n    def log_prob(\n        self,\n        state: nnx.State,\n    ) -&gt; ScalarFloat:\n        \"\"\"\n        Compute the log probability of the kernel parameters.\n\n        Parameters\n        ----------\n        state : nnx.State\n            The kernel state with the parameters.\n\n        Returns\n        -------\n        ScalarFloat\n            The log probability of the kernel parameters.\n        \"\"\"\n\n        return self.log_prob_subset(\n            state,\n            jnp.arange(self.max_nodes),  # all nodes are considered\n        )\n\n    def log_prob_subset(\n        self,\n        state: nnx.State,\n        considered_nodes: Int[jnp.ndarray, \" D\"],\n    ) -&gt; ScalarFloat:\n        \"\"\"\n        Compute the log probability of the kernel parameters.\n        Same as 'log_prob' method but with additional\n        parameter 'considered_nodes', which can be used\n        to only calculate the log probability for a subset of nodes.\n\n        Parameters\n        ----------\n        state : nnx.State\n            The kernel state with the parameters.\n        considered_nodes : Int[jnp.ndarray, \" D\"]\n            An array that contains the indices of the nodes that\n            are supposed to be sampled. (Padded with -1 if\n            necessary.)\n\n        Returns\n        -------\n        ScalarFloat\n            The log probability of the kernel parameters.\n        \"\"\"\n\n        return log_prob_parameters(\n            state,\n            considered_nodes,\n            self.num_parameter_array,\n            self.max_leaves,\n            self.max_atom_parameters,\n            self.support_mapping_array,\n            self.inverse_bijectors,\n        )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.ParameterPrior.__init__","title":"<code>__init__(kernel_library, max_depth)</code>","text":"<p>Initialize the ParameterPrior class.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_library</code> <code>KernelLibrary</code> <p>An instance of the KernelLibrary class, containing the kernel classes and operators, and transformation functions. The prior_transforms dictionary is used to transform the sampled parameters to the desired prior distribution.</p> required <code>max_depth</code> <code>int</code> <p>The maximum depth of the kernel tree.</p> required Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def __init__(\n    self,\n    kernel_library: KernelLibrary,\n    max_depth: int,\n):\n    \"\"\"\n    Initialize the ParameterPrior class.\n\n    Parameters\n    ----------\n    kernel_library : KernelLibrary\n        An instance of the KernelLibrary class, containing\n        the kernel classes and operators, and transformation functions.\n        The prior_transforms dictionary is used to transform the\n        sampled parameters to the desired prior distribution.\n    max_depth : int\n        The maximum depth of the kernel tree.\n\n    \"\"\"\n    self.num_parameter_array = jnp.array(\n        [atom.num_parameter for atom in kernel_library.library]\n    )\n\n    self.max_atom_parameters = kernel_library.max_atom_parameters\n    self.max_leaves = int(calculate_max_leaves(max_depth))\n    self.max_nodes = int(calculate_max_nodes(max_depth))\n\n    self.support_mapping_array = kernel_library.support_mapping_array\n\n    # create tuples of callables for forward and inverse bijectors\n    self.forward_bijectors = tuple(\n        [\n            kernel_library.prior_transforms[tag].forward  # type: ignore\n            for tag in kernel_library.support_tag_mapping\n        ]\n    )\n\n    self.inverse_bijectors = tuple(\n        [\n            kernel_library.prior_transforms[tag].inverse  # type: ignore\n            for tag in kernel_library.support_tag_mapping\n        ]\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.ParameterPrior.log_prob","title":"<code>log_prob(state)</code>","text":"<p>Compute the log probability of the kernel parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The kernel state with the parameters.</p> required <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The log probability of the kernel parameters.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def log_prob(\n    self,\n    state: nnx.State,\n) -&gt; ScalarFloat:\n    \"\"\"\n    Compute the log probability of the kernel parameters.\n\n    Parameters\n    ----------\n    state : nnx.State\n        The kernel state with the parameters.\n\n    Returns\n    -------\n    ScalarFloat\n        The log probability of the kernel parameters.\n    \"\"\"\n\n    return self.log_prob_subset(\n        state,\n        jnp.arange(self.max_nodes),  # all nodes are considered\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.ParameterPrior.log_prob_subset","title":"<code>log_prob_subset(state, considered_nodes)</code>","text":"<p>Compute the log probability of the kernel parameters. Same as 'log_prob' method but with additional parameter 'considered_nodes', which can be used to only calculate the log probability for a subset of nodes.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The kernel state with the parameters.</p> required <code>considered_nodes</code> <code>Int[ndarray, ' D']</code> <p>An array that contains the indices of the nodes that are supposed to be sampled. (Padded with -1 if necessary.)</p> required <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The log probability of the kernel parameters.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def log_prob_subset(\n    self,\n    state: nnx.State,\n    considered_nodes: Int[jnp.ndarray, \" D\"],\n) -&gt; ScalarFloat:\n    \"\"\"\n    Compute the log probability of the kernel parameters.\n    Same as 'log_prob' method but with additional\n    parameter 'considered_nodes', which can be used\n    to only calculate the log probability for a subset of nodes.\n\n    Parameters\n    ----------\n    state : nnx.State\n        The kernel state with the parameters.\n    considered_nodes : Int[jnp.ndarray, \" D\"]\n        An array that contains the indices of the nodes that\n        are supposed to be sampled. (Padded with -1 if\n        necessary.)\n\n    Returns\n    -------\n    ScalarFloat\n        The log probability of the kernel parameters.\n    \"\"\"\n\n    return log_prob_parameters(\n        state,\n        considered_nodes,\n        self.num_parameter_array,\n        self.max_leaves,\n        self.max_atom_parameters,\n        self.support_mapping_array,\n        self.inverse_bijectors,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.ParameterPrior.sample","title":"<code>sample(key, state)</code>","text":"<p>Sample kernel parameter and assign it to the kernel. Also returns the log probability of the sampled parameters.</p> <p>The kernel parameter are sampled from a standard normal and transformed to follow their corresponding prior distributions defined by the parameter_transforms dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for sampling.</p> required <code>state</code> <code>State</code> <p>The original kernel state to be filled with new parameters.</p> required <p>Returns:</p> Type Description <code>State</code> <p>The state with the sampled parameters.</p> <code>ScalarFloat</code> <p>The log probability of the sampled parameters.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def sample(\n    self,\n    key: PRNGKeyArray,\n    state: nnx.State,\n) -&gt; tuple[nnx.State, ScalarFloat]:\n    \"\"\"\n    Sample kernel parameter and assign it to the kernel. Also\n    returns the log probability of the sampled parameters.\n\n\n    The kernel parameter are sampled from a standard normal and\n    transformed to follow their corresponding prior distributions\n    defined by the parameter_transforms dictionary.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for sampling.\n    state : nnx.State\n        The original kernel state to be filled with new parameters.\n\n    Returns\n    -------\n    nnx.State\n        The state with the sampled parameters.\n    ScalarFloat\n        The log probability of the sampled parameters.\n\n    \"\"\"\n    return self.sample_subset(\n        key,\n        state,\n        jnp.arange(self.max_nodes),  # all nodes are considered\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.ParameterPrior.sample_subset","title":"<code>sample_subset(key, state, considered_nodes)</code>","text":"<p>Sample kernel parameter and assign it to the kernel. Same as 'sample' method but with additional parameter 'considered_nodes', which can be used to only sample parameters for a subset of nodes.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for sampling.</p> required <code>state</code> <code>State</code> <p>The original kernel state to be filled with new parameters.</p> required <code>considered_nodes</code> <code>Int[ndarray, ' D']</code> <p>An array that contains the indices of the nodes that are supposed to be sampled. (Padded with -1 if necessary.)</p> required <p>Returns:</p> Type Description <code>State</code> <p>The state with the sampled parameters.</p> <code>ScalarFloat</code> <p>The log probability of the sampled parameters.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def sample_subset(\n    self,\n    key: PRNGKeyArray,\n    state: nnx.State,\n    considered_nodes: Int[jnp.ndarray, \" D\"],\n) -&gt; tuple[nnx.State, ScalarFloat]:\n    \"\"\"\n    Sample kernel parameter and assign it to the kernel.\n    Same as 'sample' method but with additional\n    parameter 'considered_nodes', which can be used\n    to only sample parameters for a subset of nodes.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for sampling.\n    state : nnx.State\n        The original kernel state to be filled with new parameters.\n    considered_nodes : Int[jnp.ndarray, \" D\"]\n        An array that contains the indices of the nodes that\n        are supposed to be sampled. (Padded with -1 if\n        necessary.)\n\n    Returns\n    -------\n    nnx.State\n        The state with the sampled parameters.\n    ScalarFloat\n        The log probability of the sampled parameters.\n    \"\"\"\n\n    new_state, log_prob = sample_parameters(\n        key,\n        state,\n        considered_nodes,\n        self.num_parameter_array,\n        self.max_leaves,\n        self.max_atom_parameters,\n        self.support_mapping_array,\n        self.forward_bijectors,\n    )\n    return new_state, log_prob\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.PeriodicAtom","title":"<code>PeriodicAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Periodic atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class PeriodicAtom(AbstractAtom):\n    \"\"\"Periodic atom.\"\"\"\n\n    name = \"Periodic\"\n    num_parameter = 3\n    parameter_support = [\"positive\", \"positive\", \"positive\"]\n    parameter_names = [\"lengthscale\", \"variance\", \"period\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        lengthscale = params[0]\n        variance = params[1]\n        period = params[2]\n        sine_squared = (jnp.sin(jnp.pi * (x - y) / period) / lengthscale) ** 2\n        k = variance * jnp.exp(-0.5 * sine_squared)\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.PoweredExponentialAtom","title":"<code>PoweredExponentialAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Powered Exponential atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class PoweredExponentialAtom(AbstractAtom):\n    \"\"\"Powered Exponential atom.\"\"\"\n\n    name = \"PoweredExponential\"\n    num_parameter = 3\n    parameter_support = [\"positive\", \"positive\", \"sigmoid\"]\n    parameter_names = [\"lengthscale\", \"variance\", \"power\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        lengthscale = params[0]\n        variance = params[1]\n        power = params[2]\n        tau = jnp.abs(x - y) / lengthscale\n        k = variance * jnp.exp(-jnp.power(tau, power))\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.ProductOperator","title":"<code>ProductOperator</code>","text":"<p>               Bases: <code>AbstractOperator</code></p> <p>Product Operator to combine two kernel functions.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class ProductOperator(AbstractOperator):\n    \"\"\"Product Operator to combine two kernel functions.\"\"\"\n\n    name = \"*\"\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        return jnp.asarray(x * y).squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.RBFAtom","title":"<code>RBFAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Radial Basis Function/Squared Exponential atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class RBFAtom(AbstractAtom):\n    \"\"\"Radial Basis Function/Squared Exponential atom.\"\"\"\n\n    name = \"RBF\"\n    num_parameter = 2\n    parameter_support = [\"positive\", \"positive\"]\n    parameter_names = [\"lengthscale\", \"variance\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        lengthscale = params[0]\n        variance = params[1]\n        d_squared = jnp.square(x - y) / jnp.square(lengthscale)\n        k = variance * jnp.exp(-0.5 * d_squared)\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.RationalQuadraticAtom","title":"<code>RationalQuadraticAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Rational Quadratic atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class RationalQuadraticAtom(AbstractAtom):\n    \"\"\"Rational Quadratic atom.\"\"\"\n\n    name = \"RationalQuadratic\"\n    num_parameter = 3\n    parameter_support = [\"positive\", \"positive\", \"positive\"]\n    parameter_names = [\"lengthscale\", \"variance\", \"alpha\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        lengthscale = params[0]\n        variance = params[1]\n        alpha = params[2]\n        tau = jnp.abs(x - y) / lengthscale\n        k = variance * (1 + 0.5 * jnp.square(tau) / alpha) ** (-alpha)\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.SumOperator","title":"<code>SumOperator</code>","text":"<p>               Bases: <code>AbstractOperator</code></p> <p>Sum Operator to combine two kernel functions.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class SumOperator(AbstractOperator):\n    \"\"\"Sum Operator to combine two kernel functions.\"\"\"\n\n    name = \"+\"\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        return jnp.asarray(x + y).squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeKernel","title":"<code>TreeKernel</code>","text":"<p>               Bases: <code>Module</code></p> <p>A kernel class to evaluate a tree-like kernel expression. The tree expression is a jnp array, with the tree structure encoded as a level-order array. Similar to NestedCombinationKernel in GPJax, but with explicit tree structure.</p> <p>Assume for example that the tree expression is [2, 1, 2, -1, -1, 0, 1] and the kernel library is [RBFAtom, LinearAtom, SumAtom, ProductAtom]. In this case -1 -&gt; empty node, 0 -&gt; RBFKernel, 1 -&gt; LinearKernel, 2 -&gt; Sum, 3 -&gt; Product. The is_operator array should be [False, False, True, True]. The tree expression would then be evaluated to Sum(Linear(x, y), Sum(RBF(x,y), Linear(x,y)).</p> <p>The same kernel could be constructed using NestedCombinationKernel, but the tree structure would not be explicit.</p> <p>Attributes (NOTE: A lot of values are initialized as nnx.Variable, use .value to access the</p> actual value.) <p>tree_expression : Int[jnp.ndarray, \" D\"]     The tree expression, a level-order array describing the tree structure.     Negative values indicate empty nodes. max_nodes : ScalarInt     The maximum number of nodes in the tree expression. max_depth : int     The maximum depth of the tree expression. max_stack : ScalarInt     The maximum size of the stack used to evaluate the tree expression,     which is equal to the maximum depth of the tree expression + 1. max_leaves : ScalarInt     The maximum number of leaves in the tree expression. max_atom_parameters : ScalarInt     The maximum number of parameters of any atom in the atom library. Comes     from the kernel library. max_total_parameters : ScalarInt     The maximum number of parameters in the tree expression (max_leaves     * max_atom_parameters). num_datapoints : ScalarInt     The number of datapoints in the input training data. (This is needed to     correctly construct the gram matrix, in a jit-compatible way.) root_idx : ScalarInt     The index of the root node in the tree expression. atoms : tuple[AbstractAtom]     The atoms in the kernel library. operators : tuple[AbstractOperator]     The operators in the kernel library. num_atoms : ScalarInt     The number of atoms in the kernel library. num_operators : ScalarInt     The number of operators in the kernel library. is_operator : Bool[jnp.ndarray, \" D\"]     A boolean array indicating whether the node in the tree expression is an     operator or not. Comes from the kernel library. post_order_expression : Int[jnp.ndarray, \" D\"]     The tree_expression in post-order traversal notation. Used for efficient     evaluation of the tree expression. Negative values indicate empty nodes. post_level_map : Int[jnp.ndarray, \" D\"]     A map from the post-order index to the level-order index. The value at     a given index in the post-order expression corresponds to the index of     the same node in the level-order expression. Negative values indicate     empty nodes. num_nodes : ScalarInt     The actual number of nodes in the tree expression. node_sizes : Int[jnp.ndarray, \" D\"]     An array containing the sizes of the nodes in the tree. The size of a node     is the number of nodes in the subtree rooted at that node. The size of a     given node is the value at its index in the level-order expression. node_heights : Int[jnp.ndarray, \" D\"]     An array containing the heights of the nodes in the tree. The height of a node     is the length of the longest path from the node to a leaf node (i.e. the heigh     of a leaf node is 0). The height of a given node is the value at its index in     the level-order expression. parameters : KernelParameter(Float[jnp.ndarray, \"M N\"])     KernelParameter instance, that holds a 2D array of kernel parameters with shape     (max_leaves, max_atom_parameters). The parameters are used to evaluate the tree     kernel. leaf_level_map : Int[jnp.ndarray, \" N\"]     A map from the leaf index to the level-order index. The leaf index i corresponds     to the i-th entry in the parameters 0th axis. The value of the leaf_level_map     at index i is the index of the node with these parameters in the level-order     expression.     NOTE: The leaf index is decided based on the depth of the tree, and might not be     in a simple order. See 'gallifrey.utils.tree_helper.get_parameter_leaf_idx' for     more information.</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>class TreeKernel(nnx.Module):\n    \"\"\"\n    A kernel class to evaluate a tree-like kernel expression. The tree expression\n    is a jnp array, with the tree structure encoded as a level-order array.\n    Similar to NestedCombinationKernel in GPJax, but with explicit tree\n    structure.\n\n    Assume for example that the tree expression is [2, 1, 2, -1, -1, 0, 1]\n    and the kernel library is [RBFAtom, LinearAtom, SumAtom, ProductAtom]. In this case\n    -1 -&gt; empty node,\n    0 -&gt; RBFKernel,\n    1 -&gt; LinearKernel,\n    2 -&gt; Sum,\n    3 -&gt; Product.\n    The is_operator array should be [False, False, True, True].\n    The tree expression would then be evaluated to\n    Sum(Linear(x, y), Sum(RBF(x,y), Linear(x,y)).\n\n    The same kernel could be constructed using NestedCombinationKernel, but the\n    tree structure would not be explicit.\n\n    Attributes\n    (NOTE: A lot of values are initialized as nnx.Variable, use .value to access the\n    actual value.)\n    ----------\n    tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression, a level-order array describing the tree structure.\n        Negative values indicate empty nodes.\n    max_nodes : ScalarInt\n        The maximum number of nodes in the tree expression.\n    max_depth : int\n        The maximum depth of the tree expression.\n    max_stack : ScalarInt\n        The maximum size of the stack used to evaluate the tree expression,\n        which is equal to the maximum depth of the tree expression + 1.\n    max_leaves : ScalarInt\n        The maximum number of leaves in the tree expression.\n    max_atom_parameters : ScalarInt\n        The maximum number of parameters of any atom in the atom library. Comes\n        from the kernel library.\n    max_total_parameters : ScalarInt\n        The maximum number of parameters in the tree expression (max_leaves\n        * max_atom_parameters).\n    num_datapoints : ScalarInt\n        The number of datapoints in the input training data. (This is needed to\n        correctly construct the gram matrix, in a jit-compatible way.)\n    root_idx : ScalarInt\n        The index of the root node in the tree expression.\n    atoms : tuple[AbstractAtom]\n        The atoms in the kernel library.\n    operators : tuple[AbstractOperator]\n        The operators in the kernel library.\n    num_atoms : ScalarInt\n        The number of atoms in the kernel library.\n    num_operators : ScalarInt\n        The number of operators in the kernel library.\n    is_operator : Bool[jnp.ndarray, \" D\"]\n        A boolean array indicating whether the node in the tree expression is an\n        operator or not. Comes from the kernel library.\n    post_order_expression : Int[jnp.ndarray, \" D\"]\n        The tree_expression in post-order traversal notation. Used for efficient\n        evaluation of the tree expression. Negative values indicate empty nodes.\n    post_level_map : Int[jnp.ndarray, \" D\"]\n        A map from the post-order index to the level-order index. The value at\n        a given index in the post-order expression corresponds to the index of\n        the same node in the level-order expression. Negative values indicate\n        empty nodes.\n    num_nodes : ScalarInt\n        The actual number of nodes in the tree expression.\n    node_sizes : Int[jnp.ndarray, \" D\"]\n        An array containing the sizes of the nodes in the tree. The size of a node\n        is the number of nodes in the subtree rooted at that node. The size of a\n        given node is the value at its index in the level-order expression.\n    node_heights : Int[jnp.ndarray, \" D\"]\n        An array containing the heights of the nodes in the tree. The height of a node\n        is the length of the longest path from the node to a leaf node (i.e. the heigh\n        of a leaf node is 0). The height of a given node is the value at its index in\n        the level-order expression.\n    parameters : KernelParameter(Float[jnp.ndarray, \"M N\"])\n        KernelParameter instance, that holds a 2D array of kernel parameters with shape\n        (max_leaves, max_atom_parameters). The parameters are used to evaluate the tree\n        kernel.\n    leaf_level_map : Int[jnp.ndarray, \" N\"]\n        A map from the leaf index to the level-order index. The leaf index i corresponds\n        to the i-th entry in the parameters 0th axis. The value of the leaf_level_map\n        at index i is the index of the node with these parameters in the level-order\n        expression.\n        NOTE: The leaf index is decided based on the depth of the tree, and might not be\n        in a simple order. See 'gallifrey.utils.tree_helper.get_parameter_leaf_idx' for\n        more information.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        tree_expression: Int[jnp.ndarray, \" D\"],\n        kernel_library: KernelLibrary,\n        max_depth: int,\n        num_datapoints: int,\n        root_idx: Int[jnp.ndarray, \"\"] = jnp.array(0),\n    ):\n        \"\"\"\n        Initialize a tree kernel.\n\n        Parameters\n        ----------\n        tree_expression : Int[jnp.ndarray, \" D\"]\n            The tree expression, a level-order expression of the tree.\n        kernel_library : KernelLibrary\n            An instance of the KernelLibrary class, containing the atoms\n            and operators to evaluate the tree expression.\n        max_depth : int\n            The maximum depth of the tree expression. The length of the\n            tree expression should be 2^(max_depth+1) - 1.\n        num_datapoints : int, optional\n            The number of datapoints in the input data. (This is needed to\n            correctly construct the gram matrix.)\n        root_idx : Int[jnp.ndarray, \"\"], optional\n            The index of the root node in the tree expression. By default 0.\n\n        \"\"\"\n        self.init_tree(\n            tree_expression,\n            kernel_library,\n            max_depth,\n            num_datapoints,\n            root_idx,\n        )\n        self.init_parameters()\n\n    def init_tree(\n        self,\n        tree_expression: Int[jnp.ndarray, \" D\"],\n        kernel_library: KernelLibrary,\n        max_depth: int,\n        num_datapoints: int,\n        root_idx: Int[jnp.ndarray, \"\"] = jnp.array(0),\n    ) -&gt; None:\n        \"\"\"\n        Initialize the base attributes of the tree kernel, which means\n        setting the\n        - tree_expression,\n        - max_nodes,\n        - root_idx,\n        - max_depth,\n        - is_operator (from the kernel library),\n        - atom_library (from the kernel library),\n        - max_atom_parameters (from the kernel library),\n        - post_order_expression,\n        - post_level_map,\n        - num_nodes,\n        - node_sizes, and\n        - node_heights.\n\n        The pre-order traversal expression and map attributes are initialized\n        but set to None. They get filled when calling the 'display' method.\n\n        Parameters\n        ----------\n        tree_expression : Int[jnp.ndarray, \" D\"]\n            The tree expression, a level-order expression of the tree.\n        kernel_library : tp.Optional[KernelLibrary], optional\n            An instance of the KernelLibrary class, containing the atoms and\n            operators to evaluate the tree expression.\n        max_depth : int\n            The maximum depth of the tree expression. The length of the\n            tree expression should be 2^(max_depth+1) - 1.\n        num_datapoints : int, optional\n            The number of datapoints in the input data. (This is needed to\n            correctly construct the gram matrix.)\n        root_idx : Int[jnp.ndarray, \"\"], optional\n            The index of the root node in the tree expression. By default 0.\n\n        \"\"\"\n        # set base attributes\n        self.tree_expression = nnx.Variable(tree_expression)\n\n        self.max_depth = nnx.Variable(max_depth)\n        self.max_stack = nnx.Variable(calculate_max_stack_size(max_depth))\n        self.max_nodes = nnx.Variable(calculate_max_nodes(max_depth))\n        self.max_leaves = nnx.Variable(calculate_max_leaves(max_depth))\n\n        self.num_datapoints = nnx.Variable(num_datapoints)\n\n        self.root_idx = nnx.Variable(root_idx)\n\n        # check if the root index is valid\n        # if self.root_idx.value &gt; self.max_nodes - 1:\n        #     raise ValueError(\n        #         \"Root index must be less than the maximum number of nodes.\"\n        #     )\n\n        # get the kernel library attributes\n        self.atoms = tuple(kernel_library.atoms)\n        self.operators = tuple(kernel_library.operators)\n        self.num_atoms = nnx.Variable(kernel_library.num_atoms)\n        self.num_operators = nnx.Variable(kernel_library.num_operators)\n        self.is_operator = nnx.Variable(kernel_library.is_operator)\n        self.max_atom_parameters = nnx.Variable(kernel_library.max_atom_parameters)\n        self.max_total_parameters = nnx.Variable(\n            self.max_leaves.value * self.max_atom_parameters.value  # type: ignore\n        )\n\n        # run the post-order traversal and get node metrics\n        (\n            post_order_expression,\n            num_nodes,\n            post_level_map,\n            node_sizes,\n            node_heights,\n            leaf_level_map,\n        ) = self.get_post_order_and_node_metrics()\n\n        self.post_order_expression = nnx.Variable(post_order_expression)\n        self.post_level_map = nnx.Variable(post_level_map)\n        self.num_nodes = nnx.Variable(num_nodes)\n        self.node_sizes = nnx.Variable(node_sizes)\n        self.node_heights = nnx.Variable(node_heights)\n        self.leaf_level_map = nnx.Variable(leaf_level_map)\n\n    def init_parameters(self) -&gt; None:\n        \"\"\"\n        Initialize the parameters of the tree kernel. The parameters are\n        a KernelParameter instance, which holds a 2D array of kernel parameters\n        with shape (max_leaves, max_atom_parameters).\n        \"\"\"\n        # initialise parameters\n        self.parameters = KernelParameter(\n            jnp.ones(\n                (\n                    self.max_leaves.value,\n                    self.max_atom_parameters.value,\n                )\n            )\n        )\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \" D\"],\n        y: Float[jnp.ndarray, \" D\"],\n    ) -&gt; ScalarFloat:\n        \"\"\"\n        Evaluate the tree kernel by traversing the tree expression.\n\n        Parameters\n        ----------\n        x : Float[jnp.ndarray, \" D\"]\n            Input x data.\n        y : Float[jnp.ndarray]\n            Input y data.\n\n        Returns\n        -------\n        ScalarFloat\n            The kernel value.\n        \"\"\"\n        return _evaluate_tree(\n            jnp.atleast_1d(x),\n            jnp.atleast_1d(y),\n            self.post_order_expression.value,\n            self.post_level_map.value,\n            self.is_operator.value,\n            self.parameters.value,\n            self.atoms,\n            self.operators,\n            (1, 1),\n            int(self.num_atoms.value),\n            int(self.max_depth.value),\n        ).squeeze()\n\n    def _tree_viz(self, include_parameters: bool = True) -&gt; str:\n        \"\"\"\n        Helper method for __str__ with optional parameter to include the\n        kernel parameters in the visualization.\n\n        Parameters\n        ----------\n        include_parameters : bool, optional\n            Whether to include the kernel parameters in the visualization.\n\n        Returns\n        -------\n        str\n            A string containing the visual representation of the tree kernel.\n        \"\"\"\n\n        pre_order_expression, num_nodes, pre_level_map = self.get_pre_order()\n\n        description = tree_visualization(\n            pre_order_expression[:num_nodes],\n            pre_level_map,\n            self.atoms + self.operators,\n            self.parameters.value,\n            self.is_operator.value,\n            self.root_idx.value,\n            self.max_depth.value,\n            include_parameters=include_parameters,\n            print_str=False,\n            return_str=True,\n        )\n        assert isinstance(description, str)\n        return description\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Construct a visual representation of the tree kernel.\n        See tree_visualization for more information.\n\n        Returns\n        -------\n        str\n            A string containing the visual representation of the tree kernel (if\n            return_str is True).\n        \"\"\"\n        return self._tree_viz(include_parameters=True)\n\n    def cross_covariance(\n        self,\n        x: Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \" M 1\"],\n        y: Float[jnp.ndarray, \" N\"] | Float[jnp.ndarray, \" N 1\"],\n    ) -&gt; Float[jnp.ndarray, \"M N\"]:\n        \"\"\"\n        Calculate the cross-covariance matrix between x and y\n        for the tree kernel.\n\n        Parameters\n        ----------\n        x : Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \" M 1\"]\n            Input x data.\n        y : Float[jnp.ndarray, \" N\"] | Float[jnp.ndarray, \" N 1\"]\n            Input y data.\n\n\n        Returns\n        -------\n        Float[jnp.ndarray, \"M N\"]\n            The cross-covariance matrix.\n        \"\"\"\n        return _evaluate_tree(\n            x,\n            y,\n            self.post_order_expression.value,\n            self.post_level_map.value,\n            self.is_operator.value,\n            self.parameters.value,\n            tuple([atom.cross_covariance for atom in self.atoms]),\n            self.operators,\n            (int(len(x)), int(len(y))),\n            int(self.num_atoms.value),\n            int(self.max_depth.value),\n        )\n\n    def gram(\n        self,\n        x: Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \" M 1\"],\n    ) -&gt; Float[jnp.ndarray, \"M M\"]:\n        \"\"\"\n        Calculate the gram matrix for a given input x using\n        the tree kernel.\n\n        Parameters\n        ----------\n        x : Float[jnp.ndarray, \" D\"] | Float[jnp.ndarray, \" D 1\"]\n            Input x data.\n\n        Returns\n        -------\n        Float[jnp.ndarray, \"M M\"]\n            The gram matrix.\n        \"\"\"\n        return self.cross_covariance(x, x)\n\n    def _gram_train(\n        self,\n        x: Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \" M 1\"],\n    ) -&gt; Float[jnp.ndarray, \"M M\"]:\n        \"\"\"\n        Calculate the gram matrix for a given input x using\n        the tree kernel. This version has the number of datapoints\n        fixed to the initial number of datapoints in the training data,\n        and is used for evaluation in a jit-compatible way.\n\n        Returns\n        -------\n        Float[jnp.ndarray, \"M M\"]\n            The gram matrix.\n        \"\"\"\n\n        return _evaluate_tree(\n            x,\n            x,\n            self.post_order_expression.value,\n            self.post_level_map.value,\n            self.is_operator.value,\n            self.parameters.value,\n            tuple([atom.cross_covariance for atom in self.atoms]),\n            self.operators,\n            (int(self.num_datapoints.value), int(self.num_datapoints.value)),\n            int(self.num_atoms.value),\n            int(self.max_depth.value),\n        )\n\n    def get_pre_order(\n        self,\n    ) -&gt; tuple[Int[jnp.ndarray, \" D\"], ScalarInt, Int[jnp.ndarray, \" D\"]]:\n        \"\"\"\n        Get the pre-order traversal expression of the tree, as well as\n        the index of the last node in the tree and a map from the pre-order\n        index to the level-order index.\n\n\n        Returns\n        -------\n        Int[jnp.ndarray, \" D\"]\n            The pre-order traversal expression of the tree.\n        ScalarInt\n            The total number of nodes in the tree.\n        Int[jnp.ndarray, \" D\"]\n            A map from the pre-order index to the level-order index. The value at a\n            given index in the pre-order expression corresponds to the index of the\n            same node in the level-order expression\n        \"\"\"\n\n        initial_expression = jnp.full(self.max_nodes.value, -1)\n        initial_expression_pointer = 0\n        pre_level_map = jnp.copy(initial_expression)\n        initial_stack = (\n            jnp.copy(initial_expression).at[0].set(self.root_idx)\n        )  # fill with root\n        initial_stack_pointer = 0\n\n        initial_state = (\n            initial_expression,\n            initial_expression_pointer,\n            pre_level_map,\n            initial_stack,\n            initial_stack_pointer,\n        )\n\n        return level_order_to_pre_order(\n            self.tree_expression.value,\n            initial_state,\n            self.max_nodes.value,\n        )\n\n    def get_post_order_and_node_metrics(\n        self,\n    ) -&gt; tuple[\n        Int[jnp.ndarray, \" D\"],\n        ScalarInt,\n        Int[jnp.ndarray, \" D\"],\n        Int[jnp.ndarray, \" D\"],\n        Int[jnp.ndarray, \" D\"],\n        Int[jnp.ndarray, \" D\"],\n    ]:\n        \"\"\"\n        Get the post-order traversal expression of the tree, the output is\n        - the post-order traversal expression of the tree,\n        - the total number of nodes in the tree,\n        - a map from the post-order index to the level-order index,\n        - an array containing the sizes of the nodes in the tree (located at the\n          level-order index),\n        - an array containing the heights of the nodes in the tree (located at\n          the level-order index).\n        - a map from the leaf index to the level-order index,\n        - the total number of leaves in the tree.\n\n        (Unlike the get_pre_order method, this method also returns the\n        node sizes, heights, leaf level map, and number of leaves.)\n\n        Returns\n        -------\n        Int[jnp.ndarray, \" D\"]\n            The post-order traversal expression of the tree.\n        ScalarInt\n            The total number of nodes in the tree.\n        Int[jnp.ndarray, \" D\"]\n            A map from the post-order index to the level-order index. The value at a\n            given index in the post-order expression corresponds to the index of the\n            same node in the level-order expression\n        Int[jnp.ndarray, \" D\"]\n            An array containing the sizes of the nodes in the tree. The size of a node\n            is the number of nodes in the subtree rooted at that node. The size of a\n            given node is the value at its index in the level-order expression.\n        Int[jnp.ndarray, \" D\"]\n            An array containing the heights of the nodes in the tree. The height of a\n            node is the length of the longest path from the node to a leaf node\n            (i.e. the height of a leaf node is 0). The height of a given\n            node is the value at its index in the level-order expression.\n        Int[jnp.ndarray, \" D\"]\n            A map from the leaf index to the level-order index. The value at a given\n            index in the leaf level map corresponds to the index of the node with\n            these parameters in the level-order expression.\n            NOTE: The leaf index is decided based on the depth of the tree, and might\n            not be in a simple order. See\n            'gallifrey.utils.tree_helper.get_parameter_leaf_idx' for more information.\n        \"\"\"\n        initial_expression = jnp.full(self.max_nodes.value, -1)\n        initial_expression_pointer = 0\n        post_level_map = jnp.copy(initial_expression)\n        initial_stack = (\n            jnp.copy(initial_expression).at[0].set(self.root_idx)\n        )  # fill with root\n        initial_stack_pointer = 0\n        last_processed_idx = -1\n        node_sizes = jnp.full(self.max_nodes, 0)\n        node_heights = jnp.copy(node_sizes)\n        leaf_level_map = jnp.full(self.max_leaves, -1)\n\n        initial_state = (\n            initial_expression,\n            initial_expression_pointer,\n            post_level_map,\n            initial_stack,\n            initial_stack_pointer,\n            last_processed_idx,\n            node_sizes,\n            node_heights,\n            leaf_level_map,\n        )\n\n        return level_order_to_post_order_and_metrics(\n            self.tree_expression.value,\n            initial_state,\n            self.is_operator.value,\n            self.max_nodes.value,\n            self.max_depth.value,\n        )\n\n    def print_atoms(self) -&gt; None:\n        \"\"\"\n        Print the atoms in the atom library, together with their parameter names.\n        \"\"\"\n        for atom in self.atoms:\n            print(f\"{atom.name}: {atom.parameter_names}\")\n\n    def display(self) -&gt; None:\n        print(self.__str__())\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeKernel.__call__","title":"<code>__call__(x, y)</code>","text":"<p>Evaluate the tree kernel by traversing the tree expression.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[ndarray, ' D']</code> <p>Input x data.</p> required <code>y</code> <code>Float[ndarray]</code> <p>Input y data.</p> required <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The kernel value.</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def __call__(\n    self,\n    x: Float[jnp.ndarray, \" D\"],\n    y: Float[jnp.ndarray, \" D\"],\n) -&gt; ScalarFloat:\n    \"\"\"\n    Evaluate the tree kernel by traversing the tree expression.\n\n    Parameters\n    ----------\n    x : Float[jnp.ndarray, \" D\"]\n        Input x data.\n    y : Float[jnp.ndarray]\n        Input y data.\n\n    Returns\n    -------\n    ScalarFloat\n        The kernel value.\n    \"\"\"\n    return _evaluate_tree(\n        jnp.atleast_1d(x),\n        jnp.atleast_1d(y),\n        self.post_order_expression.value,\n        self.post_level_map.value,\n        self.is_operator.value,\n        self.parameters.value,\n        self.atoms,\n        self.operators,\n        (1, 1),\n        int(self.num_atoms.value),\n        int(self.max_depth.value),\n    ).squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeKernel.__init__","title":"<code>__init__(tree_expression, kernel_library, max_depth, num_datapoints, root_idx=jnp.array(0))</code>","text":"<p>Initialize a tree kernel.</p> <p>Parameters:</p> Name Type Description Default <code>tree_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression, a level-order expression of the tree.</p> required <code>kernel_library</code> <code>KernelLibrary</code> <p>An instance of the KernelLibrary class, containing the atoms and operators to evaluate the tree expression.</p> required <code>max_depth</code> <code>int</code> <p>The maximum depth of the tree expression. The length of the tree expression should be 2^(max_depth+1) - 1.</p> required <code>num_datapoints</code> <code>int</code> <p>The number of datapoints in the input data. (This is needed to correctly construct the gram matrix.)</p> required <code>root_idx</code> <code>Int[ndarray, '']</code> <p>The index of the root node in the tree expression. By default 0.</p> <code>array(0)</code> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def __init__(\n    self,\n    tree_expression: Int[jnp.ndarray, \" D\"],\n    kernel_library: KernelLibrary,\n    max_depth: int,\n    num_datapoints: int,\n    root_idx: Int[jnp.ndarray, \"\"] = jnp.array(0),\n):\n    \"\"\"\n    Initialize a tree kernel.\n\n    Parameters\n    ----------\n    tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression, a level-order expression of the tree.\n    kernel_library : KernelLibrary\n        An instance of the KernelLibrary class, containing the atoms\n        and operators to evaluate the tree expression.\n    max_depth : int\n        The maximum depth of the tree expression. The length of the\n        tree expression should be 2^(max_depth+1) - 1.\n    num_datapoints : int, optional\n        The number of datapoints in the input data. (This is needed to\n        correctly construct the gram matrix.)\n    root_idx : Int[jnp.ndarray, \"\"], optional\n        The index of the root node in the tree expression. By default 0.\n\n    \"\"\"\n    self.init_tree(\n        tree_expression,\n        kernel_library,\n        max_depth,\n        num_datapoints,\n        root_idx,\n    )\n    self.init_parameters()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeKernel.__str__","title":"<code>__str__()</code>","text":"<p>Construct a visual representation of the tree kernel. See tree_visualization for more information.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string containing the visual representation of the tree kernel (if return_str is True).</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Construct a visual representation of the tree kernel.\n    See tree_visualization for more information.\n\n    Returns\n    -------\n    str\n        A string containing the visual representation of the tree kernel (if\n        return_str is True).\n    \"\"\"\n    return self._tree_viz(include_parameters=True)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeKernel.cross_covariance","title":"<code>cross_covariance(x, y)</code>","text":"<p>Calculate the cross-covariance matrix between x and y for the tree kernel.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[ndarray, ' M'] | Float[ndarray, ' M 1']</code> <p>Input x data.</p> required <code>y</code> <code>Float[ndarray, ' N'] | Float[ndarray, ' N 1']</code> <p>Input y data.</p> required <p>Returns:</p> Type Description <code>Float[ndarray, 'M N']</code> <p>The cross-covariance matrix.</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def cross_covariance(\n    self,\n    x: Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \" M 1\"],\n    y: Float[jnp.ndarray, \" N\"] | Float[jnp.ndarray, \" N 1\"],\n) -&gt; Float[jnp.ndarray, \"M N\"]:\n    \"\"\"\n    Calculate the cross-covariance matrix between x and y\n    for the tree kernel.\n\n    Parameters\n    ----------\n    x : Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \" M 1\"]\n        Input x data.\n    y : Float[jnp.ndarray, \" N\"] | Float[jnp.ndarray, \" N 1\"]\n        Input y data.\n\n\n    Returns\n    -------\n    Float[jnp.ndarray, \"M N\"]\n        The cross-covariance matrix.\n    \"\"\"\n    return _evaluate_tree(\n        x,\n        y,\n        self.post_order_expression.value,\n        self.post_level_map.value,\n        self.is_operator.value,\n        self.parameters.value,\n        tuple([atom.cross_covariance for atom in self.atoms]),\n        self.operators,\n        (int(len(x)), int(len(y))),\n        int(self.num_atoms.value),\n        int(self.max_depth.value),\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeKernel.get_post_order_and_node_metrics","title":"<code>get_post_order_and_node_metrics()</code>","text":"<p>Get the post-order traversal expression of the tree, the output is - the post-order traversal expression of the tree, - the total number of nodes in the tree, - a map from the post-order index to the level-order index, - an array containing the sizes of the nodes in the tree (located at the   level-order index), - an array containing the heights of the nodes in the tree (located at   the level-order index). - a map from the leaf index to the level-order index, - the total number of leaves in the tree.</p> <p>(Unlike the get_pre_order method, this method also returns the node sizes, heights, leaf level map, and number of leaves.)</p> <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The post-order traversal expression of the tree.</p> <code>ScalarInt</code> <p>The total number of nodes in the tree.</p> <code>Int[ndarray, ' D']</code> <p>A map from the post-order index to the level-order index. The value at a given index in the post-order expression corresponds to the index of the same node in the level-order expression</p> <code>Int[ndarray, ' D']</code> <p>An array containing the sizes of the nodes in the tree. The size of a node is the number of nodes in the subtree rooted at that node. The size of a given node is the value at its index in the level-order expression.</p> <code>Int[ndarray, ' D']</code> <p>An array containing the heights of the nodes in the tree. The height of a node is the length of the longest path from the node to a leaf node (i.e. the height of a leaf node is 0). The height of a given node is the value at its index in the level-order expression.</p> <code>Int[ndarray, ' D']</code> <p>A map from the leaf index to the level-order index. The value at a given index in the leaf level map corresponds to the index of the node with these parameters in the level-order expression. NOTE: The leaf index is decided based on the depth of the tree, and might not be in a simple order. See 'gallifrey.utils.tree_helper.get_parameter_leaf_idx' for more information.</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def get_post_order_and_node_metrics(\n    self,\n) -&gt; tuple[\n    Int[jnp.ndarray, \" D\"],\n    ScalarInt,\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n]:\n    \"\"\"\n    Get the post-order traversal expression of the tree, the output is\n    - the post-order traversal expression of the tree,\n    - the total number of nodes in the tree,\n    - a map from the post-order index to the level-order index,\n    - an array containing the sizes of the nodes in the tree (located at the\n      level-order index),\n    - an array containing the heights of the nodes in the tree (located at\n      the level-order index).\n    - a map from the leaf index to the level-order index,\n    - the total number of leaves in the tree.\n\n    (Unlike the get_pre_order method, this method also returns the\n    node sizes, heights, leaf level map, and number of leaves.)\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The post-order traversal expression of the tree.\n    ScalarInt\n        The total number of nodes in the tree.\n    Int[jnp.ndarray, \" D\"]\n        A map from the post-order index to the level-order index. The value at a\n        given index in the post-order expression corresponds to the index of the\n        same node in the level-order expression\n    Int[jnp.ndarray, \" D\"]\n        An array containing the sizes of the nodes in the tree. The size of a node\n        is the number of nodes in the subtree rooted at that node. The size of a\n        given node is the value at its index in the level-order expression.\n    Int[jnp.ndarray, \" D\"]\n        An array containing the heights of the nodes in the tree. The height of a\n        node is the length of the longest path from the node to a leaf node\n        (i.e. the height of a leaf node is 0). The height of a given\n        node is the value at its index in the level-order expression.\n    Int[jnp.ndarray, \" D\"]\n        A map from the leaf index to the level-order index. The value at a given\n        index in the leaf level map corresponds to the index of the node with\n        these parameters in the level-order expression.\n        NOTE: The leaf index is decided based on the depth of the tree, and might\n        not be in a simple order. See\n        'gallifrey.utils.tree_helper.get_parameter_leaf_idx' for more information.\n    \"\"\"\n    initial_expression = jnp.full(self.max_nodes.value, -1)\n    initial_expression_pointer = 0\n    post_level_map = jnp.copy(initial_expression)\n    initial_stack = (\n        jnp.copy(initial_expression).at[0].set(self.root_idx)\n    )  # fill with root\n    initial_stack_pointer = 0\n    last_processed_idx = -1\n    node_sizes = jnp.full(self.max_nodes, 0)\n    node_heights = jnp.copy(node_sizes)\n    leaf_level_map = jnp.full(self.max_leaves, -1)\n\n    initial_state = (\n        initial_expression,\n        initial_expression_pointer,\n        post_level_map,\n        initial_stack,\n        initial_stack_pointer,\n        last_processed_idx,\n        node_sizes,\n        node_heights,\n        leaf_level_map,\n    )\n\n    return level_order_to_post_order_and_metrics(\n        self.tree_expression.value,\n        initial_state,\n        self.is_operator.value,\n        self.max_nodes.value,\n        self.max_depth.value,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeKernel.get_pre_order","title":"<code>get_pre_order()</code>","text":"<p>Get the pre-order traversal expression of the tree, as well as the index of the last node in the tree and a map from the pre-order index to the level-order index.</p> <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The pre-order traversal expression of the tree.</p> <code>ScalarInt</code> <p>The total number of nodes in the tree.</p> <code>Int[ndarray, ' D']</code> <p>A map from the pre-order index to the level-order index. The value at a given index in the pre-order expression corresponds to the index of the same node in the level-order expression</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def get_pre_order(\n    self,\n) -&gt; tuple[Int[jnp.ndarray, \" D\"], ScalarInt, Int[jnp.ndarray, \" D\"]]:\n    \"\"\"\n    Get the pre-order traversal expression of the tree, as well as\n    the index of the last node in the tree and a map from the pre-order\n    index to the level-order index.\n\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The pre-order traversal expression of the tree.\n    ScalarInt\n        The total number of nodes in the tree.\n    Int[jnp.ndarray, \" D\"]\n        A map from the pre-order index to the level-order index. The value at a\n        given index in the pre-order expression corresponds to the index of the\n        same node in the level-order expression\n    \"\"\"\n\n    initial_expression = jnp.full(self.max_nodes.value, -1)\n    initial_expression_pointer = 0\n    pre_level_map = jnp.copy(initial_expression)\n    initial_stack = (\n        jnp.copy(initial_expression).at[0].set(self.root_idx)\n    )  # fill with root\n    initial_stack_pointer = 0\n\n    initial_state = (\n        initial_expression,\n        initial_expression_pointer,\n        pre_level_map,\n        initial_stack,\n        initial_stack_pointer,\n    )\n\n    return level_order_to_pre_order(\n        self.tree_expression.value,\n        initial_state,\n        self.max_nodes.value,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeKernel.gram","title":"<code>gram(x)</code>","text":"<p>Calculate the gram matrix for a given input x using the tree kernel.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[ndarray, ' D'] | Float[ndarray, ' D 1']</code> <p>Input x data.</p> required <p>Returns:</p> Type Description <code>Float[ndarray, 'M M']</code> <p>The gram matrix.</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def gram(\n    self,\n    x: Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \" M 1\"],\n) -&gt; Float[jnp.ndarray, \"M M\"]:\n    \"\"\"\n    Calculate the gram matrix for a given input x using\n    the tree kernel.\n\n    Parameters\n    ----------\n    x : Float[jnp.ndarray, \" D\"] | Float[jnp.ndarray, \" D 1\"]\n        Input x data.\n\n    Returns\n    -------\n    Float[jnp.ndarray, \"M M\"]\n        The gram matrix.\n    \"\"\"\n    return self.cross_covariance(x, x)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeKernel.init_parameters","title":"<code>init_parameters()</code>","text":"<p>Initialize the parameters of the tree kernel. The parameters are a KernelParameter instance, which holds a 2D array of kernel parameters with shape (max_leaves, max_atom_parameters).</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def init_parameters(self) -&gt; None:\n    \"\"\"\n    Initialize the parameters of the tree kernel. The parameters are\n    a KernelParameter instance, which holds a 2D array of kernel parameters\n    with shape (max_leaves, max_atom_parameters).\n    \"\"\"\n    # initialise parameters\n    self.parameters = KernelParameter(\n        jnp.ones(\n            (\n                self.max_leaves.value,\n                self.max_atom_parameters.value,\n            )\n        )\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeKernel.init_tree","title":"<code>init_tree(tree_expression, kernel_library, max_depth, num_datapoints, root_idx=jnp.array(0))</code>","text":"<p>Initialize the base attributes of the tree kernel, which means setting the - tree_expression, - max_nodes, - root_idx, - max_depth, - is_operator (from the kernel library), - atom_library (from the kernel library), - max_atom_parameters (from the kernel library), - post_order_expression, - post_level_map, - num_nodes, - node_sizes, and - node_heights.</p> <p>The pre-order traversal expression and map attributes are initialized but set to None. They get filled when calling the 'display' method.</p> <p>Parameters:</p> Name Type Description Default <code>tree_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression, a level-order expression of the tree.</p> required <code>kernel_library</code> <code>Optional[KernelLibrary]</code> <p>An instance of the KernelLibrary class, containing the atoms and operators to evaluate the tree expression.</p> required <code>max_depth</code> <code>int</code> <p>The maximum depth of the tree expression. The length of the tree expression should be 2^(max_depth+1) - 1.</p> required <code>num_datapoints</code> <code>int</code> <p>The number of datapoints in the input data. (This is needed to correctly construct the gram matrix.)</p> required <code>root_idx</code> <code>Int[ndarray, '']</code> <p>The index of the root node in the tree expression. By default 0.</p> <code>array(0)</code> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def init_tree(\n    self,\n    tree_expression: Int[jnp.ndarray, \" D\"],\n    kernel_library: KernelLibrary,\n    max_depth: int,\n    num_datapoints: int,\n    root_idx: Int[jnp.ndarray, \"\"] = jnp.array(0),\n) -&gt; None:\n    \"\"\"\n    Initialize the base attributes of the tree kernel, which means\n    setting the\n    - tree_expression,\n    - max_nodes,\n    - root_idx,\n    - max_depth,\n    - is_operator (from the kernel library),\n    - atom_library (from the kernel library),\n    - max_atom_parameters (from the kernel library),\n    - post_order_expression,\n    - post_level_map,\n    - num_nodes,\n    - node_sizes, and\n    - node_heights.\n\n    The pre-order traversal expression and map attributes are initialized\n    but set to None. They get filled when calling the 'display' method.\n\n    Parameters\n    ----------\n    tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression, a level-order expression of the tree.\n    kernel_library : tp.Optional[KernelLibrary], optional\n        An instance of the KernelLibrary class, containing the atoms and\n        operators to evaluate the tree expression.\n    max_depth : int\n        The maximum depth of the tree expression. The length of the\n        tree expression should be 2^(max_depth+1) - 1.\n    num_datapoints : int, optional\n        The number of datapoints in the input data. (This is needed to\n        correctly construct the gram matrix.)\n    root_idx : Int[jnp.ndarray, \"\"], optional\n        The index of the root node in the tree expression. By default 0.\n\n    \"\"\"\n    # set base attributes\n    self.tree_expression = nnx.Variable(tree_expression)\n\n    self.max_depth = nnx.Variable(max_depth)\n    self.max_stack = nnx.Variable(calculate_max_stack_size(max_depth))\n    self.max_nodes = nnx.Variable(calculate_max_nodes(max_depth))\n    self.max_leaves = nnx.Variable(calculate_max_leaves(max_depth))\n\n    self.num_datapoints = nnx.Variable(num_datapoints)\n\n    self.root_idx = nnx.Variable(root_idx)\n\n    # check if the root index is valid\n    # if self.root_idx.value &gt; self.max_nodes - 1:\n    #     raise ValueError(\n    #         \"Root index must be less than the maximum number of nodes.\"\n    #     )\n\n    # get the kernel library attributes\n    self.atoms = tuple(kernel_library.atoms)\n    self.operators = tuple(kernel_library.operators)\n    self.num_atoms = nnx.Variable(kernel_library.num_atoms)\n    self.num_operators = nnx.Variable(kernel_library.num_operators)\n    self.is_operator = nnx.Variable(kernel_library.is_operator)\n    self.max_atom_parameters = nnx.Variable(kernel_library.max_atom_parameters)\n    self.max_total_parameters = nnx.Variable(\n        self.max_leaves.value * self.max_atom_parameters.value  # type: ignore\n    )\n\n    # run the post-order traversal and get node metrics\n    (\n        post_order_expression,\n        num_nodes,\n        post_level_map,\n        node_sizes,\n        node_heights,\n        leaf_level_map,\n    ) = self.get_post_order_and_node_metrics()\n\n    self.post_order_expression = nnx.Variable(post_order_expression)\n    self.post_level_map = nnx.Variable(post_level_map)\n    self.num_nodes = nnx.Variable(num_nodes)\n    self.node_sizes = nnx.Variable(node_sizes)\n    self.node_heights = nnx.Variable(node_heights)\n    self.leaf_level_map = nnx.Variable(leaf_level_map)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeKernel.print_atoms","title":"<code>print_atoms()</code>","text":"<p>Print the atoms in the atom library, together with their parameter names.</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def print_atoms(self) -&gt; None:\n    \"\"\"\n    Print the atoms in the atom library, together with their parameter names.\n    \"\"\"\n    for atom in self.atoms:\n        print(f\"{atom.name}: {atom.parameter_names}\")\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeStructurePrior","title":"<code>TreeStructurePrior</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A prior distribution over kernel structures.</p> <p>The TreeStructurePrior is a distribution over kernel structures. The kernel structure is represented as a tree, where each leaf corresponds to a kernel. The tree is constructed by sampling from a library of kernel classes and operators. The operators are used to construct nested kernel structures.</p> <p>Attributes:</p> Name Type Description <code>library</code> <code>List[Type[AbstractKernel] | Callable]</code> <p>A list of kernel classes and operators, as defined in the KernelLibrary class. See gallifrey.kernels.library.KernelLibrary for more details.</p> <code>is_operator</code> <code>List[bool]</code> <p>A list of booleans indicating whether each element in the library is an operator.</p> <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the nested kernel structure tree.</p> <code>probs</code> <code>ndarray</code> <p>The probabilities of sampling each kernel (or operator) in the library. See kernels.library.KernelLibrary and gallifrey.config.GPConfig for more details.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>class TreeStructurePrior(tfd.Distribution):\n    \"\"\"\n    A prior distribution over kernel structures.\n\n    The TreeStructurePrior is a distribution over kernel structures. The kernel\n    structure is represented as a tree, where each leaf corresponds to a kernel. The\n    tree is constructed by sampling from a library of kernel classes and operators.\n    The operators are used to construct nested kernel structures.\n\n    Attributes\n    ----------\n    library : tp.List[tp.Type[AbstractKernel] | tp.Callable]\n        A list of kernel classes and operators, as defined in the KernelLibrary class.\n        See gallifrey.kernels.library.KernelLibrary for more details.\n    is_operator : tp.List[bool]\n        A list of booleans indicating whether each element in the library is an\n        operator.\n    max_depth : ScalarInt\n        The maximum depth of the nested kernel structure tree.\n    probs : jnp.ndarray\n        The probabilities of sampling each kernel (or operator) in the library.\n        See kernels.library.KernelLibrary and gallifrey.config.GPConfig for more\n        details.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel_library: KernelLibrary,\n        max_depth: int,\n        probs: tp.Optional[Float[jnp.ndarray, \" D\"]] = None,\n        *,\n        validate_args: tp.Optional[bool] = None,\n    ):\n        \"\"\"\n        Initialize the KernelPrior distribution.\n\n        Parameters\n        ----------\n        kernel_library : KernelLibrary\n            An instance of the KernelLibrary class, containing the kernel classes and\n            operators. (See gallifrey.kernels.library.KernelLibrary)\n        max_depth : int\n            The maximum depth of the nested kernel structure tree.\n        probs :  Float[jnp.ndarray, \" D\"], optional\n            The probabilities of sampling each kernel (or operator) in the library.\n            The array must have the same length as the the arrays in the kernel library.\n            By default None, which will use a uniform distribution.\n        validate_args : tp.Optional[bool], optional\n            Whether to validate input, by default None. NOT IMPLEMENTED.\n\n        Raises\n        ------\n        ValueError\n            If the probs are not one-dimensional.\n        ValueError\n            If the probs do not have the same length as the kernel_library\n            along the last dimension.\n        \"\"\"\n        # initialize the kernel functions and the probabilities\n        self.library = kernel_library.library\n        self.is_operator = kernel_library.is_operator\n        self.max_depth = max_depth\n\n        self.probs = probs if probs is not None else jnp.ones(len(self.library))\n        if jnp.ndim(self.probs) != 1:\n            raise ValueError(\"'probs' must be one-dimensional.\")\n        if not len(self.library) == len(self.probs):\n            raise ValueError(\n                \"'probs' must have same length along the last \"\n                \"dimension as 'kernel_library'. \"\n                f\"Got 'probs' shape: {jnp.shape(self.probs)}, \"\n                f\"'kernel_library' length: {len(self.library)}.\"\n            )\n\n    def sample_single(\n        self,\n        key: PRNGKeyArray,\n        max_depth: tp.Optional[ScalarInt] = None,\n        root_idx: ScalarInt = 0,\n    ) -&gt; Int[jnp.ndarray, \" D\"]:\n        \"\"\"\n        Construct an abstract representation of a kernel structure by sampling from\n        the kernel library.\n        The kernels are sampled from the library according to the defined\n        probabilities.\n        The structure is represented as a one-dimensional array, according to\n        the level-order traversal of the tree. This means:\n        - The root node is at position 0.\n        - The left child of a node at position i is at position 2*i + 1.\n        - The right child of a node at position i is at position 2*i + 2.\n        - Empty nodes are labeled -1.\n        See gallifrey.kernels.tree.TreeKernel for more details and an example.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for sampling.\n        max_depth : ScalarInt, optional\n            The maximum depth of the nested kernel structure tree, by default None.\n            If None, the maximum depth is set to the value of self.max_depth.\n        root_idx : ScalarInt, optional\n            The index of the root node in the tree, by default 0. Used\n            for sub-trees.\n\n        Returns\n        -------\n        Int[jnp.ndarray, \" D\"]\n            An array that describes the kernel structure.\n        \"\"\"\n        max_depth = max_depth if max_depth is not None else self.max_depth\n\n        if max_depth &lt; 0:\n            raise ValueError(\"'max_depth' must be 0 or larger.\")\n\n        max_nodes = calculate_max_nodes(max_depth)\n\n        # create sample array to be filled, this will be the output (empty\n        # nodes are labeled -1)\n        sample = jnp.full(max_nodes, -1)\n        # create initial stack: empty except for the root node\n        initial_stack = jnp.copy(sample).at[0].set(root_idx)\n\n        pointer = 0  # initial position of the stack pointer\n\n        initial_state = (key, sample, initial_stack, pointer)\n\n        return _sample_single(\n            initial_state,\n            self.probs,\n            self.is_operator,\n            max_depth,\n        )\n\n    def log_prob_single(\n        self,\n        value: Int[jnp.ndarray, \" D\"],\n        root_idx: ScalarInt = 0,\n        path_to_hole: tp.Optional[Int[jnp.ndarray, \" D\"]] = None,\n        hole_idx: tp.Optional[ScalarInt] = None,\n    ) -&gt; ScalarFloat:\n        \"\"\"\n        Compute the log probability of a given kernel structure,\n        as represented by the level-order tree array.\n        The maximum depth of the tree is inferred from the length of the array.\n\n        The function can also be used to calculate the log probability of a\n        scaffold structure (used by the detach-attach move), by providing\n        the path to the hole and the hole index.\n\n        Parameters\n        ----------\n        value :  Int[jnp.ndarray, \" D\"]\n            An array that describes the kernel structure.\n        root_idx : ScalarInt, optional\n            The index of the root node in the tree, by default 0.\n        path_to_hole :  Int[jnp.ndarray, \" D\"], optional\n            The path to the hole in the tree. A list of indices that\n            describe the path from the root to the hole (level order\n            indices), by default None (sets it to an empty array).\n        hole_idx : ScalarInt, optional\n            The index of the hole in the tree (level order index),\n            by default None(sets it to -1 which should never be\n            reached).\n\n        Returns\n        -------\n        ScalarFloat\n            The log probability of the given kernel structure or scaffold.\n        \"\"\"\n        max_nodes = len(value)\n        max_depth = calculate_max_depth(max_nodes)\n\n        initial_log_p = 0.0\n        inital_stack = jnp.full(max_nodes, -1).at[0].set(root_idx)\n        pointer = 0\n        initial_state = (initial_log_p, inital_stack, pointer)\n\n        # set the path to the hole and the hole index, if not given\n        path_to_hole = path_to_hole if path_to_hole is not None else jnp.array([])\n        hole_idx = hole_idx if hole_idx is not None else -1\n\n        return _log_prob_single(\n            initial_state,\n            value,\n            self.probs,\n            self.is_operator,\n            max_depth,\n            path_to_hole,\n            hole_idx,\n        )\n\n    def sample(\n        self,\n        key: PRNGKeyArray,\n        max_depth: tp.Optional[ScalarInt] = None,\n        root_idx: ScalarInt = 0,\n        sample_shape: tuple = (),\n    ) -&gt; Int[jnp.ndarray, \"...\"]:\n        \"\"\"\n        Sample kernel structures. For details, see `sample_single`.\n\n        Parameters\n        ----------\n        key : PRNGKey\n            Random key for sampling.\n        max_depth : ScalarInt\n            The maximum depth of the nested kernel tree structure. If None,\n            the maximum depth is set to the value of self.max_depth.\n        root_idx : ScalarInt, optional\n            The index of the root node in the tree, by default 0.\n            Used for sub-trees.\n        sample_shape : tuple, optional\n            The sample shape for the distribution, by default ().\n\n        Returns\n        -------\n        Int[jnp.ndarray, \"...\"]\n            An array of samples describing kernel structures,\n            of shape sample_shape + (max_nodes,).\n        \"\"\"\n        max_depth = max_depth if max_depth is not None else self.max_depth\n\n        if not sample_shape:\n            return self.sample_single(key, max_depth, root_idx)\n\n        max_nodes = calculate_max_nodes(max_depth)\n\n        # flatten the sample_shape for vectorized sampling\n        num_samples = jnp.prod(jnp.array(sample_shape))\n        keys = jr.split(key, num=int(num_samples))\n\n        samples = vmap(self.sample_single, in_axes=(0, None, None))(\n            keys,\n            max_depth,\n            root_idx,\n        )\n        return samples.reshape(sample_shape + (max_nodes,))\n\n    def log_prob(\n        self,\n        value: Int[jnp.ndarray, \"...\"],\n        root_idx: ScalarInt = 0,\n        path_to_hole: tp.Optional[Int[jnp.ndarray, \"...\"]] = None,\n        hole_idx: tp.Optional[ScalarInt] = None,\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        \"\"\"\n        Compute the log probability of given kernel structures or scaffolds.\n        (See log_prob_single for details.)\n\n        Parameters\n        ----------\n        value :  Int[jnp.ndarray, \"...\"]\n            Expression of kernel structure(s).\n        root_idx : ScalarInt, optional\n            The index of the root node in the tree, by default 0.\n        path_to_hole :  Int[jnp.ndarray, \"...\"], optional\n            Optional path to the hole in the tree, if calculating the log\n            probability of a scaffold, by default None.\n        hole_idx : ScalarInt, optional\n            The index of the hole in the tree if calculating the log\n            probability of a scaffold, by default None.\n\n        Returns\n        -------\n         Float[jnp.ndarray, \"...\"]\n            The log probability of the given kernel structures.\n        \"\"\"\n        sample_shape = jnp.shape(value)[:-1]\n\n        num_samples = jnp.prod(jnp.array(sample_shape))\n        log_probs = vmap(self.log_prob_single, in_axes=(0, None, None, None))(\n            value.reshape(int(num_samples), -1),\n            root_idx,\n            path_to_hole,\n            hole_idx,\n        )\n        return jnp.asarray(log_probs).reshape(sample_shape)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeStructurePrior.__init__","title":"<code>__init__(kernel_library, max_depth, probs=None, *, validate_args=None)</code>","text":"<p>Initialize the KernelPrior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_library</code> <code>KernelLibrary</code> <p>An instance of the KernelLibrary class, containing the kernel classes and operators. (See gallifrey.kernels.library.KernelLibrary)</p> required <code>max_depth</code> <code>int</code> <p>The maximum depth of the nested kernel structure tree.</p> required <code>probs</code> <code> Float[jnp.ndarray, \" D\"]</code> <p>The probabilities of sampling each kernel (or operator) in the library. The array must have the same length as the the arrays in the kernel library. By default None, which will use a uniform distribution.</p> <code>None</code> <code>validate_args</code> <code>Optional[bool]</code> <p>Whether to validate input, by default None. NOT IMPLEMENTED.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the probs are not one-dimensional.</p> <code>ValueError</code> <p>If the probs do not have the same length as the kernel_library along the last dimension.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def __init__(\n    self,\n    kernel_library: KernelLibrary,\n    max_depth: int,\n    probs: tp.Optional[Float[jnp.ndarray, \" D\"]] = None,\n    *,\n    validate_args: tp.Optional[bool] = None,\n):\n    \"\"\"\n    Initialize the KernelPrior distribution.\n\n    Parameters\n    ----------\n    kernel_library : KernelLibrary\n        An instance of the KernelLibrary class, containing the kernel classes and\n        operators. (See gallifrey.kernels.library.KernelLibrary)\n    max_depth : int\n        The maximum depth of the nested kernel structure tree.\n    probs :  Float[jnp.ndarray, \" D\"], optional\n        The probabilities of sampling each kernel (or operator) in the library.\n        The array must have the same length as the the arrays in the kernel library.\n        By default None, which will use a uniform distribution.\n    validate_args : tp.Optional[bool], optional\n        Whether to validate input, by default None. NOT IMPLEMENTED.\n\n    Raises\n    ------\n    ValueError\n        If the probs are not one-dimensional.\n    ValueError\n        If the probs do not have the same length as the kernel_library\n        along the last dimension.\n    \"\"\"\n    # initialize the kernel functions and the probabilities\n    self.library = kernel_library.library\n    self.is_operator = kernel_library.is_operator\n    self.max_depth = max_depth\n\n    self.probs = probs if probs is not None else jnp.ones(len(self.library))\n    if jnp.ndim(self.probs) != 1:\n        raise ValueError(\"'probs' must be one-dimensional.\")\n    if not len(self.library) == len(self.probs):\n        raise ValueError(\n            \"'probs' must have same length along the last \"\n            \"dimension as 'kernel_library'. \"\n            f\"Got 'probs' shape: {jnp.shape(self.probs)}, \"\n            f\"'kernel_library' length: {len(self.library)}.\"\n        )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeStructurePrior.log_prob","title":"<code>log_prob(value, root_idx=0, path_to_hole=None, hole_idx=None)</code>","text":"<p>Compute the log probability of given kernel structures or scaffolds. (See log_prob_single for details.)</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code> Int[jnp.ndarray, \"...\"]</code> <p>Expression of kernel structure(s).</p> required <code>root_idx</code> <code>ScalarInt</code> <p>The index of the root node in the tree, by default 0.</p> <code>0</code> <code>path_to_hole</code> <code> Int[jnp.ndarray, \"...\"]</code> <p>Optional path to the hole in the tree, if calculating the log probability of a scaffold, by default None.</p> <code>None</code> <code>hole_idx</code> <code>ScalarInt</code> <p>The index of the hole in the tree if calculating the log probability of a scaffold, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code> Float[jnp.ndarray, \"...\"]</code> <p>The log probability of the given kernel structures.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def log_prob(\n    self,\n    value: Int[jnp.ndarray, \"...\"],\n    root_idx: ScalarInt = 0,\n    path_to_hole: tp.Optional[Int[jnp.ndarray, \"...\"]] = None,\n    hole_idx: tp.Optional[ScalarInt] = None,\n) -&gt; Float[jnp.ndarray, \"...\"]:\n    \"\"\"\n    Compute the log probability of given kernel structures or scaffolds.\n    (See log_prob_single for details.)\n\n    Parameters\n    ----------\n    value :  Int[jnp.ndarray, \"...\"]\n        Expression of kernel structure(s).\n    root_idx : ScalarInt, optional\n        The index of the root node in the tree, by default 0.\n    path_to_hole :  Int[jnp.ndarray, \"...\"], optional\n        Optional path to the hole in the tree, if calculating the log\n        probability of a scaffold, by default None.\n    hole_idx : ScalarInt, optional\n        The index of the hole in the tree if calculating the log\n        probability of a scaffold, by default None.\n\n    Returns\n    -------\n     Float[jnp.ndarray, \"...\"]\n        The log probability of the given kernel structures.\n    \"\"\"\n    sample_shape = jnp.shape(value)[:-1]\n\n    num_samples = jnp.prod(jnp.array(sample_shape))\n    log_probs = vmap(self.log_prob_single, in_axes=(0, None, None, None))(\n        value.reshape(int(num_samples), -1),\n        root_idx,\n        path_to_hole,\n        hole_idx,\n    )\n    return jnp.asarray(log_probs).reshape(sample_shape)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeStructurePrior.log_prob_single","title":"<code>log_prob_single(value, root_idx=0, path_to_hole=None, hole_idx=None)</code>","text":"<p>Compute the log probability of a given kernel structure, as represented by the level-order tree array. The maximum depth of the tree is inferred from the length of the array.</p> <p>The function can also be used to calculate the log probability of a scaffold structure (used by the detach-attach move), by providing the path to the hole and the hole index.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code> Int[jnp.ndarray, \" D\"]</code> <p>An array that describes the kernel structure.</p> required <code>root_idx</code> <code>ScalarInt</code> <p>The index of the root node in the tree, by default 0.</p> <code>0</code> <code>path_to_hole</code> <code> Int[jnp.ndarray, \" D\"]</code> <p>The path to the hole in the tree. A list of indices that describe the path from the root to the hole (level order indices), by default None (sets it to an empty array).</p> <code>None</code> <code>hole_idx</code> <code>ScalarInt</code> <p>The index of the hole in the tree (level order index), by default None(sets it to -1 which should never be reached).</p> <code>None</code> <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The log probability of the given kernel structure or scaffold.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def log_prob_single(\n    self,\n    value: Int[jnp.ndarray, \" D\"],\n    root_idx: ScalarInt = 0,\n    path_to_hole: tp.Optional[Int[jnp.ndarray, \" D\"]] = None,\n    hole_idx: tp.Optional[ScalarInt] = None,\n) -&gt; ScalarFloat:\n    \"\"\"\n    Compute the log probability of a given kernel structure,\n    as represented by the level-order tree array.\n    The maximum depth of the tree is inferred from the length of the array.\n\n    The function can also be used to calculate the log probability of a\n    scaffold structure (used by the detach-attach move), by providing\n    the path to the hole and the hole index.\n\n    Parameters\n    ----------\n    value :  Int[jnp.ndarray, \" D\"]\n        An array that describes the kernel structure.\n    root_idx : ScalarInt, optional\n        The index of the root node in the tree, by default 0.\n    path_to_hole :  Int[jnp.ndarray, \" D\"], optional\n        The path to the hole in the tree. A list of indices that\n        describe the path from the root to the hole (level order\n        indices), by default None (sets it to an empty array).\n    hole_idx : ScalarInt, optional\n        The index of the hole in the tree (level order index),\n        by default None(sets it to -1 which should never be\n        reached).\n\n    Returns\n    -------\n    ScalarFloat\n        The log probability of the given kernel structure or scaffold.\n    \"\"\"\n    max_nodes = len(value)\n    max_depth = calculate_max_depth(max_nodes)\n\n    initial_log_p = 0.0\n    inital_stack = jnp.full(max_nodes, -1).at[0].set(root_idx)\n    pointer = 0\n    initial_state = (initial_log_p, inital_stack, pointer)\n\n    # set the path to the hole and the hole index, if not given\n    path_to_hole = path_to_hole if path_to_hole is not None else jnp.array([])\n    hole_idx = hole_idx if hole_idx is not None else -1\n\n    return _log_prob_single(\n        initial_state,\n        value,\n        self.probs,\n        self.is_operator,\n        max_depth,\n        path_to_hole,\n        hole_idx,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeStructurePrior.sample","title":"<code>sample(key, max_depth=None, root_idx=0, sample_shape=())</code>","text":"<p>Sample kernel structures. For details, see <code>sample_single</code>.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>Random key for sampling.</p> required <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the nested kernel tree structure. If None, the maximum depth is set to the value of self.max_depth.</p> <code>None</code> <code>root_idx</code> <code>ScalarInt</code> <p>The index of the root node in the tree, by default 0. Used for sub-trees.</p> <code>0</code> <code>sample_shape</code> <code>tuple</code> <p>The sample shape for the distribution, by default ().</p> <code>()</code> <p>Returns:</p> Type Description <code>Int[ndarray, ...]</code> <p>An array of samples describing kernel structures, of shape sample_shape + (max_nodes,).</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def sample(\n    self,\n    key: PRNGKeyArray,\n    max_depth: tp.Optional[ScalarInt] = None,\n    root_idx: ScalarInt = 0,\n    sample_shape: tuple = (),\n) -&gt; Int[jnp.ndarray, \"...\"]:\n    \"\"\"\n    Sample kernel structures. For details, see `sample_single`.\n\n    Parameters\n    ----------\n    key : PRNGKey\n        Random key for sampling.\n    max_depth : ScalarInt\n        The maximum depth of the nested kernel tree structure. If None,\n        the maximum depth is set to the value of self.max_depth.\n    root_idx : ScalarInt, optional\n        The index of the root node in the tree, by default 0.\n        Used for sub-trees.\n    sample_shape : tuple, optional\n        The sample shape for the distribution, by default ().\n\n    Returns\n    -------\n    Int[jnp.ndarray, \"...\"]\n        An array of samples describing kernel structures,\n        of shape sample_shape + (max_nodes,).\n    \"\"\"\n    max_depth = max_depth if max_depth is not None else self.max_depth\n\n    if not sample_shape:\n        return self.sample_single(key, max_depth, root_idx)\n\n    max_nodes = calculate_max_nodes(max_depth)\n\n    # flatten the sample_shape for vectorized sampling\n    num_samples = jnp.prod(jnp.array(sample_shape))\n    keys = jr.split(key, num=int(num_samples))\n\n    samples = vmap(self.sample_single, in_axes=(0, None, None))(\n        keys,\n        max_depth,\n        root_idx,\n    )\n    return samples.reshape(sample_shape + (max_nodes,))\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.TreeStructurePrior.sample_single","title":"<code>sample_single(key, max_depth=None, root_idx=0)</code>","text":"<p>Construct an abstract representation of a kernel structure by sampling from the kernel library. The kernels are sampled from the library according to the defined probabilities. The structure is represented as a one-dimensional array, according to the level-order traversal of the tree. This means: - The root node is at position 0. - The left child of a node at position i is at position 2i + 1. - The right child of a node at position i is at position 2i + 2. - Empty nodes are labeled -1. See gallifrey.kernels.tree.TreeKernel for more details and an example.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for sampling.</p> required <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the nested kernel structure tree, by default None. If None, the maximum depth is set to the value of self.max_depth.</p> <code>None</code> <code>root_idx</code> <code>ScalarInt</code> <p>The index of the root node in the tree, by default 0. Used for sub-trees.</p> <code>0</code> <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>An array that describes the kernel structure.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def sample_single(\n    self,\n    key: PRNGKeyArray,\n    max_depth: tp.Optional[ScalarInt] = None,\n    root_idx: ScalarInt = 0,\n) -&gt; Int[jnp.ndarray, \" D\"]:\n    \"\"\"\n    Construct an abstract representation of a kernel structure by sampling from\n    the kernel library.\n    The kernels are sampled from the library according to the defined\n    probabilities.\n    The structure is represented as a one-dimensional array, according to\n    the level-order traversal of the tree. This means:\n    - The root node is at position 0.\n    - The left child of a node at position i is at position 2*i + 1.\n    - The right child of a node at position i is at position 2*i + 2.\n    - Empty nodes are labeled -1.\n    See gallifrey.kernels.tree.TreeKernel for more details and an example.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for sampling.\n    max_depth : ScalarInt, optional\n        The maximum depth of the nested kernel structure tree, by default None.\n        If None, the maximum depth is set to the value of self.max_depth.\n    root_idx : ScalarInt, optional\n        The index of the root node in the tree, by default 0. Used\n        for sub-trees.\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        An array that describes the kernel structure.\n    \"\"\"\n    max_depth = max_depth if max_depth is not None else self.max_depth\n\n    if max_depth &lt; 0:\n        raise ValueError(\"'max_depth' must be 0 or larger.\")\n\n    max_nodes = calculate_max_nodes(max_depth)\n\n    # create sample array to be filled, this will be the output (empty\n    # nodes are labeled -1)\n    sample = jnp.full(max_nodes, -1)\n    # create initial stack: empty except for the root node\n    initial_stack = jnp.copy(sample).at[0].set(root_idx)\n\n    pointer = 0  # initial position of the stack pointer\n\n    initial_state = (key, sample, initial_stack, pointer)\n\n    return _sample_single(\n        initial_state,\n        self.probs,\n        self.is_operator,\n        max_depth,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/#gallifrey.kernels.WhiteAtom","title":"<code>WhiteAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>White atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class WhiteAtom(AbstractAtom):\n    \"\"\"White atom.\"\"\"\n\n    name = \"White\"\n    num_parameter = 1\n    parameter_support = [\"positive\"]\n    parameter_names = [\"variance\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        variance = params[0]\n        k = jnp.equal(x, y) * variance\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/","title":"atoms","text":""},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.AbstractAtom","title":"<code>AbstractAtom</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for defining atom kernels (basic kernel functions).</p> <p>Atoms are the fundamental building blocks for constructing more complex kernel structure. This class defines the interface that all atom kernels must implement within the <code>gallifrey</code> library.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>A descriptive name for the atom kernel (e.g., \"RBF\", \"Linear\"). This name is used for identification and representation purposes.</p> <code>num_parameter</code> <code>int</code> <p>The number of trainable parameters associated with this atom kernel. For example, an RBF kernel might have a lengthscale and a variance parameter, so <code>num_parameter</code> would be 2.</p> <code>parameter_support</code> <code>list[Literal['positive', 'real', 'sigmoid', 'none']]</code> <p>A list defining the support constraints for each parameter of the atom kernel. Each element in the list corresponds to a parameter and specifies its constraint: - <code>\"positive\"</code>: Parameter must be positive (e.g., lengthscale, variance). - <code>\"real\"</code>: Parameter can be any real number (unconstrained). - <code>\"sigmoid\"</code>: Parameter is constrained to the range (0, 1) via a sigmoid   function. - <code>\"none\"</code>: No specific constraint is applied. The length of this list should be equal to <code>num_parameter</code>.</p> <code>parameter_names</code> <code>list[str]</code> <p>A list of names for each parameter of the atom kernel. These names should be descriptive and correspond to the parameters in the order defined. The length of this list should be equal to <code>num_parameter</code>.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Abstract method to compute the kernel value between two input points <code>x</code> and <code>y</code>. This method must be implemented by concrete atom kernel classes.</p> <code>__str__</code> <p>Returns the name of the atom kernel as a string.</p> <code>cross_covariance</code> <p>Computes the cross-covariance matrix between two sets of input vectors <code>x</code> and <code>y</code>. Innput must be a 1D vector. The output is a 2D matrix. This method uses <code>jax.vmap</code> for efficient computation over batched inputs.</p> <code>gram</code> <p>Computes the Gram matrix (covariance matrix) for a vector of input points <code>x</code>. This is a special case of <code>cross_covariance</code> where both input sets are the same.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class AbstractAtom(ABC):\n    \"\"\"\n    Abstract base class for defining atom kernels\n    (basic kernel functions).\n\n    Atoms are the fundamental building blocks for constructing more complex kernel\n    structure. This class defines the interface that all atom kernels must\n    implement within the `gallifrey` library.\n\n    Attributes\n    ----------\n    name : str\n        A descriptive name for the atom kernel (e.g., \"RBF\", \"Linear\"). This name\n        is used for identification and representation purposes.\n    num_parameter : int\n        The number of trainable parameters associated with this atom kernel.\n        For example, an RBF kernel might have a lengthscale and a variance parameter,\n        so `num_parameter` would be 2.\n    parameter_support : list[Literal[\"positive\", \"real\", \"sigmoid\", \"none\"]]\n        A list defining the support constraints for each parameter of the atom kernel.\n        Each element in the list corresponds to a parameter and specifies its\n        constraint:\n        - `\"positive\"`: Parameter must be positive (e.g., lengthscale, variance).\n        - `\"real\"`: Parameter can be any real number (unconstrained).\n        - `\"sigmoid\"`: Parameter is constrained to the range (0, 1) via a sigmoid\n          function.\n        - `\"none\"`: No specific constraint is applied.\n        The length of this list should be equal to `num_parameter`.\n    parameter_names : list[str]\n        A list of names for each parameter of the atom kernel. These names should be\n        descriptive and correspond to the parameters in the order defined.\n        The length of this list should be equal to `num_parameter`.\n\n    Methods\n    -------\n    __call__(x, y, params)\n        Abstract method to compute the kernel value between two input points `x`\n        and `y`. This method must be implemented by concrete atom kernel classes.\n    __str__()\n        Returns the name of the atom kernel as a string.\n    cross_covariance(x, y, params)\n        Computes the cross-covariance matrix between two sets of input vectors `x` and\n        `y`. Innput must be a 1D vector. The output is a 2D matrix.\n        This method uses `jax.vmap` for efficient computation over batched inputs.\n    gram(x, params)\n        Computes the Gram matrix (covariance matrix) for a vector of input points `x`.\n        This is a special case of `cross_covariance` where both input sets are the same.\n\n    \"\"\"\n\n    name: str\n    num_parameter: int\n    parameter_support: list[Literal[\"positive\", \"real\", \"sigmoid\", \"none\"]]\n    parameter_names: list[str]\n\n    @abstractmethod\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        pass\n\n    def __str__(self) -&gt; str:\n        return self.name\n\n    def cross_covariance(\n        self,\n        x: Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \"M 1\"],\n        y: Float[jnp.ndarray, \" N\"] | Float[jnp.ndarray, \"N 1\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"M N\"]:\n        return vmap(lambda x_: vmap(lambda y_: self(x_, y_, params))(y))(x)\n\n    def gram(\n        self,\n        x: Float[jnp.ndarray, \" D\"] | Float[jnp.ndarray, \"D 1\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"D D\"]:\n        return self.cross_covariance(x, x, params)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.AbstractOperator","title":"<code>AbstractOperator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract class to define the operators (combination functions) in the kernel structure. The operators are used to combine the atoms in the kernel computations.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class AbstractOperator(ABC):\n    \"\"\"\n    An abstract class to define the operators (combination\n    functions) in the kernel structure. The operators are\n    used to combine the atoms in the kernel computations.\n\n    \"\"\"\n\n    name: str\n    num_parameter: int = 0\n    parameter_support: list = []\n    parameter_names: list = []\n\n    @abstractmethod\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        pass\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.ConstantAtom","title":"<code>ConstantAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Constant atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class ConstantAtom(AbstractAtom):\n    \"\"\"Constant atom.\"\"\"\n\n    name = \"Constant\"\n    num_parameter = 1\n    parameter_support = [\"real\"]\n    parameter_names = [\"constant\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        constant = params[0]\n        return jnp.asarray(constant).astype(jnp.asarray(x).dtype).squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.LinearAtom","title":"<code>LinearAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>(Non-stationary) linear atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class LinearAtom(AbstractAtom):\n    \"\"\"(Non-stationary) linear atom.\"\"\"\n\n    name = \"Linear\"\n    num_parameter = 1\n    parameter_support = [\"positive\"]\n    parameter_names = [\"variance\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        variance = params[0]\n        k = variance * (x * y)\n        return jnp.asarray(k).squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.LinearWithShiftAtom","title":"<code>LinearWithShiftAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>(Non-stationary) linear atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class LinearWithShiftAtom(AbstractAtom):\n    \"\"\"(Non-stationary) linear atom.\"\"\"\n\n    name = \"Linear\"\n    num_parameter = 3\n    parameter_support = [\"positive\", \"positive\", \"real\"]\n    parameter_names = [\"bias\", \"variance\", \"shift\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        bias = params[0]\n        variance = params[1]\n        shift = params[2]\n        k = bias + variance * ((x - shift) * (y - shift))\n        return jnp.asarray(k).squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.Matern12Atom","title":"<code>Matern12Atom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Matern 1/2 atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class Matern12Atom(AbstractAtom):\n    \"\"\"Matern 1/2 atom.\"\"\"\n\n    name = \"Matern12\"\n    num_parameter = 2\n    parameter_support = [\"positive\", \"positive\"]\n    parameter_names = [\"lengthscale\", \"variance\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        lengthscale = params[0]\n        variance = params[1]\n        tau = jnp.abs(x - y) / lengthscale\n        k = variance * jnp.exp(-tau)\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.Matern32Atom","title":"<code>Matern32Atom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Matern 3/2 atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class Matern32Atom(AbstractAtom):\n    \"\"\"Matern 3/2 atom.\"\"\"\n\n    name = \"Matern32\"\n    num_parameter = 2\n    parameter_support = [\"positive\", \"positive\"]\n    parameter_names = [\"lengthscale\", \"variance\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        lengthscale = params[0]\n        variance = params[1]\n        tau = jnp.sqrt(3.0) * jnp.abs(x - y) / lengthscale\n        k = variance * (1.0 + tau) * jnp.exp(-tau)\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.Matern52Atom","title":"<code>Matern52Atom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Matern 5/2 atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class Matern52Atom(AbstractAtom):\n    \"\"\"Matern 5/2 atom.\"\"\"\n\n    name = \"Matern52\"\n    num_parameter = 2\n    parameter_support = [\"positive\", \"positive\"]\n    parameter_names = [\"lengthscale\", \"variance\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        lengthscale = params[0]\n        variance = params[1]\n        tau = jnp.sqrt(5.0) * jnp.abs(x - y) / lengthscale\n        k = variance * (1.0 + tau + jnp.square(tau) / 3.0) * jnp.exp(-tau)\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.PeriodicAtom","title":"<code>PeriodicAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Periodic atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class PeriodicAtom(AbstractAtom):\n    \"\"\"Periodic atom.\"\"\"\n\n    name = \"Periodic\"\n    num_parameter = 3\n    parameter_support = [\"positive\", \"positive\", \"positive\"]\n    parameter_names = [\"lengthscale\", \"variance\", \"period\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        lengthscale = params[0]\n        variance = params[1]\n        period = params[2]\n        sine_squared = (jnp.sin(jnp.pi * (x - y) / period) / lengthscale) ** 2\n        k = variance * jnp.exp(-0.5 * sine_squared)\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.PoweredExponentialAtom","title":"<code>PoweredExponentialAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Powered Exponential atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class PoweredExponentialAtom(AbstractAtom):\n    \"\"\"Powered Exponential atom.\"\"\"\n\n    name = \"PoweredExponential\"\n    num_parameter = 3\n    parameter_support = [\"positive\", \"positive\", \"sigmoid\"]\n    parameter_names = [\"lengthscale\", \"variance\", \"power\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        lengthscale = params[0]\n        variance = params[1]\n        power = params[2]\n        tau = jnp.abs(x - y) / lengthscale\n        k = variance * jnp.exp(-jnp.power(tau, power))\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.ProductOperator","title":"<code>ProductOperator</code>","text":"<p>               Bases: <code>AbstractOperator</code></p> <p>Product Operator to combine two kernel functions.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class ProductOperator(AbstractOperator):\n    \"\"\"Product Operator to combine two kernel functions.\"\"\"\n\n    name = \"*\"\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        return jnp.asarray(x * y).squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.RBFAtom","title":"<code>RBFAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Radial Basis Function/Squared Exponential atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class RBFAtom(AbstractAtom):\n    \"\"\"Radial Basis Function/Squared Exponential atom.\"\"\"\n\n    name = \"RBF\"\n    num_parameter = 2\n    parameter_support = [\"positive\", \"positive\"]\n    parameter_names = [\"lengthscale\", \"variance\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        lengthscale = params[0]\n        variance = params[1]\n        d_squared = jnp.square(x - y) / jnp.square(lengthscale)\n        k = variance * jnp.exp(-0.5 * d_squared)\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.RationalQuadraticAtom","title":"<code>RationalQuadraticAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>Rational Quadratic atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class RationalQuadraticAtom(AbstractAtom):\n    \"\"\"Rational Quadratic atom.\"\"\"\n\n    name = \"RationalQuadratic\"\n    num_parameter = 3\n    parameter_support = [\"positive\", \"positive\", \"positive\"]\n    parameter_names = [\"lengthscale\", \"variance\", \"alpha\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        lengthscale = params[0]\n        variance = params[1]\n        alpha = params[2]\n        tau = jnp.abs(x - y) / lengthscale\n        k = variance * (1 + 0.5 * jnp.square(tau) / alpha) ** (-alpha)\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.SumOperator","title":"<code>SumOperator</code>","text":"<p>               Bases: <code>AbstractOperator</code></p> <p>Sum Operator to combine two kernel functions.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class SumOperator(AbstractOperator):\n    \"\"\"Sum Operator to combine two kernel functions.\"\"\"\n\n    name = \"+\"\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        return jnp.asarray(x + y).squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/atoms/#gallifrey.kernels.atoms.WhiteAtom","title":"<code>WhiteAtom</code>","text":"<p>               Bases: <code>AbstractAtom</code></p> <p>White atom.</p> Source code in <code>gallifrey/kernels/atoms.py</code> <pre><code>class WhiteAtom(AbstractAtom):\n    \"\"\"White atom.\"\"\"\n\n    name = \"White\"\n    num_parameter = 1\n    parameter_support = [\"positive\"]\n    parameter_names = [\"variance\"]\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \"...\"],\n        y: Float[jnp.ndarray, \"...\"],\n        params: Float[jnp.ndarray, \" P\"],\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        variance = params[0]\n        k = jnp.equal(x, y) * variance\n        return k.squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/library/","title":"library","text":""},{"location":"autoapi/gallifrey/kernels/library/#gallifrey.kernels.library.KernelLibrary","title":"<code>KernelLibrary</code>","text":"<p>A class to store the kernel library, which contains information about what atoms and operators are available for constructing the tree kernel.</p> <p>Attributes:</p> Name Type Description <code>atoms</code> <code>Optional[list[AbstractAtom]]</code> <p>A list of atoms that are used to construct the tree kernel. If None, the default atoms are used. Atoms should inherit from the AbstractAtom class in gallifrey.kernels.atoms.</p> <code>operators</code> <code>Optional[list[AbstractOperator]]</code> <p>A list of operators that are used to combine the atoms in the tree kernel. If None, the default operators are used. Operators should inherit from the AbstractOperator class in gallifrey.kernels.atoms.</p> <code>num_atoms</code> <code>int</code> <p>The number of atoms in the library.</p> <code>num_operators</code> <code>int</code> <p>The number of operators in the library.</p> <code>library</code> <code>list[Union[AbstractAtom, AbstractOperator]]</code> <p>A list combining the atoms and operators in the library.</p> <code>is_operator</code> <code>ndarray</code> <p>A boolean array to indicate if an entry in the library is an operator or not.</p> <code>max_atom_parameters</code> <code>int</code> <p>The maximum number of parameters any atom or operator in the library takes as input.</p> <code>prior_transforms</code> <code>Optional[dict[str, Bijector]]</code> <p>A dictionary that maps the parameter tags to the corresponding transform functions. Transformation functions must transform parameter from a standard normal distribution to the desired (prior) distribution, and should be implemented via tensorflow_probability bijectors. The default transformations are used if None. The default transformations are: - \"positive\": Log-normal transformation(mu=0.0, sigma=1.0) - \"real\": Log-normal transformation(mu=0.0, sigma=1.0) - \"sigmoid\": Logit-normal transform(scale=1.0, mu=0.0, sigma=1.0) NOTE: To make functions that use this dict jittable, the prior_transforms is converted to a FrozenDict, to make it hashable. NOTE: The \"none\" key is reserved for internal use and should not be used in the prior_transforms dictionary.</p> <code>support_tag_mapping</code> <code>dict[str, int]</code> <p>A dictionary that maps the support tags to integer values.</p> <code>support_mapping_array</code> <code>ndarray</code> <p>An array that contains the support tags as integers for each parameter in the library.</p> <code>support_transforms</code> <code>FrozenDict[str, Transformation]</code> <p>A (frozen) dict that maps the parameter tags to the corresponding transform functions. Transformation functions must transform parameter from a constrained space to an unconstrained space. This is used for sampling and optimization. NOTE: This should ideally never be touched, unless you know what you are doing.</p> Source code in <code>gallifrey/kernels/library.py</code> <pre><code>class KernelLibrary:\n    \"\"\"\n    A class to store the kernel library, which contains information\n    about what atoms and operators are available for constructing\n    the tree kernel.\n\n    Attributes\n    ----------\n    atoms : tp.Optional[list[AbstractAtom]]\n        A list of atoms that are used to construct the tree kernel.\n        If None, the default atoms are used. Atoms should inherit\n        from the AbstractAtom class in gallifrey.kernels.atoms.\n    operators : tp.Optional[list[AbstractOperator]]\n        A list of operators that are used to combine the atoms in the\n        tree kernel. If None, the default operators are used. Operators\n        should inherit from the AbstractOperator class in\n        gallifrey.kernels.atoms.\n    num_atoms : int\n        The number of atoms in the library.\n    num_operators : int\n        The number of operators in the library.\n    library : list[tp.Union[AbstractAtom, AbstractOperator]]\n        A list combining the atoms and operators in the library.\n    is_operator : jnp.ndarray\n        A boolean array to indicate if an entry in the library is an operator\n        or not.\n    max_atom_parameters : int\n        The maximum number of parameters any atom or operator in the library\n        takes as input.\n    prior_transforms : tp.Optional[dict[str, tfb.Bijector]]\n        A dictionary that maps the parameter tags to the corresponding\n        transform functions. Transformation functions must transform\n        parameter from a standard normal distribution to the desired\n        (prior) distribution, and should be implemented via\n        tensorflow_probability bijectors. The default transformations\n        are used if None. The default transformations are:\n        - \"positive\": Log-normal transformation(mu=0.0, sigma=1.0)\n        - \"real\": Log-normal transformation(mu=0.0, sigma=1.0)\n        - \"sigmoid\": Logit-normal transform(scale=1.0, mu=0.0, sigma=1.0)\n        NOTE: To make functions that use this dict jittable, the prior_transforms\n        is converted to a FrozenDict, to make it hashable.\n        NOTE: The \"none\" key is reserved for internal use and should not be\n        used in the prior_transforms dictionary.\n    support_tag_mapping : dict[str, int]\n        A dictionary that maps the support tags to integer values.\n    support_mapping_array : jnp.ndarray\n        An array that contains the support tags as integers for each parameter\n        in the library.\n    support_transforms : FrozenDict[str, Transformation]\n        A (frozen) dict that maps the parameter tags to the corresponding transform\n        functions. Transformation functions must transform parameter from a constrained\n        space to an unconstrained space. This is used for sampling and optimization.\n        NOTE: This should ideally never be touched, unless you know what you are doing.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        atoms: tp.Optional[list[AbstractAtom]] = None,\n        operators: tp.Optional[list[AbstractOperator]] = None,\n        prior_transforms: tp.Optional[dict[str, tfb.Bijector]] = None,\n    ):\n        \"\"\"\n        Initialize the KernelLibrary. If no atoms or operators are provided,\n        a default library is used.\n\n        Parameters\n        ----------\n        atoms : tp.Optional[list[AbstractAtom]], optional\n            A list of atoms that are used to construct the tree kernel.\n            If None, the default atoms are used. Atoms should inherit\n            from the AbstractAtom class in gallifrey.kernels.atoms, by default None.\n        operators : tp.Optional[list[AbstractOperator]], optional\n            A list of operators that are used to combine the atoms in the\n            tree kernel. If None, the default operators are used. Operators\n            should inherit from the AbstractOperator class in\n            gallifrey.kernels.atoms, by default None.\n        prior_transforms : tp.Optional[dict[str, Transformation]], optional\n            A dictionary that maps the parameter tags to the corresponding transform\n            functions. Transformation functions must transform parameter from a standard\n            normal distribution to the desired (prior) distribution, and should be\n            implemented via tensorflow_probability bijectors.\n            The default transformations are used if None, by default None. The default\n            transformations are:\n            - \"positive\": Log-normal transformation(mu=0.0, sigma=1.0)\n            - \"real\": Log-normal transformation(mu=0.0, sigma=1.0)\n            - \"sigmoid\": Logit-normal transform(scale=1.0, mu=0.0, sigma=1.0)\n            NOTE: To make 'transformation' function (see gallifrey.kernels.prior)\n            jitable, the prior_transforms must be hashable. This is why we use\n            FrozenDict instead of dict.\n\n        \"\"\"\n        self.atoms: list[AbstractAtom] = (\n            atoms\n            if atoms is not None\n            else [\n                LinearAtom(),\n                PeriodicAtom(),\n                RBFAtom(),\n            ]\n        )\n\n        self.operators: list[AbstractOperator] = (\n            operators\n            if operators is not None\n            else [\n                SumOperator(),\n                ProductOperator(),\n            ]\n        )\n\n        self.num_atoms = len(self.atoms)\n        self.num_operators = len(self.operators)\n\n        # construct the library by combining the atoms and operators\n        self.library = self.atoms + self.operators\n\n        # create boolean array to indicate if entry is operator or not\n        self.is_operator = jnp.array(\n            [False] * len(self.atoms) + [True] * len(self.operators)\n        )\n\n        # get the maximum number of parameters any atom or operator takes\n        self.max_atom_parameters = max([item.num_parameter for item in self.atoms])\n\n        # transformation functions from normal distribution to prior distribution\n        # NOTE: names are inherited from GPJax, but we don't use the same\n        # transformations (which is why sigmoid is the name for the logit-normal)\n        if prior_transforms is None:\n            self.prior_transforms: FrozenDict[str, tfb.Bijector] = FrozenDict(\n                {\n                    # this is the transformation y = exp(mu + sigma * z),\n                    # with mu = 0 and sigma = 1,\n                    # if z ~ normal(0, 1) then y ~ log-normal(mu, sigma)\n                    \"real\": tfb.Chain(\n                        [\n                            tfb.Exp(),\n                            tfb.Shift(jnp.array(0.0)),\n                            tfb.Scale(jnp.array(1.0)),\n                        ]\n                    ),\n                    \"positive\": tfb.Chain(\n                        [\n                            tfb.Exp(),\n                            tfb.Shift(jnp.array(0.0)),\n                            tfb.Scale(jnp.array(1.0)),\n                        ]\n                    ),\n                    # this is the transformation y = 1/(1 + exp(-(mu + sigma * z))),\n                    # with mu = 0 and sigma = 1,\n                    # if z ~ normal(0, 1) then y ~ logit-normal(mu, sigma)\n                    \"sigmoid\": tfb.Chain(\n                        [\n                            tfb.Sigmoid(\n                                low=jnp.array(0.0),\n                                high=jnp.array(0.95),  # to avoid numerical issues\n                            ),\n                            tfb.Shift(jnp.array(0.0)),\n                            tfb.Scale(jnp.array(0.0)),\n                        ]\n                    ),\n                    # identity transformation for parameter that do not fall under\n                    # the above categories\n                    \"none\": tfb.Identity(),\n                }\n            )\n\n        else:\n            if \"none\" in prior_transforms.keys():\n                raise ValueError(\n                    \"'prior_transforms' should not contain the key 'none', \"\n                    \"as it is reserved for internal use.\"\n                )\n            prior_transforms[\"none\"] = tfb.Identity()\n            self.prior_transforms = FrozenDict(prior_transforms)\n\n        self.support_mapping_array, self.support_tag_mapping = (\n            self.get_support_mapping()\n        )\n\n        # besides the prior transforms, we also perform transforms\n        # between a constrained and unconstrained space for fitting/sampling,\n        # this probably should never be touched unless you implement a new\n        # kernel with new constraints\n        self.support_transforms: FrozenDict[str, tfb.Bijector] = FrozenDict(\n            {\n                \"positive\": tfb.Softplus(),\n                \"real\": tfb.Identity(),\n                \"sigmoid\": tfb.Sigmoid(low=0.0, high=0.95),\n                \"lower_triangular\": tfb.FillTriangular(),\n                \"none\": tfb.Identity(),\n            }\n        )\n\n        self._check_tags()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Get the (technical) string representation of the KernelLibrary.\n\n        Returns\n        -------\n        str\n            The (technical) string representation of the KernelLibrary.\n        \"\"\"\n\n        return (\n            f\"KernelLibrary(\\n  Atoms={self.atoms},\\n   \"\n            f\"operators={self.operators}\\n)\"\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Get the (simplified) string representation of the KernelLibrary.\n\n        Returns\n        -------\n        str\n            The (simplified) string representation of the KernelLibrary.\n        \"\"\"\n        try:\n            atom_names: list[tp.Any] = [atom.name for atom in self.atoms]\n        except Exception:\n            atom_names = self.atoms\n        try:\n            operator_names: list[tp.Any] = [\n                operator.name for operator in self.operators\n            ]\n        except Exception:\n            operator_names = self.operators\n\n        return (\n            f\"KernelLibrary(\\n  Atoms={atom_names},\\n   \"\n            f\"operators={operator_names}\\n)\"\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Get the number of items in the library.\n\n        Returns\n        -------\n        int\n            The number of items in the library.\n        \"\"\"\n        return len(self.library)\n\n    def get_support_mapping(self) -&gt; tuple[jnp.ndarray, dict[str, int]]:\n        \"\"\"\n        Create a mapping between support tags and the integer values, then\n        create an array that contains the support tags for each parameter\n        in the library.\n        (Used for jittable sampling and parameter transformations)\n\n        Returns\n        -------\n        support_mapping_array : jnp.ndarray\n            An array that contains the support tags for each parameter\n            in the library, with shape (len(self), self.max_atom_parameters),\n            enocded as integers.\n        support_tag_mapping : dict[str, int]\n            A dictionary that maps the support tags to the corresponding\n            integer values.\n        \"\"\"\n\n        # create dict that maps string tags to integer values\n        support_tag_mapping = {\n            support_tag: i for i, support_tag in enumerate(self.prior_transforms.keys())\n        }\n        support_tag_mapping[\"none\"] = -1\n\n        # create array that contains the support tag integers for each parameter\n        # in the library\n        support_mapping_array = jnp.full(\n            (len(self), self.max_atom_parameters), support_tag_mapping[\"none\"]\n        )\n\n        for i in range(len(self)):\n            for j in range(self.library[i].num_parameter):\n                support_mapping_array = support_mapping_array.at[i, j].set(\n                    support_tag_mapping[self.library[i].parameter_support[j]]\n                )\n        return support_mapping_array, support_tag_mapping\n\n    def _check_tags(self) -&gt; None:\n        \"\"\"\n        Check if the parameters of the atoms have constraints that are not yet\n        implemented in the library. If so, raise a warning.\n\n        \"\"\"\n        parameter_support_tags = {\n            atom.name: atom.parameter_support for atom in self.atoms\n        }\n\n        for atom_name, tags in parameter_support_tags.items():\n            if not all(tag in self.prior_transforms.keys() for tag in tags):\n                warnings.warn(\n                    f\"Atom {atom_name!r} has parameter tags that are not \"\n                    \"in the prior_transforms dictionary. Tag will be ignored. \"\n                    \"This will lead to problems when performing parameter \"\n                    \"transformations. Add the missing tags to the prior_transforms\"\n                    \"by manually passing the prior_transforms dictionary to the \"\n                    \"KernelLibrary constructor.\"\n                )\n\n        if not all(tag in self.support_transforms.keys() for tag in tags):\n            warnings.warn(\n                f\"Kernel {atom_name!r} has parameter tags that are not \"\n                \"in the support_transforms dictionary. This will lead to problems \"\n                \"in the sampling/optmization if used for a KernelTree object. \"\n                \"Add the missing tags to the by manually overriding the \"\n                \"support_transforms dictionary to the KernelLibrary constructor.\\n\"\n                \"THIS IS VERY EXPERIMENTAL AND NOT RECOMMENDED.\"\n            )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/library/#gallifrey.kernels.library.KernelLibrary.__init__","title":"<code>__init__(atoms=None, operators=None, prior_transforms=None)</code>","text":"<p>Initialize the KernelLibrary. If no atoms or operators are provided, a default library is used.</p> <p>Parameters:</p> Name Type Description Default <code>atoms</code> <code>Optional[list[AbstractAtom]]</code> <p>A list of atoms that are used to construct the tree kernel. If None, the default atoms are used. Atoms should inherit from the AbstractAtom class in gallifrey.kernels.atoms, by default None.</p> <code>None</code> <code>operators</code> <code>Optional[list[AbstractOperator]]</code> <p>A list of operators that are used to combine the atoms in the tree kernel. If None, the default operators are used. Operators should inherit from the AbstractOperator class in gallifrey.kernels.atoms, by default None.</p> <code>None</code> <code>prior_transforms</code> <code>Optional[dict[str, Transformation]]</code> <p>A dictionary that maps the parameter tags to the corresponding transform functions. Transformation functions must transform parameter from a standard normal distribution to the desired (prior) distribution, and should be implemented via tensorflow_probability bijectors. The default transformations are used if None, by default None. The default transformations are: - \"positive\": Log-normal transformation(mu=0.0, sigma=1.0) - \"real\": Log-normal transformation(mu=0.0, sigma=1.0) - \"sigmoid\": Logit-normal transform(scale=1.0, mu=0.0, sigma=1.0) NOTE: To make 'transformation' function (see gallifrey.kernels.prior) jitable, the prior_transforms must be hashable. This is why we use FrozenDict instead of dict.</p> <code>None</code> Source code in <code>gallifrey/kernels/library.py</code> <pre><code>def __init__(\n    self,\n    atoms: tp.Optional[list[AbstractAtom]] = None,\n    operators: tp.Optional[list[AbstractOperator]] = None,\n    prior_transforms: tp.Optional[dict[str, tfb.Bijector]] = None,\n):\n    \"\"\"\n    Initialize the KernelLibrary. If no atoms or operators are provided,\n    a default library is used.\n\n    Parameters\n    ----------\n    atoms : tp.Optional[list[AbstractAtom]], optional\n        A list of atoms that are used to construct the tree kernel.\n        If None, the default atoms are used. Atoms should inherit\n        from the AbstractAtom class in gallifrey.kernels.atoms, by default None.\n    operators : tp.Optional[list[AbstractOperator]], optional\n        A list of operators that are used to combine the atoms in the\n        tree kernel. If None, the default operators are used. Operators\n        should inherit from the AbstractOperator class in\n        gallifrey.kernels.atoms, by default None.\n    prior_transforms : tp.Optional[dict[str, Transformation]], optional\n        A dictionary that maps the parameter tags to the corresponding transform\n        functions. Transformation functions must transform parameter from a standard\n        normal distribution to the desired (prior) distribution, and should be\n        implemented via tensorflow_probability bijectors.\n        The default transformations are used if None, by default None. The default\n        transformations are:\n        - \"positive\": Log-normal transformation(mu=0.0, sigma=1.0)\n        - \"real\": Log-normal transformation(mu=0.0, sigma=1.0)\n        - \"sigmoid\": Logit-normal transform(scale=1.0, mu=0.0, sigma=1.0)\n        NOTE: To make 'transformation' function (see gallifrey.kernels.prior)\n        jitable, the prior_transforms must be hashable. This is why we use\n        FrozenDict instead of dict.\n\n    \"\"\"\n    self.atoms: list[AbstractAtom] = (\n        atoms\n        if atoms is not None\n        else [\n            LinearAtom(),\n            PeriodicAtom(),\n            RBFAtom(),\n        ]\n    )\n\n    self.operators: list[AbstractOperator] = (\n        operators\n        if operators is not None\n        else [\n            SumOperator(),\n            ProductOperator(),\n        ]\n    )\n\n    self.num_atoms = len(self.atoms)\n    self.num_operators = len(self.operators)\n\n    # construct the library by combining the atoms and operators\n    self.library = self.atoms + self.operators\n\n    # create boolean array to indicate if entry is operator or not\n    self.is_operator = jnp.array(\n        [False] * len(self.atoms) + [True] * len(self.operators)\n    )\n\n    # get the maximum number of parameters any atom or operator takes\n    self.max_atom_parameters = max([item.num_parameter for item in self.atoms])\n\n    # transformation functions from normal distribution to prior distribution\n    # NOTE: names are inherited from GPJax, but we don't use the same\n    # transformations (which is why sigmoid is the name for the logit-normal)\n    if prior_transforms is None:\n        self.prior_transforms: FrozenDict[str, tfb.Bijector] = FrozenDict(\n            {\n                # this is the transformation y = exp(mu + sigma * z),\n                # with mu = 0 and sigma = 1,\n                # if z ~ normal(0, 1) then y ~ log-normal(mu, sigma)\n                \"real\": tfb.Chain(\n                    [\n                        tfb.Exp(),\n                        tfb.Shift(jnp.array(0.0)),\n                        tfb.Scale(jnp.array(1.0)),\n                    ]\n                ),\n                \"positive\": tfb.Chain(\n                    [\n                        tfb.Exp(),\n                        tfb.Shift(jnp.array(0.0)),\n                        tfb.Scale(jnp.array(1.0)),\n                    ]\n                ),\n                # this is the transformation y = 1/(1 + exp(-(mu + sigma * z))),\n                # with mu = 0 and sigma = 1,\n                # if z ~ normal(0, 1) then y ~ logit-normal(mu, sigma)\n                \"sigmoid\": tfb.Chain(\n                    [\n                        tfb.Sigmoid(\n                            low=jnp.array(0.0),\n                            high=jnp.array(0.95),  # to avoid numerical issues\n                        ),\n                        tfb.Shift(jnp.array(0.0)),\n                        tfb.Scale(jnp.array(0.0)),\n                    ]\n                ),\n                # identity transformation for parameter that do not fall under\n                # the above categories\n                \"none\": tfb.Identity(),\n            }\n        )\n\n    else:\n        if \"none\" in prior_transforms.keys():\n            raise ValueError(\n                \"'prior_transforms' should not contain the key 'none', \"\n                \"as it is reserved for internal use.\"\n            )\n        prior_transforms[\"none\"] = tfb.Identity()\n        self.prior_transforms = FrozenDict(prior_transforms)\n\n    self.support_mapping_array, self.support_tag_mapping = (\n        self.get_support_mapping()\n    )\n\n    # besides the prior transforms, we also perform transforms\n    # between a constrained and unconstrained space for fitting/sampling,\n    # this probably should never be touched unless you implement a new\n    # kernel with new constraints\n    self.support_transforms: FrozenDict[str, tfb.Bijector] = FrozenDict(\n        {\n            \"positive\": tfb.Softplus(),\n            \"real\": tfb.Identity(),\n            \"sigmoid\": tfb.Sigmoid(low=0.0, high=0.95),\n            \"lower_triangular\": tfb.FillTriangular(),\n            \"none\": tfb.Identity(),\n        }\n    )\n\n    self._check_tags()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/library/#gallifrey.kernels.library.KernelLibrary.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of items in the library.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of items in the library.</p> Source code in <code>gallifrey/kernels/library.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Get the number of items in the library.\n\n    Returns\n    -------\n    int\n        The number of items in the library.\n    \"\"\"\n    return len(self.library)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/library/#gallifrey.kernels.library.KernelLibrary.__repr__","title":"<code>__repr__()</code>","text":"<p>Get the (technical) string representation of the KernelLibrary.</p> <p>Returns:</p> Type Description <code>str</code> <p>The (technical) string representation of the KernelLibrary.</p> Source code in <code>gallifrey/kernels/library.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Get the (technical) string representation of the KernelLibrary.\n\n    Returns\n    -------\n    str\n        The (technical) string representation of the KernelLibrary.\n    \"\"\"\n\n    return (\n        f\"KernelLibrary(\\n  Atoms={self.atoms},\\n   \"\n        f\"operators={self.operators}\\n)\"\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/library/#gallifrey.kernels.library.KernelLibrary.__str__","title":"<code>__str__()</code>","text":"<p>Get the (simplified) string representation of the KernelLibrary.</p> <p>Returns:</p> Type Description <code>str</code> <p>The (simplified) string representation of the KernelLibrary.</p> Source code in <code>gallifrey/kernels/library.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Get the (simplified) string representation of the KernelLibrary.\n\n    Returns\n    -------\n    str\n        The (simplified) string representation of the KernelLibrary.\n    \"\"\"\n    try:\n        atom_names: list[tp.Any] = [atom.name for atom in self.atoms]\n    except Exception:\n        atom_names = self.atoms\n    try:\n        operator_names: list[tp.Any] = [\n            operator.name for operator in self.operators\n        ]\n    except Exception:\n        operator_names = self.operators\n\n    return (\n        f\"KernelLibrary(\\n  Atoms={atom_names},\\n   \"\n        f\"operators={operator_names}\\n)\"\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/library/#gallifrey.kernels.library.KernelLibrary.get_support_mapping","title":"<code>get_support_mapping()</code>","text":"<p>Create a mapping between support tags and the integer values, then create an array that contains the support tags for each parameter in the library. (Used for jittable sampling and parameter transformations)</p> <p>Returns:</p> Name Type Description <code>support_mapping_array</code> <code>ndarray</code> <p>An array that contains the support tags for each parameter in the library, with shape (len(self), self.max_atom_parameters), enocded as integers.</p> <code>support_tag_mapping</code> <code>dict[str, int]</code> <p>A dictionary that maps the support tags to the corresponding integer values.</p> Source code in <code>gallifrey/kernels/library.py</code> <pre><code>def get_support_mapping(self) -&gt; tuple[jnp.ndarray, dict[str, int]]:\n    \"\"\"\n    Create a mapping between support tags and the integer values, then\n    create an array that contains the support tags for each parameter\n    in the library.\n    (Used for jittable sampling and parameter transformations)\n\n    Returns\n    -------\n    support_mapping_array : jnp.ndarray\n        An array that contains the support tags for each parameter\n        in the library, with shape (len(self), self.max_atom_parameters),\n        enocded as integers.\n    support_tag_mapping : dict[str, int]\n        A dictionary that maps the support tags to the corresponding\n        integer values.\n    \"\"\"\n\n    # create dict that maps string tags to integer values\n    support_tag_mapping = {\n        support_tag: i for i, support_tag in enumerate(self.prior_transforms.keys())\n    }\n    support_tag_mapping[\"none\"] = -1\n\n    # create array that contains the support tag integers for each parameter\n    # in the library\n    support_mapping_array = jnp.full(\n        (len(self), self.max_atom_parameters), support_tag_mapping[\"none\"]\n    )\n\n    for i in range(len(self)):\n        for j in range(self.library[i].num_parameter):\n            support_mapping_array = support_mapping_array.at[i, j].set(\n                support_tag_mapping[self.library[i].parameter_support[j]]\n            )\n    return support_mapping_array, support_tag_mapping\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/","title":"prior","text":""},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.KernelPrior","title":"<code>KernelPrior</code>","text":"<p>A prior distribution over kernel structures and parameters.</p> <p>Attributes:</p> Name Type Description <code>kernel_library</code> <code>KernelLibrary</code> <p>An instance of the KernelLibrary class, containing the kernel classes and operators, and transformation functions.</p> <code>kernel_structure_prior</code> <code>TreeStructurePrior</code> <p>A prior distribution over kernel structures.</p> <code>parameter_prior</code> <code>ParameterPrior</code> <p>A prior distribution over kernel parameters.</p> <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the nested kernel structure tree.</p> <code>num_datapoints</code> <code>ScalarInt</code> <p>The number of data points in the dataset.</p> <code>max_kernel_parameter</code> <code>int</code> <p>The maximum number of kernel parameters (max_atom_parameters * max_leaves).</p> <code>graphdef</code> <code>GraphDef</code> <p>The graph definition of the kernel. Used together with the kernel state to create a TreeKernel object.</p> <code>kernels</code> <code>List[Type[TreeKernel]]</code> <p>A list of kernel classes. (inherited from the kernel_library, see gallifrey.kernels.library.KernelLibrary)</p> <code>operators</code> <code>List[Callable]</code> <p>A list of operators. (inherited from the kernel_library, see gallifrey.kernels.library.KernelLibrary)</p> <code>is_operator</code> <code>Bool[ndarray, ' D']</code> <p>An array that indicates whether each kernel in the library is an operator. (inherited from the kernel_library, see gallifrey.kernels.library.KernelLibrary)</p> <code>probs</code> <code>Float[ndarray, ' D']</code> <p>The probabilities of sampling each kernel (or operator) in the library. (inherited from the kernel_structure_prior,   see gallifrey.kernels.prior.TreeStructurePrior)</p> <code>prior_transforms</code> <code>FrozenDict[str, Bijector]</code> <p>A (frozen) dictionary containing bijectors for transforming the distribution of the sampled parameters from a standard normal to the desired prior distribution. (inherited from the kernel_library, see gallifrey.kernels.library.KernelLibrary)</p> <code>support_bijectors</code> <code>tuple[Bijector, ...]</code> <p>A tuple of bijectors for transforming the parameters from a constrained support space to an unconstrained space. Primarely used for the numerically stable optimization and sampling of the parameters (in a jit-compatible way).</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>class KernelPrior:\n    \"\"\"\n    A prior distribution over kernel structures and parameters.\n\n    Attributes\n    ----------\n    kernel_library : KernelLibrary\n        An instance of the KernelLibrary class, containing\n        the kernel classes and operators, and transformation functions.\n    kernel_structure_prior : TreeStructurePrior\n        A prior distribution over kernel structures.\n    parameter_prior : ParameterPrior\n        A prior distribution over kernel parameters.\n    max_depth : ScalarInt\n        The maximum depth of the nested kernel structure tree.\n    num_datapoints : ScalarInt\n        The number of data points in the dataset.\n    max_kernel_parameter : int\n        The maximum number of kernel parameters (max_atom_parameters\n        * max_leaves).\n\n    graphdef : nnx.GraphDef\n        The graph definition of the kernel. Used together with the\n        kernel state to create a TreeKernel object.\n\n    kernels : tp.List[tp.Type[TreeKernel]]\n        A list of kernel classes. (inherited from the kernel_library,\n        see gallifrey.kernels.library.KernelLibrary)\n    operators : tp.List[tp.Callable]\n        A list of operators. (inherited from the kernel_library,\n        see gallifrey.kernels.library.KernelLibrary)\n    is_operator : Bool[jnp.ndarray, \" D\"]\n        An array that indicates whether each kernel in the library is an operator.\n        (inherited from the kernel_library, see gallifrey.kernels.library.KernelLibrary)\n    probs :  Float[jnp.ndarray, \" D\"]\n        The probabilities of sampling each kernel (or operator) in the library.\n        (inherited from the kernel_structure_prior,\n          see gallifrey.kernels.prior.TreeStructurePrior)\n    prior_transforms : FrozenDict[str, tfb.Bijector]\n        A (frozen) dictionary containing bijectors for transforming the\n        distribution of the sampled parameters from a standard\n        normal to the desired prior distribution. (inherited from the kernel_library,\n        see gallifrey.kernels.library.KernelLibrary)\n    support_bijectors : tuple[tfb.Bijector, ...]\n        A tuple of bijectors for transforming the parameters from a constrained\n        support space to an unconstrained space. Primarely used for the\n        numerically stable optimization and sampling of the parameters (in a\n        jit-compatible way).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel_library: KernelLibrary,\n        max_depth: int,\n        num_datapoints: int,\n        probs: tp.Optional[Float[jnp.ndarray, \" D\"]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the KernelPrior class.\n\n        Parameters\n        ----------\n        kernel_library : KernelLibrary\n            An instance of the KernelLibrary class, containing\n            the kernel classes and operators, and transformation functions.\n            The TreeStructurePrior and ParameterPrior classes are initialized\n            using the kernel_library.\n        max_depth : int\n            The maximum depth of the nested kernel structure tree.\n        num_datapoints : int\n            The number of data points in the dataset.\n        probs : tp.Optional[ Float[jnp.ndarray, \" D\"]], optional\n            The probabilities of sampling each kernel (or operator) in the library.\n            The array must have the same length as the the arrays in the kernel library.\n            By default None, which will use a uniform distribution.\n        \"\"\"\n        self.kernel_library = kernel_library\n\n        self.kernel_structure_prior = TreeStructurePrior(\n            self.kernel_library,\n            max_depth,\n            probs,\n        )\n\n        self.num_datapoints = num_datapoints\n\n        self.parameter_prior = ParameterPrior(self.kernel_library, max_depth)\n\n        self.graphdef = self._get_graphdef()\n\n        self.max_kernel_parameter = (\n            kernel_library.max_atom_parameters * calculate_max_leaves(self.max_depth)\n        )\n\n        self.support_bijectors = tuple(\n            [\n                self.kernel_library.support_transforms[tag]\n                for tag in self.kernel_library.support_tag_mapping\n            ]\n        )\n\n    def sample(self, key: PRNGKeyArray) -&gt; nnx.State:\n        \"\"\"\n        Sample a kernel structure and its parameters from the prior distribution.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for sampling.\n\n        Returns\n        -------\n        nnx.State\n        The state of the sampled kernel, combine with the graphdef\n        to create a TreeKernel object.\n\n        \"\"\"\n        tree_key, parameter_key = jr.split(key)\n        kernel_structure = self.kernel_structure_prior.sample(\n            tree_key,\n            self.kernel_structure_prior.max_depth,\n        )\n\n        kernel = TreeKernel(\n            kernel_structure,\n            self.kernel_library,\n            self.max_depth,\n            self.num_datapoints,\n        )\n\n        _, state = nnx.split(kernel)\n\n        kernel_state, _ = self.parameter_prior.sample(\n            parameter_key,\n            nnx.State(state),\n        )\n        return kernel_state\n\n    def sample_kernel(self, key: PRNGKeyArray) -&gt; TreeKernel:\n        \"\"\"\n        Sample a kernel state using self.sample and merge\n        it with the graphdef and static_state to create a\n        TreeKernel.\n\n        This function is convenient for sampling a kernel\n        directly, but not vmap or jittable (at least not\n        using the jax commands, potentially using the\n        nnx ones.)\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for sampling.\n\n        Returns\n        -------\n        TreeKernel\n            The sampled kernel.\n        \"\"\"\n\n        kernel_state = self.sample(key)\n        return nnx.merge(self.graphdef, kernel_state)\n\n    def reconstruct_kernel(\n        self,\n        kernel_state: nnx.State,\n    ) -&gt; TreeKernel:\n        \"\"\"\n        Create new TreeKernel from kernel state using the graphdef,\n        and reset some kernel attributes. In principle,\n        this is unnecessary since the values are already\n        set in the kernel state, but we need to make it\n        explicit for the jit compilation.\n\n        Parameters\n        ----------\n        kernel_state : nnx.State\n            The kernel state to be used to create the kernel.\n\n        Returns\n        -------\n        TreeKernel\n            The TreeKernel instance.\n        \"\"\"\n\n        kernel = nnx.merge(self.graphdef, kernel_state)\n\n        max_depth = self.max_depth\n        max_nodes = calculate_max_nodes(max_depth)\n        max_leaves = calculate_max_leaves(max_depth)\n        max_stack = calculate_max_stack_size(max_depth)\n        kernel.max_depth = kernel.max_depth.replace(max_depth)\n        kernel.max_nodes = kernel.max_nodes.replace(max_nodes)\n        kernel.max_leaves = kernel.max_leaves.replace(max_leaves)\n        kernel.max_stack = kernel.max_stack.replace(max_stack)\n        kernel.num_atoms = kernel.num_atoms.replace(self.kernel_library.num_atoms)\n        kernel.num_datapoints = kernel.num_datapoints.replace(self.num_datapoints)\n        return kernel\n\n    def _get_graphdef(\n        self,\n    ) -&gt; nnx.GraphDef:\n        \"\"\"\n        Create a random kernel and return the graphdef.\n\n        This function is used to create a graphdef for the\n        kernel (which is set as attribute in the __init__).\n        The graphdef should be the same for all possible\n        kernels, so we can reuse it whenever we need to\n        create a KernelTree object from a kernel state.\n\n        Returns\n        -------\n        nnx.GraphDef\n            The graph definition of the kernel.\n        \"\"\"\n\n        key = jr.PRNGKey(42)\n\n        kernel_structure = self.kernel_structure_prior.sample(\n            key,\n            self.kernel_structure_prior.max_depth,\n        )\n\n        kernel = TreeKernel(\n            kernel_structure,\n            self.kernel_library,\n            self.max_depth,\n            self.num_datapoints,\n        )\n\n        graphdef, _ = nnx.split(kernel)\n        return graphdef\n\n    @property\n    def atoms(self) -&gt; tp.List[AbstractAtom]:\n        return self.kernel_library.atoms\n\n    @property\n    def operators(self) -&gt; tp.List[AbstractOperator]:\n        return self.kernel_library.operators\n\n    @property\n    def is_operator(self) -&gt; Bool[jnp.ndarray, \" D\"]:\n        return self.kernel_library.is_operator\n\n    @property\n    def probs(self) -&gt; Float[jnp.ndarray, \" D\"]:\n        return self.kernel_structure_prior.probs\n\n    @property\n    def max_depth(self) -&gt; int:\n        return self.kernel_structure_prior.max_depth\n\n    @property\n    def prior_transforms(self) -&gt; FrozenDict[str, tfb.Bijector]:\n        return self.kernel_library.prior_transforms\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.KernelPrior.__init__","title":"<code>__init__(kernel_library, max_depth, num_datapoints, probs=None)</code>","text":"<p>Initialize the KernelPrior class.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_library</code> <code>KernelLibrary</code> <p>An instance of the KernelLibrary class, containing the kernel classes and operators, and transformation functions. The TreeStructurePrior and ParameterPrior classes are initialized using the kernel_library.</p> required <code>max_depth</code> <code>int</code> <p>The maximum depth of the nested kernel structure tree.</p> required <code>num_datapoints</code> <code>int</code> <p>The number of data points in the dataset.</p> required <code>probs</code> <code>Optional[Float[ndarray, ' D']]</code> <p>The probabilities of sampling each kernel (or operator) in the library. The array must have the same length as the the arrays in the kernel library. By default None, which will use a uniform distribution.</p> <code>None</code> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def __init__(\n    self,\n    kernel_library: KernelLibrary,\n    max_depth: int,\n    num_datapoints: int,\n    probs: tp.Optional[Float[jnp.ndarray, \" D\"]] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the KernelPrior class.\n\n    Parameters\n    ----------\n    kernel_library : KernelLibrary\n        An instance of the KernelLibrary class, containing\n        the kernel classes and operators, and transformation functions.\n        The TreeStructurePrior and ParameterPrior classes are initialized\n        using the kernel_library.\n    max_depth : int\n        The maximum depth of the nested kernel structure tree.\n    num_datapoints : int\n        The number of data points in the dataset.\n    probs : tp.Optional[ Float[jnp.ndarray, \" D\"]], optional\n        The probabilities of sampling each kernel (or operator) in the library.\n        The array must have the same length as the the arrays in the kernel library.\n        By default None, which will use a uniform distribution.\n    \"\"\"\n    self.kernel_library = kernel_library\n\n    self.kernel_structure_prior = TreeStructurePrior(\n        self.kernel_library,\n        max_depth,\n        probs,\n    )\n\n    self.num_datapoints = num_datapoints\n\n    self.parameter_prior = ParameterPrior(self.kernel_library, max_depth)\n\n    self.graphdef = self._get_graphdef()\n\n    self.max_kernel_parameter = (\n        kernel_library.max_atom_parameters * calculate_max_leaves(self.max_depth)\n    )\n\n    self.support_bijectors = tuple(\n        [\n            self.kernel_library.support_transforms[tag]\n            for tag in self.kernel_library.support_tag_mapping\n        ]\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.KernelPrior.reconstruct_kernel","title":"<code>reconstruct_kernel(kernel_state)</code>","text":"<p>Create new TreeKernel from kernel state using the graphdef, and reset some kernel attributes. In principle, this is unnecessary since the values are already set in the kernel state, but we need to make it explicit for the jit compilation.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_state</code> <code>State</code> <p>The kernel state to be used to create the kernel.</p> required <p>Returns:</p> Type Description <code>TreeKernel</code> <p>The TreeKernel instance.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def reconstruct_kernel(\n    self,\n    kernel_state: nnx.State,\n) -&gt; TreeKernel:\n    \"\"\"\n    Create new TreeKernel from kernel state using the graphdef,\n    and reset some kernel attributes. In principle,\n    this is unnecessary since the values are already\n    set in the kernel state, but we need to make it\n    explicit for the jit compilation.\n\n    Parameters\n    ----------\n    kernel_state : nnx.State\n        The kernel state to be used to create the kernel.\n\n    Returns\n    -------\n    TreeKernel\n        The TreeKernel instance.\n    \"\"\"\n\n    kernel = nnx.merge(self.graphdef, kernel_state)\n\n    max_depth = self.max_depth\n    max_nodes = calculate_max_nodes(max_depth)\n    max_leaves = calculate_max_leaves(max_depth)\n    max_stack = calculate_max_stack_size(max_depth)\n    kernel.max_depth = kernel.max_depth.replace(max_depth)\n    kernel.max_nodes = kernel.max_nodes.replace(max_nodes)\n    kernel.max_leaves = kernel.max_leaves.replace(max_leaves)\n    kernel.max_stack = kernel.max_stack.replace(max_stack)\n    kernel.num_atoms = kernel.num_atoms.replace(self.kernel_library.num_atoms)\n    kernel.num_datapoints = kernel.num_datapoints.replace(self.num_datapoints)\n    return kernel\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.KernelPrior.sample","title":"<code>sample(key)</code>","text":"<p>Sample a kernel structure and its parameters from the prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for sampling.</p> required <p>Returns:</p> Type Description <code>State</code> <code>The state of the sampled kernel, combine with the graphdef</code> <code>to create a TreeKernel object.</code> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def sample(self, key: PRNGKeyArray) -&gt; nnx.State:\n    \"\"\"\n    Sample a kernel structure and its parameters from the prior distribution.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for sampling.\n\n    Returns\n    -------\n    nnx.State\n    The state of the sampled kernel, combine with the graphdef\n    to create a TreeKernel object.\n\n    \"\"\"\n    tree_key, parameter_key = jr.split(key)\n    kernel_structure = self.kernel_structure_prior.sample(\n        tree_key,\n        self.kernel_structure_prior.max_depth,\n    )\n\n    kernel = TreeKernel(\n        kernel_structure,\n        self.kernel_library,\n        self.max_depth,\n        self.num_datapoints,\n    )\n\n    _, state = nnx.split(kernel)\n\n    kernel_state, _ = self.parameter_prior.sample(\n        parameter_key,\n        nnx.State(state),\n    )\n    return kernel_state\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.KernelPrior.sample_kernel","title":"<code>sample_kernel(key)</code>","text":"<p>Sample a kernel state using self.sample and merge it with the graphdef and static_state to create a TreeKernel.</p> <p>This function is convenient for sampling a kernel directly, but not vmap or jittable (at least not using the jax commands, potentially using the nnx ones.)</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for sampling.</p> required <p>Returns:</p> Type Description <code>TreeKernel</code> <p>The sampled kernel.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def sample_kernel(self, key: PRNGKeyArray) -&gt; TreeKernel:\n    \"\"\"\n    Sample a kernel state using self.sample and merge\n    it with the graphdef and static_state to create a\n    TreeKernel.\n\n    This function is convenient for sampling a kernel\n    directly, but not vmap or jittable (at least not\n    using the jax commands, potentially using the\n    nnx ones.)\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for sampling.\n\n    Returns\n    -------\n    TreeKernel\n        The sampled kernel.\n    \"\"\"\n\n    kernel_state = self.sample(key)\n    return nnx.merge(self.graphdef, kernel_state)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.ParameterPrior","title":"<code>ParameterPrior</code>","text":"<p>This class defines a prior distribution over kernel parameters.</p> <p>Attributes:</p> Name Type Description <code>num_parameter_array</code> <code>Int[ndarray, ' D']</code> <p>An array that contains the number of parameters for each atom in the kernel structure. Needs to be in same order as the atom library.</p> <code>max_atom_parameters</code> <code>int</code> <p>The maximum number of parameters for an atom in the kernel structure.</p> <code>max_leaves</code> <code>int</code> <p>The maximum number of leaves in the tree.</p> <code>max_nodes</code> <code>int</code> <p>The maximum number of nodes in the kernel structure.</p> <code>support_mapping_array</code> <code>Int[ndarray, ' D']</code> <p>An array that maps the support of the parameters to the corresponding bijector index.</p> <code>forward_bijectors</code> <code>tuple[Callable]</code> <p>A tuple of bijectors to transform the sampled parameters from a standard normal to the desired prior distribution.</p> <code>inverse_bijectors</code> <code>tuple[Callable]</code> <p>A tuple of bijectors to transform the sampled parameters from the prior distribution back to a standard normal.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>class ParameterPrior:\n    \"\"\"\n    This class defines a prior distribution over kernel parameters.\n\n    Attributes\n    ----------\n    num_parameter_array : Int[jnp.ndarray, \" D\"]\n        An array that contains the number of parameters for each\n        atom in the kernel structure. Needs to be in same order\n        as the atom library.\n    max_atom_parameters : int\n        The maximum number of parameters for an atom in the kernel structure.\n    max_leaves : int\n        The maximum number of leaves in the tree.\n    max_nodes : int\n        The maximum number of nodes in the kernel structure.\n    support_mapping_array : Int[jnp.ndarray, \" D\"]\n        An array that maps the support of the parameters to the\n        corresponding bijector index.\n    forward_bijectors : tuple[tfb.Callable]\n        A tuple of bijectors to transform the sampled parameters from\n        a standard normal to the desired prior distribution.\n    inverse_bijectors : tuple[tfb.Callable]\n        A tuple of bijectors to transform the sampled parameters from\n        the prior distribution back to a standard normal.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel_library: KernelLibrary,\n        max_depth: int,\n    ):\n        \"\"\"\n        Initialize the ParameterPrior class.\n\n        Parameters\n        ----------\n        kernel_library : KernelLibrary\n            An instance of the KernelLibrary class, containing\n            the kernel classes and operators, and transformation functions.\n            The prior_transforms dictionary is used to transform the\n            sampled parameters to the desired prior distribution.\n        max_depth : int\n            The maximum depth of the kernel tree.\n\n        \"\"\"\n        self.num_parameter_array = jnp.array(\n            [atom.num_parameter for atom in kernel_library.library]\n        )\n\n        self.max_atom_parameters = kernel_library.max_atom_parameters\n        self.max_leaves = int(calculate_max_leaves(max_depth))\n        self.max_nodes = int(calculate_max_nodes(max_depth))\n\n        self.support_mapping_array = kernel_library.support_mapping_array\n\n        # create tuples of callables for forward and inverse bijectors\n        self.forward_bijectors = tuple(\n            [\n                kernel_library.prior_transforms[tag].forward  # type: ignore\n                for tag in kernel_library.support_tag_mapping\n            ]\n        )\n\n        self.inverse_bijectors = tuple(\n            [\n                kernel_library.prior_transforms[tag].inverse  # type: ignore\n                for tag in kernel_library.support_tag_mapping\n            ]\n        )\n\n    def sample(\n        self,\n        key: PRNGKeyArray,\n        state: nnx.State,\n    ) -&gt; tuple[nnx.State, ScalarFloat]:\n        \"\"\"\n        Sample kernel parameter and assign it to the kernel. Also\n        returns the log probability of the sampled parameters.\n\n\n        The kernel parameter are sampled from a standard normal and\n        transformed to follow their corresponding prior distributions\n        defined by the parameter_transforms dictionary.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for sampling.\n        state : nnx.State\n            The original kernel state to be filled with new parameters.\n\n        Returns\n        -------\n        nnx.State\n            The state with the sampled parameters.\n        ScalarFloat\n            The log probability of the sampled parameters.\n\n        \"\"\"\n        return self.sample_subset(\n            key,\n            state,\n            jnp.arange(self.max_nodes),  # all nodes are considered\n        )\n\n    def sample_subset(\n        self,\n        key: PRNGKeyArray,\n        state: nnx.State,\n        considered_nodes: Int[jnp.ndarray, \" D\"],\n    ) -&gt; tuple[nnx.State, ScalarFloat]:\n        \"\"\"\n        Sample kernel parameter and assign it to the kernel.\n        Same as 'sample' method but with additional\n        parameter 'considered_nodes', which can be used\n        to only sample parameters for a subset of nodes.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for sampling.\n        state : nnx.State\n            The original kernel state to be filled with new parameters.\n        considered_nodes : Int[jnp.ndarray, \" D\"]\n            An array that contains the indices of the nodes that\n            are supposed to be sampled. (Padded with -1 if\n            necessary.)\n\n        Returns\n        -------\n        nnx.State\n            The state with the sampled parameters.\n        ScalarFloat\n            The log probability of the sampled parameters.\n        \"\"\"\n\n        new_state, log_prob = sample_parameters(\n            key,\n            state,\n            considered_nodes,\n            self.num_parameter_array,\n            self.max_leaves,\n            self.max_atom_parameters,\n            self.support_mapping_array,\n            self.forward_bijectors,\n        )\n        return new_state, log_prob\n\n    def log_prob(\n        self,\n        state: nnx.State,\n    ) -&gt; ScalarFloat:\n        \"\"\"\n        Compute the log probability of the kernel parameters.\n\n        Parameters\n        ----------\n        state : nnx.State\n            The kernel state with the parameters.\n\n        Returns\n        -------\n        ScalarFloat\n            The log probability of the kernel parameters.\n        \"\"\"\n\n        return self.log_prob_subset(\n            state,\n            jnp.arange(self.max_nodes),  # all nodes are considered\n        )\n\n    def log_prob_subset(\n        self,\n        state: nnx.State,\n        considered_nodes: Int[jnp.ndarray, \" D\"],\n    ) -&gt; ScalarFloat:\n        \"\"\"\n        Compute the log probability of the kernel parameters.\n        Same as 'log_prob' method but with additional\n        parameter 'considered_nodes', which can be used\n        to only calculate the log probability for a subset of nodes.\n\n        Parameters\n        ----------\n        state : nnx.State\n            The kernel state with the parameters.\n        considered_nodes : Int[jnp.ndarray, \" D\"]\n            An array that contains the indices of the nodes that\n            are supposed to be sampled. (Padded with -1 if\n            necessary.)\n\n        Returns\n        -------\n        ScalarFloat\n            The log probability of the kernel parameters.\n        \"\"\"\n\n        return log_prob_parameters(\n            state,\n            considered_nodes,\n            self.num_parameter_array,\n            self.max_leaves,\n            self.max_atom_parameters,\n            self.support_mapping_array,\n            self.inverse_bijectors,\n        )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.ParameterPrior.__init__","title":"<code>__init__(kernel_library, max_depth)</code>","text":"<p>Initialize the ParameterPrior class.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_library</code> <code>KernelLibrary</code> <p>An instance of the KernelLibrary class, containing the kernel classes and operators, and transformation functions. The prior_transforms dictionary is used to transform the sampled parameters to the desired prior distribution.</p> required <code>max_depth</code> <code>int</code> <p>The maximum depth of the kernel tree.</p> required Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def __init__(\n    self,\n    kernel_library: KernelLibrary,\n    max_depth: int,\n):\n    \"\"\"\n    Initialize the ParameterPrior class.\n\n    Parameters\n    ----------\n    kernel_library : KernelLibrary\n        An instance of the KernelLibrary class, containing\n        the kernel classes and operators, and transformation functions.\n        The prior_transforms dictionary is used to transform the\n        sampled parameters to the desired prior distribution.\n    max_depth : int\n        The maximum depth of the kernel tree.\n\n    \"\"\"\n    self.num_parameter_array = jnp.array(\n        [atom.num_parameter for atom in kernel_library.library]\n    )\n\n    self.max_atom_parameters = kernel_library.max_atom_parameters\n    self.max_leaves = int(calculate_max_leaves(max_depth))\n    self.max_nodes = int(calculate_max_nodes(max_depth))\n\n    self.support_mapping_array = kernel_library.support_mapping_array\n\n    # create tuples of callables for forward and inverse bijectors\n    self.forward_bijectors = tuple(\n        [\n            kernel_library.prior_transforms[tag].forward  # type: ignore\n            for tag in kernel_library.support_tag_mapping\n        ]\n    )\n\n    self.inverse_bijectors = tuple(\n        [\n            kernel_library.prior_transforms[tag].inverse  # type: ignore\n            for tag in kernel_library.support_tag_mapping\n        ]\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.ParameterPrior.log_prob","title":"<code>log_prob(state)</code>","text":"<p>Compute the log probability of the kernel parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The kernel state with the parameters.</p> required <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The log probability of the kernel parameters.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def log_prob(\n    self,\n    state: nnx.State,\n) -&gt; ScalarFloat:\n    \"\"\"\n    Compute the log probability of the kernel parameters.\n\n    Parameters\n    ----------\n    state : nnx.State\n        The kernel state with the parameters.\n\n    Returns\n    -------\n    ScalarFloat\n        The log probability of the kernel parameters.\n    \"\"\"\n\n    return self.log_prob_subset(\n        state,\n        jnp.arange(self.max_nodes),  # all nodes are considered\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.ParameterPrior.log_prob_subset","title":"<code>log_prob_subset(state, considered_nodes)</code>","text":"<p>Compute the log probability of the kernel parameters. Same as 'log_prob' method but with additional parameter 'considered_nodes', which can be used to only calculate the log probability for a subset of nodes.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The kernel state with the parameters.</p> required <code>considered_nodes</code> <code>Int[ndarray, ' D']</code> <p>An array that contains the indices of the nodes that are supposed to be sampled. (Padded with -1 if necessary.)</p> required <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The log probability of the kernel parameters.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def log_prob_subset(\n    self,\n    state: nnx.State,\n    considered_nodes: Int[jnp.ndarray, \" D\"],\n) -&gt; ScalarFloat:\n    \"\"\"\n    Compute the log probability of the kernel parameters.\n    Same as 'log_prob' method but with additional\n    parameter 'considered_nodes', which can be used\n    to only calculate the log probability for a subset of nodes.\n\n    Parameters\n    ----------\n    state : nnx.State\n        The kernel state with the parameters.\n    considered_nodes : Int[jnp.ndarray, \" D\"]\n        An array that contains the indices of the nodes that\n        are supposed to be sampled. (Padded with -1 if\n        necessary.)\n\n    Returns\n    -------\n    ScalarFloat\n        The log probability of the kernel parameters.\n    \"\"\"\n\n    return log_prob_parameters(\n        state,\n        considered_nodes,\n        self.num_parameter_array,\n        self.max_leaves,\n        self.max_atom_parameters,\n        self.support_mapping_array,\n        self.inverse_bijectors,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.ParameterPrior.sample","title":"<code>sample(key, state)</code>","text":"<p>Sample kernel parameter and assign it to the kernel. Also returns the log probability of the sampled parameters.</p> <p>The kernel parameter are sampled from a standard normal and transformed to follow their corresponding prior distributions defined by the parameter_transforms dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for sampling.</p> required <code>state</code> <code>State</code> <p>The original kernel state to be filled with new parameters.</p> required <p>Returns:</p> Type Description <code>State</code> <p>The state with the sampled parameters.</p> <code>ScalarFloat</code> <p>The log probability of the sampled parameters.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def sample(\n    self,\n    key: PRNGKeyArray,\n    state: nnx.State,\n) -&gt; tuple[nnx.State, ScalarFloat]:\n    \"\"\"\n    Sample kernel parameter and assign it to the kernel. Also\n    returns the log probability of the sampled parameters.\n\n\n    The kernel parameter are sampled from a standard normal and\n    transformed to follow their corresponding prior distributions\n    defined by the parameter_transforms dictionary.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for sampling.\n    state : nnx.State\n        The original kernel state to be filled with new parameters.\n\n    Returns\n    -------\n    nnx.State\n        The state with the sampled parameters.\n    ScalarFloat\n        The log probability of the sampled parameters.\n\n    \"\"\"\n    return self.sample_subset(\n        key,\n        state,\n        jnp.arange(self.max_nodes),  # all nodes are considered\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.ParameterPrior.sample_subset","title":"<code>sample_subset(key, state, considered_nodes)</code>","text":"<p>Sample kernel parameter and assign it to the kernel. Same as 'sample' method but with additional parameter 'considered_nodes', which can be used to only sample parameters for a subset of nodes.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for sampling.</p> required <code>state</code> <code>State</code> <p>The original kernel state to be filled with new parameters.</p> required <code>considered_nodes</code> <code>Int[ndarray, ' D']</code> <p>An array that contains the indices of the nodes that are supposed to be sampled. (Padded with -1 if necessary.)</p> required <p>Returns:</p> Type Description <code>State</code> <p>The state with the sampled parameters.</p> <code>ScalarFloat</code> <p>The log probability of the sampled parameters.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def sample_subset(\n    self,\n    key: PRNGKeyArray,\n    state: nnx.State,\n    considered_nodes: Int[jnp.ndarray, \" D\"],\n) -&gt; tuple[nnx.State, ScalarFloat]:\n    \"\"\"\n    Sample kernel parameter and assign it to the kernel.\n    Same as 'sample' method but with additional\n    parameter 'considered_nodes', which can be used\n    to only sample parameters for a subset of nodes.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for sampling.\n    state : nnx.State\n        The original kernel state to be filled with new parameters.\n    considered_nodes : Int[jnp.ndarray, \" D\"]\n        An array that contains the indices of the nodes that\n        are supposed to be sampled. (Padded with -1 if\n        necessary.)\n\n    Returns\n    -------\n    nnx.State\n        The state with the sampled parameters.\n    ScalarFloat\n        The log probability of the sampled parameters.\n    \"\"\"\n\n    new_state, log_prob = sample_parameters(\n        key,\n        state,\n        considered_nodes,\n        self.num_parameter_array,\n        self.max_leaves,\n        self.max_atom_parameters,\n        self.support_mapping_array,\n        self.forward_bijectors,\n    )\n    return new_state, log_prob\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.TreeStructurePrior","title":"<code>TreeStructurePrior</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A prior distribution over kernel structures.</p> <p>The TreeStructurePrior is a distribution over kernel structures. The kernel structure is represented as a tree, where each leaf corresponds to a kernel. The tree is constructed by sampling from a library of kernel classes and operators. The operators are used to construct nested kernel structures.</p> <p>Attributes:</p> Name Type Description <code>library</code> <code>List[Type[AbstractKernel] | Callable]</code> <p>A list of kernel classes and operators, as defined in the KernelLibrary class. See gallifrey.kernels.library.KernelLibrary for more details.</p> <code>is_operator</code> <code>List[bool]</code> <p>A list of booleans indicating whether each element in the library is an operator.</p> <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the nested kernel structure tree.</p> <code>probs</code> <code>ndarray</code> <p>The probabilities of sampling each kernel (or operator) in the library. See kernels.library.KernelLibrary and gallifrey.config.GPConfig for more details.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>class TreeStructurePrior(tfd.Distribution):\n    \"\"\"\n    A prior distribution over kernel structures.\n\n    The TreeStructurePrior is a distribution over kernel structures. The kernel\n    structure is represented as a tree, where each leaf corresponds to a kernel. The\n    tree is constructed by sampling from a library of kernel classes and operators.\n    The operators are used to construct nested kernel structures.\n\n    Attributes\n    ----------\n    library : tp.List[tp.Type[AbstractKernel] | tp.Callable]\n        A list of kernel classes and operators, as defined in the KernelLibrary class.\n        See gallifrey.kernels.library.KernelLibrary for more details.\n    is_operator : tp.List[bool]\n        A list of booleans indicating whether each element in the library is an\n        operator.\n    max_depth : ScalarInt\n        The maximum depth of the nested kernel structure tree.\n    probs : jnp.ndarray\n        The probabilities of sampling each kernel (or operator) in the library.\n        See kernels.library.KernelLibrary and gallifrey.config.GPConfig for more\n        details.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel_library: KernelLibrary,\n        max_depth: int,\n        probs: tp.Optional[Float[jnp.ndarray, \" D\"]] = None,\n        *,\n        validate_args: tp.Optional[bool] = None,\n    ):\n        \"\"\"\n        Initialize the KernelPrior distribution.\n\n        Parameters\n        ----------\n        kernel_library : KernelLibrary\n            An instance of the KernelLibrary class, containing the kernel classes and\n            operators. (See gallifrey.kernels.library.KernelLibrary)\n        max_depth : int\n            The maximum depth of the nested kernel structure tree.\n        probs :  Float[jnp.ndarray, \" D\"], optional\n            The probabilities of sampling each kernel (or operator) in the library.\n            The array must have the same length as the the arrays in the kernel library.\n            By default None, which will use a uniform distribution.\n        validate_args : tp.Optional[bool], optional\n            Whether to validate input, by default None. NOT IMPLEMENTED.\n\n        Raises\n        ------\n        ValueError\n            If the probs are not one-dimensional.\n        ValueError\n            If the probs do not have the same length as the kernel_library\n            along the last dimension.\n        \"\"\"\n        # initialize the kernel functions and the probabilities\n        self.library = kernel_library.library\n        self.is_operator = kernel_library.is_operator\n        self.max_depth = max_depth\n\n        self.probs = probs if probs is not None else jnp.ones(len(self.library))\n        if jnp.ndim(self.probs) != 1:\n            raise ValueError(\"'probs' must be one-dimensional.\")\n        if not len(self.library) == len(self.probs):\n            raise ValueError(\n                \"'probs' must have same length along the last \"\n                \"dimension as 'kernel_library'. \"\n                f\"Got 'probs' shape: {jnp.shape(self.probs)}, \"\n                f\"'kernel_library' length: {len(self.library)}.\"\n            )\n\n    def sample_single(\n        self,\n        key: PRNGKeyArray,\n        max_depth: tp.Optional[ScalarInt] = None,\n        root_idx: ScalarInt = 0,\n    ) -&gt; Int[jnp.ndarray, \" D\"]:\n        \"\"\"\n        Construct an abstract representation of a kernel structure by sampling from\n        the kernel library.\n        The kernels are sampled from the library according to the defined\n        probabilities.\n        The structure is represented as a one-dimensional array, according to\n        the level-order traversal of the tree. This means:\n        - The root node is at position 0.\n        - The left child of a node at position i is at position 2*i + 1.\n        - The right child of a node at position i is at position 2*i + 2.\n        - Empty nodes are labeled -1.\n        See gallifrey.kernels.tree.TreeKernel for more details and an example.\n\n        Parameters\n        ----------\n        key : PRNGKeyArray\n            Random key for sampling.\n        max_depth : ScalarInt, optional\n            The maximum depth of the nested kernel structure tree, by default None.\n            If None, the maximum depth is set to the value of self.max_depth.\n        root_idx : ScalarInt, optional\n            The index of the root node in the tree, by default 0. Used\n            for sub-trees.\n\n        Returns\n        -------\n        Int[jnp.ndarray, \" D\"]\n            An array that describes the kernel structure.\n        \"\"\"\n        max_depth = max_depth if max_depth is not None else self.max_depth\n\n        if max_depth &lt; 0:\n            raise ValueError(\"'max_depth' must be 0 or larger.\")\n\n        max_nodes = calculate_max_nodes(max_depth)\n\n        # create sample array to be filled, this will be the output (empty\n        # nodes are labeled -1)\n        sample = jnp.full(max_nodes, -1)\n        # create initial stack: empty except for the root node\n        initial_stack = jnp.copy(sample).at[0].set(root_idx)\n\n        pointer = 0  # initial position of the stack pointer\n\n        initial_state = (key, sample, initial_stack, pointer)\n\n        return _sample_single(\n            initial_state,\n            self.probs,\n            self.is_operator,\n            max_depth,\n        )\n\n    def log_prob_single(\n        self,\n        value: Int[jnp.ndarray, \" D\"],\n        root_idx: ScalarInt = 0,\n        path_to_hole: tp.Optional[Int[jnp.ndarray, \" D\"]] = None,\n        hole_idx: tp.Optional[ScalarInt] = None,\n    ) -&gt; ScalarFloat:\n        \"\"\"\n        Compute the log probability of a given kernel structure,\n        as represented by the level-order tree array.\n        The maximum depth of the tree is inferred from the length of the array.\n\n        The function can also be used to calculate the log probability of a\n        scaffold structure (used by the detach-attach move), by providing\n        the path to the hole and the hole index.\n\n        Parameters\n        ----------\n        value :  Int[jnp.ndarray, \" D\"]\n            An array that describes the kernel structure.\n        root_idx : ScalarInt, optional\n            The index of the root node in the tree, by default 0.\n        path_to_hole :  Int[jnp.ndarray, \" D\"], optional\n            The path to the hole in the tree. A list of indices that\n            describe the path from the root to the hole (level order\n            indices), by default None (sets it to an empty array).\n        hole_idx : ScalarInt, optional\n            The index of the hole in the tree (level order index),\n            by default None(sets it to -1 which should never be\n            reached).\n\n        Returns\n        -------\n        ScalarFloat\n            The log probability of the given kernel structure or scaffold.\n        \"\"\"\n        max_nodes = len(value)\n        max_depth = calculate_max_depth(max_nodes)\n\n        initial_log_p = 0.0\n        inital_stack = jnp.full(max_nodes, -1).at[0].set(root_idx)\n        pointer = 0\n        initial_state = (initial_log_p, inital_stack, pointer)\n\n        # set the path to the hole and the hole index, if not given\n        path_to_hole = path_to_hole if path_to_hole is not None else jnp.array([])\n        hole_idx = hole_idx if hole_idx is not None else -1\n\n        return _log_prob_single(\n            initial_state,\n            value,\n            self.probs,\n            self.is_operator,\n            max_depth,\n            path_to_hole,\n            hole_idx,\n        )\n\n    def sample(\n        self,\n        key: PRNGKeyArray,\n        max_depth: tp.Optional[ScalarInt] = None,\n        root_idx: ScalarInt = 0,\n        sample_shape: tuple = (),\n    ) -&gt; Int[jnp.ndarray, \"...\"]:\n        \"\"\"\n        Sample kernel structures. For details, see `sample_single`.\n\n        Parameters\n        ----------\n        key : PRNGKey\n            Random key for sampling.\n        max_depth : ScalarInt\n            The maximum depth of the nested kernel tree structure. If None,\n            the maximum depth is set to the value of self.max_depth.\n        root_idx : ScalarInt, optional\n            The index of the root node in the tree, by default 0.\n            Used for sub-trees.\n        sample_shape : tuple, optional\n            The sample shape for the distribution, by default ().\n\n        Returns\n        -------\n        Int[jnp.ndarray, \"...\"]\n            An array of samples describing kernel structures,\n            of shape sample_shape + (max_nodes,).\n        \"\"\"\n        max_depth = max_depth if max_depth is not None else self.max_depth\n\n        if not sample_shape:\n            return self.sample_single(key, max_depth, root_idx)\n\n        max_nodes = calculate_max_nodes(max_depth)\n\n        # flatten the sample_shape for vectorized sampling\n        num_samples = jnp.prod(jnp.array(sample_shape))\n        keys = jr.split(key, num=int(num_samples))\n\n        samples = vmap(self.sample_single, in_axes=(0, None, None))(\n            keys,\n            max_depth,\n            root_idx,\n        )\n        return samples.reshape(sample_shape + (max_nodes,))\n\n    def log_prob(\n        self,\n        value: Int[jnp.ndarray, \"...\"],\n        root_idx: ScalarInt = 0,\n        path_to_hole: tp.Optional[Int[jnp.ndarray, \"...\"]] = None,\n        hole_idx: tp.Optional[ScalarInt] = None,\n    ) -&gt; Float[jnp.ndarray, \"...\"]:\n        \"\"\"\n        Compute the log probability of given kernel structures or scaffolds.\n        (See log_prob_single for details.)\n\n        Parameters\n        ----------\n        value :  Int[jnp.ndarray, \"...\"]\n            Expression of kernel structure(s).\n        root_idx : ScalarInt, optional\n            The index of the root node in the tree, by default 0.\n        path_to_hole :  Int[jnp.ndarray, \"...\"], optional\n            Optional path to the hole in the tree, if calculating the log\n            probability of a scaffold, by default None.\n        hole_idx : ScalarInt, optional\n            The index of the hole in the tree if calculating the log\n            probability of a scaffold, by default None.\n\n        Returns\n        -------\n         Float[jnp.ndarray, \"...\"]\n            The log probability of the given kernel structures.\n        \"\"\"\n        sample_shape = jnp.shape(value)[:-1]\n\n        num_samples = jnp.prod(jnp.array(sample_shape))\n        log_probs = vmap(self.log_prob_single, in_axes=(0, None, None, None))(\n            value.reshape(int(num_samples), -1),\n            root_idx,\n            path_to_hole,\n            hole_idx,\n        )\n        return jnp.asarray(log_probs).reshape(sample_shape)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.TreeStructurePrior.__init__","title":"<code>__init__(kernel_library, max_depth, probs=None, *, validate_args=None)</code>","text":"<p>Initialize the KernelPrior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_library</code> <code>KernelLibrary</code> <p>An instance of the KernelLibrary class, containing the kernel classes and operators. (See gallifrey.kernels.library.KernelLibrary)</p> required <code>max_depth</code> <code>int</code> <p>The maximum depth of the nested kernel structure tree.</p> required <code>probs</code> <code> Float[jnp.ndarray, \" D\"]</code> <p>The probabilities of sampling each kernel (or operator) in the library. The array must have the same length as the the arrays in the kernel library. By default None, which will use a uniform distribution.</p> <code>None</code> <code>validate_args</code> <code>Optional[bool]</code> <p>Whether to validate input, by default None. NOT IMPLEMENTED.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the probs are not one-dimensional.</p> <code>ValueError</code> <p>If the probs do not have the same length as the kernel_library along the last dimension.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def __init__(\n    self,\n    kernel_library: KernelLibrary,\n    max_depth: int,\n    probs: tp.Optional[Float[jnp.ndarray, \" D\"]] = None,\n    *,\n    validate_args: tp.Optional[bool] = None,\n):\n    \"\"\"\n    Initialize the KernelPrior distribution.\n\n    Parameters\n    ----------\n    kernel_library : KernelLibrary\n        An instance of the KernelLibrary class, containing the kernel classes and\n        operators. (See gallifrey.kernels.library.KernelLibrary)\n    max_depth : int\n        The maximum depth of the nested kernel structure tree.\n    probs :  Float[jnp.ndarray, \" D\"], optional\n        The probabilities of sampling each kernel (or operator) in the library.\n        The array must have the same length as the the arrays in the kernel library.\n        By default None, which will use a uniform distribution.\n    validate_args : tp.Optional[bool], optional\n        Whether to validate input, by default None. NOT IMPLEMENTED.\n\n    Raises\n    ------\n    ValueError\n        If the probs are not one-dimensional.\n    ValueError\n        If the probs do not have the same length as the kernel_library\n        along the last dimension.\n    \"\"\"\n    # initialize the kernel functions and the probabilities\n    self.library = kernel_library.library\n    self.is_operator = kernel_library.is_operator\n    self.max_depth = max_depth\n\n    self.probs = probs if probs is not None else jnp.ones(len(self.library))\n    if jnp.ndim(self.probs) != 1:\n        raise ValueError(\"'probs' must be one-dimensional.\")\n    if not len(self.library) == len(self.probs):\n        raise ValueError(\n            \"'probs' must have same length along the last \"\n            \"dimension as 'kernel_library'. \"\n            f\"Got 'probs' shape: {jnp.shape(self.probs)}, \"\n            f\"'kernel_library' length: {len(self.library)}.\"\n        )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.TreeStructurePrior.log_prob","title":"<code>log_prob(value, root_idx=0, path_to_hole=None, hole_idx=None)</code>","text":"<p>Compute the log probability of given kernel structures or scaffolds. (See log_prob_single for details.)</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code> Int[jnp.ndarray, \"...\"]</code> <p>Expression of kernel structure(s).</p> required <code>root_idx</code> <code>ScalarInt</code> <p>The index of the root node in the tree, by default 0.</p> <code>0</code> <code>path_to_hole</code> <code> Int[jnp.ndarray, \"...\"]</code> <p>Optional path to the hole in the tree, if calculating the log probability of a scaffold, by default None.</p> <code>None</code> <code>hole_idx</code> <code>ScalarInt</code> <p>The index of the hole in the tree if calculating the log probability of a scaffold, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code> Float[jnp.ndarray, \"...\"]</code> <p>The log probability of the given kernel structures.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def log_prob(\n    self,\n    value: Int[jnp.ndarray, \"...\"],\n    root_idx: ScalarInt = 0,\n    path_to_hole: tp.Optional[Int[jnp.ndarray, \"...\"]] = None,\n    hole_idx: tp.Optional[ScalarInt] = None,\n) -&gt; Float[jnp.ndarray, \"...\"]:\n    \"\"\"\n    Compute the log probability of given kernel structures or scaffolds.\n    (See log_prob_single for details.)\n\n    Parameters\n    ----------\n    value :  Int[jnp.ndarray, \"...\"]\n        Expression of kernel structure(s).\n    root_idx : ScalarInt, optional\n        The index of the root node in the tree, by default 0.\n    path_to_hole :  Int[jnp.ndarray, \"...\"], optional\n        Optional path to the hole in the tree, if calculating the log\n        probability of a scaffold, by default None.\n    hole_idx : ScalarInt, optional\n        The index of the hole in the tree if calculating the log\n        probability of a scaffold, by default None.\n\n    Returns\n    -------\n     Float[jnp.ndarray, \"...\"]\n        The log probability of the given kernel structures.\n    \"\"\"\n    sample_shape = jnp.shape(value)[:-1]\n\n    num_samples = jnp.prod(jnp.array(sample_shape))\n    log_probs = vmap(self.log_prob_single, in_axes=(0, None, None, None))(\n        value.reshape(int(num_samples), -1),\n        root_idx,\n        path_to_hole,\n        hole_idx,\n    )\n    return jnp.asarray(log_probs).reshape(sample_shape)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.TreeStructurePrior.log_prob_single","title":"<code>log_prob_single(value, root_idx=0, path_to_hole=None, hole_idx=None)</code>","text":"<p>Compute the log probability of a given kernel structure, as represented by the level-order tree array. The maximum depth of the tree is inferred from the length of the array.</p> <p>The function can also be used to calculate the log probability of a scaffold structure (used by the detach-attach move), by providing the path to the hole and the hole index.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code> Int[jnp.ndarray, \" D\"]</code> <p>An array that describes the kernel structure.</p> required <code>root_idx</code> <code>ScalarInt</code> <p>The index of the root node in the tree, by default 0.</p> <code>0</code> <code>path_to_hole</code> <code> Int[jnp.ndarray, \" D\"]</code> <p>The path to the hole in the tree. A list of indices that describe the path from the root to the hole (level order indices), by default None (sets it to an empty array).</p> <code>None</code> <code>hole_idx</code> <code>ScalarInt</code> <p>The index of the hole in the tree (level order index), by default None(sets it to -1 which should never be reached).</p> <code>None</code> <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The log probability of the given kernel structure or scaffold.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def log_prob_single(\n    self,\n    value: Int[jnp.ndarray, \" D\"],\n    root_idx: ScalarInt = 0,\n    path_to_hole: tp.Optional[Int[jnp.ndarray, \" D\"]] = None,\n    hole_idx: tp.Optional[ScalarInt] = None,\n) -&gt; ScalarFloat:\n    \"\"\"\n    Compute the log probability of a given kernel structure,\n    as represented by the level-order tree array.\n    The maximum depth of the tree is inferred from the length of the array.\n\n    The function can also be used to calculate the log probability of a\n    scaffold structure (used by the detach-attach move), by providing\n    the path to the hole and the hole index.\n\n    Parameters\n    ----------\n    value :  Int[jnp.ndarray, \" D\"]\n        An array that describes the kernel structure.\n    root_idx : ScalarInt, optional\n        The index of the root node in the tree, by default 0.\n    path_to_hole :  Int[jnp.ndarray, \" D\"], optional\n        The path to the hole in the tree. A list of indices that\n        describe the path from the root to the hole (level order\n        indices), by default None (sets it to an empty array).\n    hole_idx : ScalarInt, optional\n        The index of the hole in the tree (level order index),\n        by default None(sets it to -1 which should never be\n        reached).\n\n    Returns\n    -------\n    ScalarFloat\n        The log probability of the given kernel structure or scaffold.\n    \"\"\"\n    max_nodes = len(value)\n    max_depth = calculate_max_depth(max_nodes)\n\n    initial_log_p = 0.0\n    inital_stack = jnp.full(max_nodes, -1).at[0].set(root_idx)\n    pointer = 0\n    initial_state = (initial_log_p, inital_stack, pointer)\n\n    # set the path to the hole and the hole index, if not given\n    path_to_hole = path_to_hole if path_to_hole is not None else jnp.array([])\n    hole_idx = hole_idx if hole_idx is not None else -1\n\n    return _log_prob_single(\n        initial_state,\n        value,\n        self.probs,\n        self.is_operator,\n        max_depth,\n        path_to_hole,\n        hole_idx,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.TreeStructurePrior.sample","title":"<code>sample(key, max_depth=None, root_idx=0, sample_shape=())</code>","text":"<p>Sample kernel structures. For details, see <code>sample_single</code>.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>Random key for sampling.</p> required <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the nested kernel tree structure. If None, the maximum depth is set to the value of self.max_depth.</p> <code>None</code> <code>root_idx</code> <code>ScalarInt</code> <p>The index of the root node in the tree, by default 0. Used for sub-trees.</p> <code>0</code> <code>sample_shape</code> <code>tuple</code> <p>The sample shape for the distribution, by default ().</p> <code>()</code> <p>Returns:</p> Type Description <code>Int[ndarray, ...]</code> <p>An array of samples describing kernel structures, of shape sample_shape + (max_nodes,).</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def sample(\n    self,\n    key: PRNGKeyArray,\n    max_depth: tp.Optional[ScalarInt] = None,\n    root_idx: ScalarInt = 0,\n    sample_shape: tuple = (),\n) -&gt; Int[jnp.ndarray, \"...\"]:\n    \"\"\"\n    Sample kernel structures. For details, see `sample_single`.\n\n    Parameters\n    ----------\n    key : PRNGKey\n        Random key for sampling.\n    max_depth : ScalarInt\n        The maximum depth of the nested kernel tree structure. If None,\n        the maximum depth is set to the value of self.max_depth.\n    root_idx : ScalarInt, optional\n        The index of the root node in the tree, by default 0.\n        Used for sub-trees.\n    sample_shape : tuple, optional\n        The sample shape for the distribution, by default ().\n\n    Returns\n    -------\n    Int[jnp.ndarray, \"...\"]\n        An array of samples describing kernel structures,\n        of shape sample_shape + (max_nodes,).\n    \"\"\"\n    max_depth = max_depth if max_depth is not None else self.max_depth\n\n    if not sample_shape:\n        return self.sample_single(key, max_depth, root_idx)\n\n    max_nodes = calculate_max_nodes(max_depth)\n\n    # flatten the sample_shape for vectorized sampling\n    num_samples = jnp.prod(jnp.array(sample_shape))\n    keys = jr.split(key, num=int(num_samples))\n\n    samples = vmap(self.sample_single, in_axes=(0, None, None))(\n        keys,\n        max_depth,\n        root_idx,\n    )\n    return samples.reshape(sample_shape + (max_nodes,))\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.TreeStructurePrior.sample_single","title":"<code>sample_single(key, max_depth=None, root_idx=0)</code>","text":"<p>Construct an abstract representation of a kernel structure by sampling from the kernel library. The kernels are sampled from the library according to the defined probabilities. The structure is represented as a one-dimensional array, according to the level-order traversal of the tree. This means: - The root node is at position 0. - The left child of a node at position i is at position 2i + 1. - The right child of a node at position i is at position 2i + 2. - Empty nodes are labeled -1. See gallifrey.kernels.tree.TreeKernel for more details and an example.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for sampling.</p> required <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the nested kernel structure tree, by default None. If None, the maximum depth is set to the value of self.max_depth.</p> <code>None</code> <code>root_idx</code> <code>ScalarInt</code> <p>The index of the root node in the tree, by default 0. Used for sub-trees.</p> <code>0</code> <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>An array that describes the kernel structure.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>def sample_single(\n    self,\n    key: PRNGKeyArray,\n    max_depth: tp.Optional[ScalarInt] = None,\n    root_idx: ScalarInt = 0,\n) -&gt; Int[jnp.ndarray, \" D\"]:\n    \"\"\"\n    Construct an abstract representation of a kernel structure by sampling from\n    the kernel library.\n    The kernels are sampled from the library according to the defined\n    probabilities.\n    The structure is represented as a one-dimensional array, according to\n    the level-order traversal of the tree. This means:\n    - The root node is at position 0.\n    - The left child of a node at position i is at position 2*i + 1.\n    - The right child of a node at position i is at position 2*i + 2.\n    - Empty nodes are labeled -1.\n    See gallifrey.kernels.tree.TreeKernel for more details and an example.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for sampling.\n    max_depth : ScalarInt, optional\n        The maximum depth of the nested kernel structure tree, by default None.\n        If None, the maximum depth is set to the value of self.max_depth.\n    root_idx : ScalarInt, optional\n        The index of the root node in the tree, by default 0. Used\n        for sub-trees.\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        An array that describes the kernel structure.\n    \"\"\"\n    max_depth = max_depth if max_depth is not None else self.max_depth\n\n    if max_depth &lt; 0:\n        raise ValueError(\"'max_depth' must be 0 or larger.\")\n\n    max_nodes = calculate_max_nodes(max_depth)\n\n    # create sample array to be filled, this will be the output (empty\n    # nodes are labeled -1)\n    sample = jnp.full(max_nodes, -1)\n    # create initial stack: empty except for the root node\n    initial_stack = jnp.copy(sample).at[0].set(root_idx)\n\n    pointer = 0  # initial position of the stack pointer\n\n    initial_state = (key, sample, initial_stack, pointer)\n\n    return _sample_single(\n        initial_state,\n        self.probs,\n        self.is_operator,\n        max_depth,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.log_prob_parameters","title":"<code>log_prob_parameters(state, considered_nodes, num_parameter_array, max_leaves, max_atom_parameters, support_mapping_array, inverse_bijectors)</code>","text":"<p>A function takes a state and calculates the log probability of the kernel parameters.</p> <p>It is assumed the parameters are sampled from a standard normal and then transformed to the desired prior distribution using the support_mapping_array and the forward_bijectors. In this function, we transform the parameters back to the standard normal and calculate the log probability.</p> <p>We include an input 'considered_nodes' to only sample a specific subset of nodes. This is useful for structure moves, where only a subset of the parameters are changed. (Since arrays need to be of fixed shape, consider_nodes can be padded with -1 or any value that's not a valid leaf index.)</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The original kernel state to be filled with new parameters. The state must contain the following attributes: - tree_expression: The tree expression that describes the kernel structure. - leaf_level_map: A array that maps the index of the leaf parameter array</p> required <code>considered_nodes</code> <code>Int[ndarray, ' D']</code> <p>This array needs to contain all the indices that are supposed to be sampled. For first time sampling, that should be all indices (or at least all leaves). For structure moves (e.g. subtree-replace move), it should contain the indices that have been changed from the previous tree.</p> required <code>num_parameter_array</code> <code>Int[ndarray, ' D']</code> <p>An array that contains the number of parameters for each atom in the kernel structure. Needs to be in same order as the atom library.</p> required <code>max_leaves</code> <code>int</code> <p>The maximum number of leaves in the tree.</p> required <code>max_atom_parameters</code> <code>int</code> <p>The maximum number of parameters any atom will take.</p> required <code>support_mapping_array</code> <code>Int[ndarray, 'M N']</code> <p>An array that maps the support of the parameters to the corresponding bijector index.</p> required <code>inverse_bijectors</code> <code>tuple[Callable, ...]</code> <p>A tuple of bijectors to transform the parameters back to a standard normal.</p> required <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The (total) log probability of the parameters, sum of all individual log probabilities.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>@partial(\n    jit,\n    static_argnames=(\n        \"max_leaves\",\n        \"max_atom_parameters\",\n        \"inverse_bijectors\",\n    ),\n)\ndef log_prob_parameters(\n    state: nnx.State,\n    considered_nodes: Int[jnp.ndarray, \" D\"],\n    num_parameter_array: Int[jnp.ndarray, \" D\"],\n    max_leaves: int,\n    max_atom_parameters: int,\n    support_mapping_array: Int[jnp.ndarray, \"M N\"],\n    inverse_bijectors: tuple[tp.Callable, ...],\n) -&gt; ScalarFloat:\n    \"\"\"\n    A function takes a state and calculates the log probability of the\n    kernel parameters.\n\n    It is assumed the parameters are sampled from a standard normal and\n    then transformed to the desired prior distribution using the\n    support_mapping_array and the forward_bijectors. In this function,\n    we transform the parameters back to the standard normal and\n    calculate the log probability.\n\n    We include an input 'considered_nodes' to only sample a specific\n    subset of nodes. This is useful for structure moves, where only\n    a subset of the parameters are changed. (Since arrays need to\n    be of fixed shape, consider_nodes can be padded with -1 or\n    any value that's not a valid leaf index.)\n\n    Parameters\n    ----------\n    state : nnx.State\n        The original kernel state to be filled with new parameters.\n        The state must contain the following attributes:\n        - tree_expression: The tree expression that describes the kernel structure.\n        - leaf_level_map: A array that maps the index of the leaf parameter array\n    considered_nodes : Int[jnp.ndarray, \" D\"]\n        This array needs to contain all the indices that are\n        supposed to be sampled. For first time sampling, that\n        should be all indices (or at least all leaves). For\n        structure moves (e.g. subtree-replace move), it should\n        contain the indices that have been changed from the\n        previous tree.\n    num_parameter_array : Int[jnp.ndarray, \" D\"]\n        An array that contains the number of parameters for each\n        atom in the kernel structure. Needs to be in same order\n        as the atom library.\n    max_leaves : int\n        The maximum number of leaves in the tree.\n    max_atom_parameters : int\n        The maximum number of parameters any atom will take.\n    support_mapping_array : Int[jnp.ndarray, \"M N\"]\n        An array that maps the support of the parameters to the\n        corresponding bijector index.\n    inverse_bijectors : tuple[tp.Callable, ...]\n        A tuple of bijectors to transform the parameters back to\n        a standard normal.\n\n    Returns\n    -------\n    ScalarFloat\n        The (total) log probability of the parameters, sum\n        of all individual log probabilities.\n\n    \"\"\"\n\n    tree_expression: jnp.ndarray = state.tree_expression.value  # type: ignore\n    leaf_level_map: jnp.ndarray = state.leaf_level_map.value  # type: ignore\n\n    def process_params(\n        state: tuple[Float[jnp.ndarray, \"M N\"], Float[jnp.ndarray, \"\"]],\n        indices: Int[jnp.ndarray, \"2\"],\n    ) -&gt; tuple[\n        tuple[Float[jnp.ndarray, \"M N\"], Float[jnp.ndarray, \"\"]],\n        Int[jnp.ndarray, \"2\"],\n    ]:\n        \"\"\"Process parameter based on if it's active and considered.\"\"\"\n\n        # unpack state and indices\n        kernel_parameters, log_probability = state\n        leaf_idx, parameter_idx = indices\n\n        # check whether the parameter is active\n        node_value = tree_expression[leaf_level_map[leaf_idx]]\n        atom_num_parameters = num_parameter_array[node_value]\n\n        is_active_node = leaf_level_map[leaf_idx] &gt;= 0\n        is_active_param = parameter_idx &lt; atom_num_parameters\n        is_active = is_active_node &amp; is_active_param\n\n        # check whether the parameter node is part of the considered nodes\n        considered_for_sampling = jnp.isin(\n            leaf_level_map[leaf_idx],\n            considered_nodes,\n        )\n\n        def process_active_and_considered(indices: tuple) -&gt; ScalarFloat:\n            \"\"\"If active and considered for sampling, transform parameter back\n            to standard normal and calculate probability.\"\"\"\n            leaf_idx, parameter_idx = indices\n\n            # select parameter and transform back to standard normal\n            param = kernel_parameters[leaf_idx, parameter_idx]\n\n            transformed_param = lax.switch(\n                support_mapping_array[node_value, parameter_idx],\n                inverse_bijectors,\n                param,\n            )\n\n            # calculate log probability (standard normal)\n            log_prob_param = -0.5 * (transformed_param**2 + jnp.log(2 * jnp.pi))\n\n            return log_prob_param\n\n        def process_others(indices: tuple) -&gt; ScalarFloat:\n            \"\"\"If not active or not considered for sampling, don't\n            consider for log probability.\"\"\"\n            return jnp.array(0.0)\n\n        # process parameter, get probabilities\n        log_prob_param = lax.cond(\n            is_active &amp; considered_for_sampling,\n            process_active_and_considered,\n            process_others,\n            (leaf_idx, parameter_idx),\n        )\n\n        # update the log probability\n        log_probability += log_prob_param\n\n        return (kernel_parameters, log_probability), indices\n\n    # get initial kernel parameters\n    kernel_parameters: jnp.ndarray = state.parameters.value  # type: ignore\n\n    # get all parameter index combinations\n    parameter_indices = jnp.indices((max_leaves, max_atom_parameters)).reshape(2, -1).T\n\n    # scan over all parameters and get probabilities\n    parameters_and_total_log_prob, _ = lax.scan(\n        process_params,\n        (kernel_parameters, jnp.array(0.0)),\n        parameter_indices,\n    )\n\n    return parameters_and_total_log_prob[1]\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/prior/#gallifrey.kernels.prior.sample_parameters","title":"<code>sample_parameters(key, state, considered_nodes, num_parameter_array, max_leaves, max_atom_parameters, support_mapping_array, forward_bijectors)</code>","text":"<p>A function to sample new parameters for a state, and apply them to the kernel. Also returns the log probability of the sampled parameters.</p> <p>The parameters are sampled from a standard normal. Whether a parameter is sampled or not is determined by the tree expression and the leaf level map (See 'gallifrey.kernels.tree' for more information). The parameter are transformed to the desired prior distribution using the support_mapping_array and the forward_bijectors.</p> <p>We include an input 'considered_nodes' to only sample a specific subset of nodes. This is useful for structure moves, where only a subset of the parameters are changed. (Since arrays need to be of fixed shape, consider_nodes can be padded with -1 or any value that's not a valid leaf index.)</p> <p>Inactive parameters are set to -1.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for sampling.</p> required <code>state</code> <code>State</code> <p>The original kernel state to be filled with new parameters. The state must contain the following attributes: - tree_expression: The tree expression that describes the kernel structure. - leaf_level_map: A array that maps the index of the leaf parameter array</p> required <code>considered_nodes</code> <code>Int[ndarray, ' D']</code> <p>This array needs to contain all the indices that are supposed to be sampled. For first time sampling, that should be all indices (or at least all leaves). For structure moves (e.g. subtree-replace move), it should contain the indices that have been changed from the previous tree.</p> required <code>num_parameter_array</code> <code>Int[ndarray, ' D']</code> <p>An array that contains the number of parameters for each atom in the kernel structure. Needs to be in same order as the atom library.</p> required <code>max_leaves</code> <code>int</code> <p>The maximum number of leaves in the tree.</p> required <code>max_atom_parameters</code> <code>int</code> <p>The maximum number of parameters any atom will take.</p> required <code>support_mapping_array</code> <code>Int[ndarray, 'M N']</code> <p>An array that maps the support of the parameters to the corresponding bijector index.</p> required <code>forward_bijectors</code> <code>tuple[Callable, ...]</code> <p>A tuple of bijectors to transform the sampled parameters from a standard normal to the desired prior distribution.</p> required <p>Returns:</p> Type Description <code>State</code> <p>The state with the sampled parameters.</p> <code>ScalarFloat</code> <p>The (total) log probability of the sampled parameters, sum of all individual log probabilities.</p> Source code in <code>gallifrey/kernels/prior.py</code> <pre><code>@partial(\n    jit,\n    static_argnames=(\n        \"max_leaves\",\n        \"max_atom_parameters\",\n        \"forward_bijectors\",\n    ),\n)\ndef sample_parameters(\n    key: PRNGKeyArray,\n    state: nnx.State,\n    considered_nodes: Int[jnp.ndarray, \" D\"],\n    num_parameter_array: Int[jnp.ndarray, \" D\"],\n    max_leaves: int,\n    max_atom_parameters: int,\n    support_mapping_array: Int[jnp.ndarray, \"M N\"],\n    forward_bijectors: tuple[tp.Callable, ...],\n) -&gt; tuple[nnx.State, ScalarFloat]:\n    \"\"\"\n    A function to sample new parameters for a state, and apply them to\n    the kernel. Also returns the log probability of the sampled parameters.\n\n    The parameters are sampled from a standard normal. Whether a\n    parameter is sampled or not is determined by the tree expression\n    and the leaf level map (See 'gallifrey.kernels.tree' for more\n    information).\n    The parameter are transformed to the desired prior distribution\n    using the support_mapping_array and the forward_bijectors.\n\n    We include an input 'considered_nodes' to only sample a specific\n    subset of nodes. This is useful for structure moves, where only\n    a subset of the parameters are changed. (Since arrays need to\n    be of fixed shape, consider_nodes can be padded with -1 or\n    any value that's not a valid leaf index.)\n\n    Inactive parameters are set to -1.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for sampling.\n    state : nnx.State\n        The original kernel state to be filled with new parameters.\n        The state must contain the following attributes:\n        - tree_expression: The tree expression that describes the kernel structure.\n        - leaf_level_map: A array that maps the index of the leaf parameter array\n    considered_nodes : Int[jnp.ndarray, \" D\"]\n        This array needs to contain all the indices that are\n        supposed to be sampled. For first time sampling, that\n        should be all indices (or at least all leaves). For\n        structure moves (e.g. subtree-replace move), it should\n        contain the indices that have been changed from the\n        previous tree.\n    num_parameter_array : Int[jnp.ndarray, \" D\"]\n        An array that contains the number of parameters for each\n        atom in the kernel structure. Needs to be in same order\n        as the atom library.\n    max_leaves : int\n        The maximum number of leaves in the tree.\n    max_atom_parameters : int\n        The maximum number of parameters any atom will take.\n    support_mapping_array : Int[jnp.ndarray, \"M N\"]\n        An array that maps the support of the parameters to the\n        corresponding bijector index.\n    forward_bijectors : tuple[tp.Callable, ...]\n        A tuple of bijectors to transform the sampled parameters from\n        a standard normal to the desired prior distribution.\n\n    Returns\n    -------\n    nnx.State\n        The state with the sampled parameters.\n    ScalarFloat\n        The (total) log probability of the sampled parameters, sum\n        of all individual log probabilities.\n\n    \"\"\"\n\n    tree_expression: jnp.ndarray = state.tree_expression.value  # type: ignore\n    leaf_level_map: jnp.ndarray = state.leaf_level_map.value  # type: ignore\n\n    def process_params(\n        state: tuple[Float[jnp.ndarray, \"M N\"], Float[jnp.ndarray, \"\"], PRNGKeyArray],\n        indices: Int[jnp.ndarray, \"2\"],\n    ) -&gt; tuple[\n        tuple[Float[jnp.ndarray, \"M N\"], Float[jnp.ndarray, \"\"], PRNGKeyArray],\n        Int[jnp.ndarray, \"2\"],\n    ]:\n        \"\"\"Process parameter based on if it's active and considered for sampling.\"\"\"\n\n        # unpack state and indices\n        kernel_parameter, log_probability, key = state\n        leaf_idx, parameter_idx = indices\n\n        # check whether the parameter is active\n        node_value = tree_expression[leaf_level_map[leaf_idx]]\n        atom_num_parameters = num_parameter_array[node_value]\n\n        is_active_node = leaf_level_map[leaf_idx] &gt;= 0\n        is_active_param = parameter_idx &lt; atom_num_parameters\n        is_active = is_active_node &amp; is_active_param\n\n        # check whether the parameter node is part of the considered nodes\n        considered_for_sampling = jnp.isin(\n            leaf_level_map[leaf_idx],\n            considered_nodes,\n        )\n\n        def process_active_and_considered(indices_and_key: tuple) -&gt; tuple:\n            \"\"\"If active and considered for sampling, sample and transform the\n            parameter, and calculate the log probability.\"\"\"\n            leaf_idx, parameter_idx, key = indices_and_key\n\n            # sample parameter\n            key, subkey = jr.split(key)\n            sampled_param = jr.normal(subkey)\n\n            # calculate log probability (standard normal)\n            log_prob_param = -0.5 * (sampled_param**2 + jnp.log(2 * jnp.pi))\n\n            # transform parameter to desired prior distribution\n            transformed_param = lax.switch(\n                support_mapping_array[node_value, parameter_idx],\n                forward_bijectors,\n                sampled_param,\n            )\n            return transformed_param, log_prob_param, key\n\n        def process_active_not_considered(indices_and_key: tuple) -&gt; tuple:\n            \"\"\"If active but not considered, return the current parameter.\"\"\"\n            leaf_idx, parameter_idx, key = indices_and_key\n            return kernel_parameter[leaf_idx, parameter_idx], 0.0, key\n\n        def process_inactive(indices_and_key: tuple) -&gt; tuple:\n            \"\"\"If inactive, return -1.\"\"\"\n            _, _, key = indices_and_key\n            return jnp.array(-1.0), 0.0, key  # inactive parameters are set to -1\n\n        # process parameter, get new parameter and key\n        sampled_param, log_prob_param, key = lax.cond(\n            is_active,\n            lambda _: lax.cond(\n                considered_for_sampling,\n                process_active_and_considered,\n                process_active_not_considered,\n                (leaf_idx, parameter_idx, key),\n            ),\n            process_inactive,\n            (leaf_idx, parameter_idx, key),\n        )\n\n        # update kernel parameter\n        kernel_parameter = kernel_parameter.at[leaf_idx, parameter_idx].set(\n            sampled_param\n        )\n\n        # update the log probability\n        log_probability = log_probability + log_prob_param\n\n        return (kernel_parameter, log_probability, key), indices\n\n    # get initial kernel parameters\n    kernel_parameters: jnp.ndarray = state.parameters.value  # type: ignore\n\n    # get all parameter index combinations\n    parameter_indices = jnp.indices((max_leaves, max_atom_parameters)).reshape(2, -1).T\n\n    # scan over all parameters and update kernel parameters\n    final_state, _ = lax.scan(\n        process_params,\n        (kernel_parameters, jnp.array(0.0), key),\n        parameter_indices,\n    )\n    new_kernel_parameters = final_state[0]\n    tot_log_prob = final_state[1]\n\n    state[\"parameters\"] = state[\"parameters\"].replace(\n        new_kernel_parameters,\n    )  # type: ignore\n    return state, tot_log_prob\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/tree/","title":"tree","text":""},{"location":"autoapi/gallifrey/kernels/tree/#gallifrey.kernels.tree.TreeKernel","title":"<code>TreeKernel</code>","text":"<p>               Bases: <code>Module</code></p> <p>A kernel class to evaluate a tree-like kernel expression. The tree expression is a jnp array, with the tree structure encoded as a level-order array. Similar to NestedCombinationKernel in GPJax, but with explicit tree structure.</p> <p>Assume for example that the tree expression is [2, 1, 2, -1, -1, 0, 1] and the kernel library is [RBFAtom, LinearAtom, SumAtom, ProductAtom]. In this case -1 -&gt; empty node, 0 -&gt; RBFKernel, 1 -&gt; LinearKernel, 2 -&gt; Sum, 3 -&gt; Product. The is_operator array should be [False, False, True, True]. The tree expression would then be evaluated to Sum(Linear(x, y), Sum(RBF(x,y), Linear(x,y)).</p> <p>The same kernel could be constructed using NestedCombinationKernel, but the tree structure would not be explicit.</p> <p>Attributes (NOTE: A lot of values are initialized as nnx.Variable, use .value to access the</p> actual value.) <p>tree_expression : Int[jnp.ndarray, \" D\"]     The tree expression, a level-order array describing the tree structure.     Negative values indicate empty nodes. max_nodes : ScalarInt     The maximum number of nodes in the tree expression. max_depth : int     The maximum depth of the tree expression. max_stack : ScalarInt     The maximum size of the stack used to evaluate the tree expression,     which is equal to the maximum depth of the tree expression + 1. max_leaves : ScalarInt     The maximum number of leaves in the tree expression. max_atom_parameters : ScalarInt     The maximum number of parameters of any atom in the atom library. Comes     from the kernel library. max_total_parameters : ScalarInt     The maximum number of parameters in the tree expression (max_leaves     * max_atom_parameters). num_datapoints : ScalarInt     The number of datapoints in the input training data. (This is needed to     correctly construct the gram matrix, in a jit-compatible way.) root_idx : ScalarInt     The index of the root node in the tree expression. atoms : tuple[AbstractAtom]     The atoms in the kernel library. operators : tuple[AbstractOperator]     The operators in the kernel library. num_atoms : ScalarInt     The number of atoms in the kernel library. num_operators : ScalarInt     The number of operators in the kernel library. is_operator : Bool[jnp.ndarray, \" D\"]     A boolean array indicating whether the node in the tree expression is an     operator or not. Comes from the kernel library. post_order_expression : Int[jnp.ndarray, \" D\"]     The tree_expression in post-order traversal notation. Used for efficient     evaluation of the tree expression. Negative values indicate empty nodes. post_level_map : Int[jnp.ndarray, \" D\"]     A map from the post-order index to the level-order index. The value at     a given index in the post-order expression corresponds to the index of     the same node in the level-order expression. Negative values indicate     empty nodes. num_nodes : ScalarInt     The actual number of nodes in the tree expression. node_sizes : Int[jnp.ndarray, \" D\"]     An array containing the sizes of the nodes in the tree. The size of a node     is the number of nodes in the subtree rooted at that node. The size of a     given node is the value at its index in the level-order expression. node_heights : Int[jnp.ndarray, \" D\"]     An array containing the heights of the nodes in the tree. The height of a node     is the length of the longest path from the node to a leaf node (i.e. the heigh     of a leaf node is 0). The height of a given node is the value at its index in     the level-order expression. parameters : KernelParameter(Float[jnp.ndarray, \"M N\"])     KernelParameter instance, that holds a 2D array of kernel parameters with shape     (max_leaves, max_atom_parameters). The parameters are used to evaluate the tree     kernel. leaf_level_map : Int[jnp.ndarray, \" N\"]     A map from the leaf index to the level-order index. The leaf index i corresponds     to the i-th entry in the parameters 0th axis. The value of the leaf_level_map     at index i is the index of the node with these parameters in the level-order     expression.     NOTE: The leaf index is decided based on the depth of the tree, and might not be     in a simple order. See 'gallifrey.utils.tree_helper.get_parameter_leaf_idx' for     more information.</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>class TreeKernel(nnx.Module):\n    \"\"\"\n    A kernel class to evaluate a tree-like kernel expression. The tree expression\n    is a jnp array, with the tree structure encoded as a level-order array.\n    Similar to NestedCombinationKernel in GPJax, but with explicit tree\n    structure.\n\n    Assume for example that the tree expression is [2, 1, 2, -1, -1, 0, 1]\n    and the kernel library is [RBFAtom, LinearAtom, SumAtom, ProductAtom]. In this case\n    -1 -&gt; empty node,\n    0 -&gt; RBFKernel,\n    1 -&gt; LinearKernel,\n    2 -&gt; Sum,\n    3 -&gt; Product.\n    The is_operator array should be [False, False, True, True].\n    The tree expression would then be evaluated to\n    Sum(Linear(x, y), Sum(RBF(x,y), Linear(x,y)).\n\n    The same kernel could be constructed using NestedCombinationKernel, but the\n    tree structure would not be explicit.\n\n    Attributes\n    (NOTE: A lot of values are initialized as nnx.Variable, use .value to access the\n    actual value.)\n    ----------\n    tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression, a level-order array describing the tree structure.\n        Negative values indicate empty nodes.\n    max_nodes : ScalarInt\n        The maximum number of nodes in the tree expression.\n    max_depth : int\n        The maximum depth of the tree expression.\n    max_stack : ScalarInt\n        The maximum size of the stack used to evaluate the tree expression,\n        which is equal to the maximum depth of the tree expression + 1.\n    max_leaves : ScalarInt\n        The maximum number of leaves in the tree expression.\n    max_atom_parameters : ScalarInt\n        The maximum number of parameters of any atom in the atom library. Comes\n        from the kernel library.\n    max_total_parameters : ScalarInt\n        The maximum number of parameters in the tree expression (max_leaves\n        * max_atom_parameters).\n    num_datapoints : ScalarInt\n        The number of datapoints in the input training data. (This is needed to\n        correctly construct the gram matrix, in a jit-compatible way.)\n    root_idx : ScalarInt\n        The index of the root node in the tree expression.\n    atoms : tuple[AbstractAtom]\n        The atoms in the kernel library.\n    operators : tuple[AbstractOperator]\n        The operators in the kernel library.\n    num_atoms : ScalarInt\n        The number of atoms in the kernel library.\n    num_operators : ScalarInt\n        The number of operators in the kernel library.\n    is_operator : Bool[jnp.ndarray, \" D\"]\n        A boolean array indicating whether the node in the tree expression is an\n        operator or not. Comes from the kernel library.\n    post_order_expression : Int[jnp.ndarray, \" D\"]\n        The tree_expression in post-order traversal notation. Used for efficient\n        evaluation of the tree expression. Negative values indicate empty nodes.\n    post_level_map : Int[jnp.ndarray, \" D\"]\n        A map from the post-order index to the level-order index. The value at\n        a given index in the post-order expression corresponds to the index of\n        the same node in the level-order expression. Negative values indicate\n        empty nodes.\n    num_nodes : ScalarInt\n        The actual number of nodes in the tree expression.\n    node_sizes : Int[jnp.ndarray, \" D\"]\n        An array containing the sizes of the nodes in the tree. The size of a node\n        is the number of nodes in the subtree rooted at that node. The size of a\n        given node is the value at its index in the level-order expression.\n    node_heights : Int[jnp.ndarray, \" D\"]\n        An array containing the heights of the nodes in the tree. The height of a node\n        is the length of the longest path from the node to a leaf node (i.e. the heigh\n        of a leaf node is 0). The height of a given node is the value at its index in\n        the level-order expression.\n    parameters : KernelParameter(Float[jnp.ndarray, \"M N\"])\n        KernelParameter instance, that holds a 2D array of kernel parameters with shape\n        (max_leaves, max_atom_parameters). The parameters are used to evaluate the tree\n        kernel.\n    leaf_level_map : Int[jnp.ndarray, \" N\"]\n        A map from the leaf index to the level-order index. The leaf index i corresponds\n        to the i-th entry in the parameters 0th axis. The value of the leaf_level_map\n        at index i is the index of the node with these parameters in the level-order\n        expression.\n        NOTE: The leaf index is decided based on the depth of the tree, and might not be\n        in a simple order. See 'gallifrey.utils.tree_helper.get_parameter_leaf_idx' for\n        more information.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        tree_expression: Int[jnp.ndarray, \" D\"],\n        kernel_library: KernelLibrary,\n        max_depth: int,\n        num_datapoints: int,\n        root_idx: Int[jnp.ndarray, \"\"] = jnp.array(0),\n    ):\n        \"\"\"\n        Initialize a tree kernel.\n\n        Parameters\n        ----------\n        tree_expression : Int[jnp.ndarray, \" D\"]\n            The tree expression, a level-order expression of the tree.\n        kernel_library : KernelLibrary\n            An instance of the KernelLibrary class, containing the atoms\n            and operators to evaluate the tree expression.\n        max_depth : int\n            The maximum depth of the tree expression. The length of the\n            tree expression should be 2^(max_depth+1) - 1.\n        num_datapoints : int, optional\n            The number of datapoints in the input data. (This is needed to\n            correctly construct the gram matrix.)\n        root_idx : Int[jnp.ndarray, \"\"], optional\n            The index of the root node in the tree expression. By default 0.\n\n        \"\"\"\n        self.init_tree(\n            tree_expression,\n            kernel_library,\n            max_depth,\n            num_datapoints,\n            root_idx,\n        )\n        self.init_parameters()\n\n    def init_tree(\n        self,\n        tree_expression: Int[jnp.ndarray, \" D\"],\n        kernel_library: KernelLibrary,\n        max_depth: int,\n        num_datapoints: int,\n        root_idx: Int[jnp.ndarray, \"\"] = jnp.array(0),\n    ) -&gt; None:\n        \"\"\"\n        Initialize the base attributes of the tree kernel, which means\n        setting the\n        - tree_expression,\n        - max_nodes,\n        - root_idx,\n        - max_depth,\n        - is_operator (from the kernel library),\n        - atom_library (from the kernel library),\n        - max_atom_parameters (from the kernel library),\n        - post_order_expression,\n        - post_level_map,\n        - num_nodes,\n        - node_sizes, and\n        - node_heights.\n\n        The pre-order traversal expression and map attributes are initialized\n        but set to None. They get filled when calling the 'display' method.\n\n        Parameters\n        ----------\n        tree_expression : Int[jnp.ndarray, \" D\"]\n            The tree expression, a level-order expression of the tree.\n        kernel_library : tp.Optional[KernelLibrary], optional\n            An instance of the KernelLibrary class, containing the atoms and\n            operators to evaluate the tree expression.\n        max_depth : int\n            The maximum depth of the tree expression. The length of the\n            tree expression should be 2^(max_depth+1) - 1.\n        num_datapoints : int, optional\n            The number of datapoints in the input data. (This is needed to\n            correctly construct the gram matrix.)\n        root_idx : Int[jnp.ndarray, \"\"], optional\n            The index of the root node in the tree expression. By default 0.\n\n        \"\"\"\n        # set base attributes\n        self.tree_expression = nnx.Variable(tree_expression)\n\n        self.max_depth = nnx.Variable(max_depth)\n        self.max_stack = nnx.Variable(calculate_max_stack_size(max_depth))\n        self.max_nodes = nnx.Variable(calculate_max_nodes(max_depth))\n        self.max_leaves = nnx.Variable(calculate_max_leaves(max_depth))\n\n        self.num_datapoints = nnx.Variable(num_datapoints)\n\n        self.root_idx = nnx.Variable(root_idx)\n\n        # check if the root index is valid\n        # if self.root_idx.value &gt; self.max_nodes - 1:\n        #     raise ValueError(\n        #         \"Root index must be less than the maximum number of nodes.\"\n        #     )\n\n        # get the kernel library attributes\n        self.atoms = tuple(kernel_library.atoms)\n        self.operators = tuple(kernel_library.operators)\n        self.num_atoms = nnx.Variable(kernel_library.num_atoms)\n        self.num_operators = nnx.Variable(kernel_library.num_operators)\n        self.is_operator = nnx.Variable(kernel_library.is_operator)\n        self.max_atom_parameters = nnx.Variable(kernel_library.max_atom_parameters)\n        self.max_total_parameters = nnx.Variable(\n            self.max_leaves.value * self.max_atom_parameters.value  # type: ignore\n        )\n\n        # run the post-order traversal and get node metrics\n        (\n            post_order_expression,\n            num_nodes,\n            post_level_map,\n            node_sizes,\n            node_heights,\n            leaf_level_map,\n        ) = self.get_post_order_and_node_metrics()\n\n        self.post_order_expression = nnx.Variable(post_order_expression)\n        self.post_level_map = nnx.Variable(post_level_map)\n        self.num_nodes = nnx.Variable(num_nodes)\n        self.node_sizes = nnx.Variable(node_sizes)\n        self.node_heights = nnx.Variable(node_heights)\n        self.leaf_level_map = nnx.Variable(leaf_level_map)\n\n    def init_parameters(self) -&gt; None:\n        \"\"\"\n        Initialize the parameters of the tree kernel. The parameters are\n        a KernelParameter instance, which holds a 2D array of kernel parameters\n        with shape (max_leaves, max_atom_parameters).\n        \"\"\"\n        # initialise parameters\n        self.parameters = KernelParameter(\n            jnp.ones(\n                (\n                    self.max_leaves.value,\n                    self.max_atom_parameters.value,\n                )\n            )\n        )\n\n    def __call__(\n        self,\n        x: Float[jnp.ndarray, \" D\"],\n        y: Float[jnp.ndarray, \" D\"],\n    ) -&gt; ScalarFloat:\n        \"\"\"\n        Evaluate the tree kernel by traversing the tree expression.\n\n        Parameters\n        ----------\n        x : Float[jnp.ndarray, \" D\"]\n            Input x data.\n        y : Float[jnp.ndarray]\n            Input y data.\n\n        Returns\n        -------\n        ScalarFloat\n            The kernel value.\n        \"\"\"\n        return _evaluate_tree(\n            jnp.atleast_1d(x),\n            jnp.atleast_1d(y),\n            self.post_order_expression.value,\n            self.post_level_map.value,\n            self.is_operator.value,\n            self.parameters.value,\n            self.atoms,\n            self.operators,\n            (1, 1),\n            int(self.num_atoms.value),\n            int(self.max_depth.value),\n        ).squeeze()\n\n    def _tree_viz(self, include_parameters: bool = True) -&gt; str:\n        \"\"\"\n        Helper method for __str__ with optional parameter to include the\n        kernel parameters in the visualization.\n\n        Parameters\n        ----------\n        include_parameters : bool, optional\n            Whether to include the kernel parameters in the visualization.\n\n        Returns\n        -------\n        str\n            A string containing the visual representation of the tree kernel.\n        \"\"\"\n\n        pre_order_expression, num_nodes, pre_level_map = self.get_pre_order()\n\n        description = tree_visualization(\n            pre_order_expression[:num_nodes],\n            pre_level_map,\n            self.atoms + self.operators,\n            self.parameters.value,\n            self.is_operator.value,\n            self.root_idx.value,\n            self.max_depth.value,\n            include_parameters=include_parameters,\n            print_str=False,\n            return_str=True,\n        )\n        assert isinstance(description, str)\n        return description\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Construct a visual representation of the tree kernel.\n        See tree_visualization for more information.\n\n        Returns\n        -------\n        str\n            A string containing the visual representation of the tree kernel (if\n            return_str is True).\n        \"\"\"\n        return self._tree_viz(include_parameters=True)\n\n    def cross_covariance(\n        self,\n        x: Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \" M 1\"],\n        y: Float[jnp.ndarray, \" N\"] | Float[jnp.ndarray, \" N 1\"],\n    ) -&gt; Float[jnp.ndarray, \"M N\"]:\n        \"\"\"\n        Calculate the cross-covariance matrix between x and y\n        for the tree kernel.\n\n        Parameters\n        ----------\n        x : Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \" M 1\"]\n            Input x data.\n        y : Float[jnp.ndarray, \" N\"] | Float[jnp.ndarray, \" N 1\"]\n            Input y data.\n\n\n        Returns\n        -------\n        Float[jnp.ndarray, \"M N\"]\n            The cross-covariance matrix.\n        \"\"\"\n        return _evaluate_tree(\n            x,\n            y,\n            self.post_order_expression.value,\n            self.post_level_map.value,\n            self.is_operator.value,\n            self.parameters.value,\n            tuple([atom.cross_covariance for atom in self.atoms]),\n            self.operators,\n            (int(len(x)), int(len(y))),\n            int(self.num_atoms.value),\n            int(self.max_depth.value),\n        )\n\n    def gram(\n        self,\n        x: Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \" M 1\"],\n    ) -&gt; Float[jnp.ndarray, \"M M\"]:\n        \"\"\"\n        Calculate the gram matrix for a given input x using\n        the tree kernel.\n\n        Parameters\n        ----------\n        x : Float[jnp.ndarray, \" D\"] | Float[jnp.ndarray, \" D 1\"]\n            Input x data.\n\n        Returns\n        -------\n        Float[jnp.ndarray, \"M M\"]\n            The gram matrix.\n        \"\"\"\n        return self.cross_covariance(x, x)\n\n    def _gram_train(\n        self,\n        x: Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \" M 1\"],\n    ) -&gt; Float[jnp.ndarray, \"M M\"]:\n        \"\"\"\n        Calculate the gram matrix for a given input x using\n        the tree kernel. This version has the number of datapoints\n        fixed to the initial number of datapoints in the training data,\n        and is used for evaluation in a jit-compatible way.\n\n        Returns\n        -------\n        Float[jnp.ndarray, \"M M\"]\n            The gram matrix.\n        \"\"\"\n\n        return _evaluate_tree(\n            x,\n            x,\n            self.post_order_expression.value,\n            self.post_level_map.value,\n            self.is_operator.value,\n            self.parameters.value,\n            tuple([atom.cross_covariance for atom in self.atoms]),\n            self.operators,\n            (int(self.num_datapoints.value), int(self.num_datapoints.value)),\n            int(self.num_atoms.value),\n            int(self.max_depth.value),\n        )\n\n    def get_pre_order(\n        self,\n    ) -&gt; tuple[Int[jnp.ndarray, \" D\"], ScalarInt, Int[jnp.ndarray, \" D\"]]:\n        \"\"\"\n        Get the pre-order traversal expression of the tree, as well as\n        the index of the last node in the tree and a map from the pre-order\n        index to the level-order index.\n\n\n        Returns\n        -------\n        Int[jnp.ndarray, \" D\"]\n            The pre-order traversal expression of the tree.\n        ScalarInt\n            The total number of nodes in the tree.\n        Int[jnp.ndarray, \" D\"]\n            A map from the pre-order index to the level-order index. The value at a\n            given index in the pre-order expression corresponds to the index of the\n            same node in the level-order expression\n        \"\"\"\n\n        initial_expression = jnp.full(self.max_nodes.value, -1)\n        initial_expression_pointer = 0\n        pre_level_map = jnp.copy(initial_expression)\n        initial_stack = (\n            jnp.copy(initial_expression).at[0].set(self.root_idx)\n        )  # fill with root\n        initial_stack_pointer = 0\n\n        initial_state = (\n            initial_expression,\n            initial_expression_pointer,\n            pre_level_map,\n            initial_stack,\n            initial_stack_pointer,\n        )\n\n        return level_order_to_pre_order(\n            self.tree_expression.value,\n            initial_state,\n            self.max_nodes.value,\n        )\n\n    def get_post_order_and_node_metrics(\n        self,\n    ) -&gt; tuple[\n        Int[jnp.ndarray, \" D\"],\n        ScalarInt,\n        Int[jnp.ndarray, \" D\"],\n        Int[jnp.ndarray, \" D\"],\n        Int[jnp.ndarray, \" D\"],\n        Int[jnp.ndarray, \" D\"],\n    ]:\n        \"\"\"\n        Get the post-order traversal expression of the tree, the output is\n        - the post-order traversal expression of the tree,\n        - the total number of nodes in the tree,\n        - a map from the post-order index to the level-order index,\n        - an array containing the sizes of the nodes in the tree (located at the\n          level-order index),\n        - an array containing the heights of the nodes in the tree (located at\n          the level-order index).\n        - a map from the leaf index to the level-order index,\n        - the total number of leaves in the tree.\n\n        (Unlike the get_pre_order method, this method also returns the\n        node sizes, heights, leaf level map, and number of leaves.)\n\n        Returns\n        -------\n        Int[jnp.ndarray, \" D\"]\n            The post-order traversal expression of the tree.\n        ScalarInt\n            The total number of nodes in the tree.\n        Int[jnp.ndarray, \" D\"]\n            A map from the post-order index to the level-order index. The value at a\n            given index in the post-order expression corresponds to the index of the\n            same node in the level-order expression\n        Int[jnp.ndarray, \" D\"]\n            An array containing the sizes of the nodes in the tree. The size of a node\n            is the number of nodes in the subtree rooted at that node. The size of a\n            given node is the value at its index in the level-order expression.\n        Int[jnp.ndarray, \" D\"]\n            An array containing the heights of the nodes in the tree. The height of a\n            node is the length of the longest path from the node to a leaf node\n            (i.e. the height of a leaf node is 0). The height of a given\n            node is the value at its index in the level-order expression.\n        Int[jnp.ndarray, \" D\"]\n            A map from the leaf index to the level-order index. The value at a given\n            index in the leaf level map corresponds to the index of the node with\n            these parameters in the level-order expression.\n            NOTE: The leaf index is decided based on the depth of the tree, and might\n            not be in a simple order. See\n            'gallifrey.utils.tree_helper.get_parameter_leaf_idx' for more information.\n        \"\"\"\n        initial_expression = jnp.full(self.max_nodes.value, -1)\n        initial_expression_pointer = 0\n        post_level_map = jnp.copy(initial_expression)\n        initial_stack = (\n            jnp.copy(initial_expression).at[0].set(self.root_idx)\n        )  # fill with root\n        initial_stack_pointer = 0\n        last_processed_idx = -1\n        node_sizes = jnp.full(self.max_nodes, 0)\n        node_heights = jnp.copy(node_sizes)\n        leaf_level_map = jnp.full(self.max_leaves, -1)\n\n        initial_state = (\n            initial_expression,\n            initial_expression_pointer,\n            post_level_map,\n            initial_stack,\n            initial_stack_pointer,\n            last_processed_idx,\n            node_sizes,\n            node_heights,\n            leaf_level_map,\n        )\n\n        return level_order_to_post_order_and_metrics(\n            self.tree_expression.value,\n            initial_state,\n            self.is_operator.value,\n            self.max_nodes.value,\n            self.max_depth.value,\n        )\n\n    def print_atoms(self) -&gt; None:\n        \"\"\"\n        Print the atoms in the atom library, together with their parameter names.\n        \"\"\"\n        for atom in self.atoms:\n            print(f\"{atom.name}: {atom.parameter_names}\")\n\n    def display(self) -&gt; None:\n        print(self.__str__())\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/tree/#gallifrey.kernels.tree.TreeKernel.__call__","title":"<code>__call__(x, y)</code>","text":"<p>Evaluate the tree kernel by traversing the tree expression.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[ndarray, ' D']</code> <p>Input x data.</p> required <code>y</code> <code>Float[ndarray]</code> <p>Input y data.</p> required <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The kernel value.</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def __call__(\n    self,\n    x: Float[jnp.ndarray, \" D\"],\n    y: Float[jnp.ndarray, \" D\"],\n) -&gt; ScalarFloat:\n    \"\"\"\n    Evaluate the tree kernel by traversing the tree expression.\n\n    Parameters\n    ----------\n    x : Float[jnp.ndarray, \" D\"]\n        Input x data.\n    y : Float[jnp.ndarray]\n        Input y data.\n\n    Returns\n    -------\n    ScalarFloat\n        The kernel value.\n    \"\"\"\n    return _evaluate_tree(\n        jnp.atleast_1d(x),\n        jnp.atleast_1d(y),\n        self.post_order_expression.value,\n        self.post_level_map.value,\n        self.is_operator.value,\n        self.parameters.value,\n        self.atoms,\n        self.operators,\n        (1, 1),\n        int(self.num_atoms.value),\n        int(self.max_depth.value),\n    ).squeeze()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/tree/#gallifrey.kernels.tree.TreeKernel.__init__","title":"<code>__init__(tree_expression, kernel_library, max_depth, num_datapoints, root_idx=jnp.array(0))</code>","text":"<p>Initialize a tree kernel.</p> <p>Parameters:</p> Name Type Description Default <code>tree_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression, a level-order expression of the tree.</p> required <code>kernel_library</code> <code>KernelLibrary</code> <p>An instance of the KernelLibrary class, containing the atoms and operators to evaluate the tree expression.</p> required <code>max_depth</code> <code>int</code> <p>The maximum depth of the tree expression. The length of the tree expression should be 2^(max_depth+1) - 1.</p> required <code>num_datapoints</code> <code>int</code> <p>The number of datapoints in the input data. (This is needed to correctly construct the gram matrix.)</p> required <code>root_idx</code> <code>Int[ndarray, '']</code> <p>The index of the root node in the tree expression. By default 0.</p> <code>array(0)</code> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def __init__(\n    self,\n    tree_expression: Int[jnp.ndarray, \" D\"],\n    kernel_library: KernelLibrary,\n    max_depth: int,\n    num_datapoints: int,\n    root_idx: Int[jnp.ndarray, \"\"] = jnp.array(0),\n):\n    \"\"\"\n    Initialize a tree kernel.\n\n    Parameters\n    ----------\n    tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression, a level-order expression of the tree.\n    kernel_library : KernelLibrary\n        An instance of the KernelLibrary class, containing the atoms\n        and operators to evaluate the tree expression.\n    max_depth : int\n        The maximum depth of the tree expression. The length of the\n        tree expression should be 2^(max_depth+1) - 1.\n    num_datapoints : int, optional\n        The number of datapoints in the input data. (This is needed to\n        correctly construct the gram matrix.)\n    root_idx : Int[jnp.ndarray, \"\"], optional\n        The index of the root node in the tree expression. By default 0.\n\n    \"\"\"\n    self.init_tree(\n        tree_expression,\n        kernel_library,\n        max_depth,\n        num_datapoints,\n        root_idx,\n    )\n    self.init_parameters()\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/tree/#gallifrey.kernels.tree.TreeKernel.__str__","title":"<code>__str__()</code>","text":"<p>Construct a visual representation of the tree kernel. See tree_visualization for more information.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string containing the visual representation of the tree kernel (if return_str is True).</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Construct a visual representation of the tree kernel.\n    See tree_visualization for more information.\n\n    Returns\n    -------\n    str\n        A string containing the visual representation of the tree kernel (if\n        return_str is True).\n    \"\"\"\n    return self._tree_viz(include_parameters=True)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/tree/#gallifrey.kernels.tree.TreeKernel.cross_covariance","title":"<code>cross_covariance(x, y)</code>","text":"<p>Calculate the cross-covariance matrix between x and y for the tree kernel.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[ndarray, ' M'] | Float[ndarray, ' M 1']</code> <p>Input x data.</p> required <code>y</code> <code>Float[ndarray, ' N'] | Float[ndarray, ' N 1']</code> <p>Input y data.</p> required <p>Returns:</p> Type Description <code>Float[ndarray, 'M N']</code> <p>The cross-covariance matrix.</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def cross_covariance(\n    self,\n    x: Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \" M 1\"],\n    y: Float[jnp.ndarray, \" N\"] | Float[jnp.ndarray, \" N 1\"],\n) -&gt; Float[jnp.ndarray, \"M N\"]:\n    \"\"\"\n    Calculate the cross-covariance matrix between x and y\n    for the tree kernel.\n\n    Parameters\n    ----------\n    x : Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \" M 1\"]\n        Input x data.\n    y : Float[jnp.ndarray, \" N\"] | Float[jnp.ndarray, \" N 1\"]\n        Input y data.\n\n\n    Returns\n    -------\n    Float[jnp.ndarray, \"M N\"]\n        The cross-covariance matrix.\n    \"\"\"\n    return _evaluate_tree(\n        x,\n        y,\n        self.post_order_expression.value,\n        self.post_level_map.value,\n        self.is_operator.value,\n        self.parameters.value,\n        tuple([atom.cross_covariance for atom in self.atoms]),\n        self.operators,\n        (int(len(x)), int(len(y))),\n        int(self.num_atoms.value),\n        int(self.max_depth.value),\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/tree/#gallifrey.kernels.tree.TreeKernel.get_post_order_and_node_metrics","title":"<code>get_post_order_and_node_metrics()</code>","text":"<p>Get the post-order traversal expression of the tree, the output is - the post-order traversal expression of the tree, - the total number of nodes in the tree, - a map from the post-order index to the level-order index, - an array containing the sizes of the nodes in the tree (located at the   level-order index), - an array containing the heights of the nodes in the tree (located at   the level-order index). - a map from the leaf index to the level-order index, - the total number of leaves in the tree.</p> <p>(Unlike the get_pre_order method, this method also returns the node sizes, heights, leaf level map, and number of leaves.)</p> <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The post-order traversal expression of the tree.</p> <code>ScalarInt</code> <p>The total number of nodes in the tree.</p> <code>Int[ndarray, ' D']</code> <p>A map from the post-order index to the level-order index. The value at a given index in the post-order expression corresponds to the index of the same node in the level-order expression</p> <code>Int[ndarray, ' D']</code> <p>An array containing the sizes of the nodes in the tree. The size of a node is the number of nodes in the subtree rooted at that node. The size of a given node is the value at its index in the level-order expression.</p> <code>Int[ndarray, ' D']</code> <p>An array containing the heights of the nodes in the tree. The height of a node is the length of the longest path from the node to a leaf node (i.e. the height of a leaf node is 0). The height of a given node is the value at its index in the level-order expression.</p> <code>Int[ndarray, ' D']</code> <p>A map from the leaf index to the level-order index. The value at a given index in the leaf level map corresponds to the index of the node with these parameters in the level-order expression. NOTE: The leaf index is decided based on the depth of the tree, and might not be in a simple order. See 'gallifrey.utils.tree_helper.get_parameter_leaf_idx' for more information.</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def get_post_order_and_node_metrics(\n    self,\n) -&gt; tuple[\n    Int[jnp.ndarray, \" D\"],\n    ScalarInt,\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n]:\n    \"\"\"\n    Get the post-order traversal expression of the tree, the output is\n    - the post-order traversal expression of the tree,\n    - the total number of nodes in the tree,\n    - a map from the post-order index to the level-order index,\n    - an array containing the sizes of the nodes in the tree (located at the\n      level-order index),\n    - an array containing the heights of the nodes in the tree (located at\n      the level-order index).\n    - a map from the leaf index to the level-order index,\n    - the total number of leaves in the tree.\n\n    (Unlike the get_pre_order method, this method also returns the\n    node sizes, heights, leaf level map, and number of leaves.)\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The post-order traversal expression of the tree.\n    ScalarInt\n        The total number of nodes in the tree.\n    Int[jnp.ndarray, \" D\"]\n        A map from the post-order index to the level-order index. The value at a\n        given index in the post-order expression corresponds to the index of the\n        same node in the level-order expression\n    Int[jnp.ndarray, \" D\"]\n        An array containing the sizes of the nodes in the tree. The size of a node\n        is the number of nodes in the subtree rooted at that node. The size of a\n        given node is the value at its index in the level-order expression.\n    Int[jnp.ndarray, \" D\"]\n        An array containing the heights of the nodes in the tree. The height of a\n        node is the length of the longest path from the node to a leaf node\n        (i.e. the height of a leaf node is 0). The height of a given\n        node is the value at its index in the level-order expression.\n    Int[jnp.ndarray, \" D\"]\n        A map from the leaf index to the level-order index. The value at a given\n        index in the leaf level map corresponds to the index of the node with\n        these parameters in the level-order expression.\n        NOTE: The leaf index is decided based on the depth of the tree, and might\n        not be in a simple order. See\n        'gallifrey.utils.tree_helper.get_parameter_leaf_idx' for more information.\n    \"\"\"\n    initial_expression = jnp.full(self.max_nodes.value, -1)\n    initial_expression_pointer = 0\n    post_level_map = jnp.copy(initial_expression)\n    initial_stack = (\n        jnp.copy(initial_expression).at[0].set(self.root_idx)\n    )  # fill with root\n    initial_stack_pointer = 0\n    last_processed_idx = -1\n    node_sizes = jnp.full(self.max_nodes, 0)\n    node_heights = jnp.copy(node_sizes)\n    leaf_level_map = jnp.full(self.max_leaves, -1)\n\n    initial_state = (\n        initial_expression,\n        initial_expression_pointer,\n        post_level_map,\n        initial_stack,\n        initial_stack_pointer,\n        last_processed_idx,\n        node_sizes,\n        node_heights,\n        leaf_level_map,\n    )\n\n    return level_order_to_post_order_and_metrics(\n        self.tree_expression.value,\n        initial_state,\n        self.is_operator.value,\n        self.max_nodes.value,\n        self.max_depth.value,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/tree/#gallifrey.kernels.tree.TreeKernel.get_pre_order","title":"<code>get_pre_order()</code>","text":"<p>Get the pre-order traversal expression of the tree, as well as the index of the last node in the tree and a map from the pre-order index to the level-order index.</p> <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The pre-order traversal expression of the tree.</p> <code>ScalarInt</code> <p>The total number of nodes in the tree.</p> <code>Int[ndarray, ' D']</code> <p>A map from the pre-order index to the level-order index. The value at a given index in the pre-order expression corresponds to the index of the same node in the level-order expression</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def get_pre_order(\n    self,\n) -&gt; tuple[Int[jnp.ndarray, \" D\"], ScalarInt, Int[jnp.ndarray, \" D\"]]:\n    \"\"\"\n    Get the pre-order traversal expression of the tree, as well as\n    the index of the last node in the tree and a map from the pre-order\n    index to the level-order index.\n\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The pre-order traversal expression of the tree.\n    ScalarInt\n        The total number of nodes in the tree.\n    Int[jnp.ndarray, \" D\"]\n        A map from the pre-order index to the level-order index. The value at a\n        given index in the pre-order expression corresponds to the index of the\n        same node in the level-order expression\n    \"\"\"\n\n    initial_expression = jnp.full(self.max_nodes.value, -1)\n    initial_expression_pointer = 0\n    pre_level_map = jnp.copy(initial_expression)\n    initial_stack = (\n        jnp.copy(initial_expression).at[0].set(self.root_idx)\n    )  # fill with root\n    initial_stack_pointer = 0\n\n    initial_state = (\n        initial_expression,\n        initial_expression_pointer,\n        pre_level_map,\n        initial_stack,\n        initial_stack_pointer,\n    )\n\n    return level_order_to_pre_order(\n        self.tree_expression.value,\n        initial_state,\n        self.max_nodes.value,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/tree/#gallifrey.kernels.tree.TreeKernel.gram","title":"<code>gram(x)</code>","text":"<p>Calculate the gram matrix for a given input x using the tree kernel.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[ndarray, ' D'] | Float[ndarray, ' D 1']</code> <p>Input x data.</p> required <p>Returns:</p> Type Description <code>Float[ndarray, 'M M']</code> <p>The gram matrix.</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def gram(\n    self,\n    x: Float[jnp.ndarray, \" M\"] | Float[jnp.ndarray, \" M 1\"],\n) -&gt; Float[jnp.ndarray, \"M M\"]:\n    \"\"\"\n    Calculate the gram matrix for a given input x using\n    the tree kernel.\n\n    Parameters\n    ----------\n    x : Float[jnp.ndarray, \" D\"] | Float[jnp.ndarray, \" D 1\"]\n        Input x data.\n\n    Returns\n    -------\n    Float[jnp.ndarray, \"M M\"]\n        The gram matrix.\n    \"\"\"\n    return self.cross_covariance(x, x)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/tree/#gallifrey.kernels.tree.TreeKernel.init_parameters","title":"<code>init_parameters()</code>","text":"<p>Initialize the parameters of the tree kernel. The parameters are a KernelParameter instance, which holds a 2D array of kernel parameters with shape (max_leaves, max_atom_parameters).</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def init_parameters(self) -&gt; None:\n    \"\"\"\n    Initialize the parameters of the tree kernel. The parameters are\n    a KernelParameter instance, which holds a 2D array of kernel parameters\n    with shape (max_leaves, max_atom_parameters).\n    \"\"\"\n    # initialise parameters\n    self.parameters = KernelParameter(\n        jnp.ones(\n            (\n                self.max_leaves.value,\n                self.max_atom_parameters.value,\n            )\n        )\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/tree/#gallifrey.kernels.tree.TreeKernel.init_tree","title":"<code>init_tree(tree_expression, kernel_library, max_depth, num_datapoints, root_idx=jnp.array(0))</code>","text":"<p>Initialize the base attributes of the tree kernel, which means setting the - tree_expression, - max_nodes, - root_idx, - max_depth, - is_operator (from the kernel library), - atom_library (from the kernel library), - max_atom_parameters (from the kernel library), - post_order_expression, - post_level_map, - num_nodes, - node_sizes, and - node_heights.</p> <p>The pre-order traversal expression and map attributes are initialized but set to None. They get filled when calling the 'display' method.</p> <p>Parameters:</p> Name Type Description Default <code>tree_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression, a level-order expression of the tree.</p> required <code>kernel_library</code> <code>Optional[KernelLibrary]</code> <p>An instance of the KernelLibrary class, containing the atoms and operators to evaluate the tree expression.</p> required <code>max_depth</code> <code>int</code> <p>The maximum depth of the tree expression. The length of the tree expression should be 2^(max_depth+1) - 1.</p> required <code>num_datapoints</code> <code>int</code> <p>The number of datapoints in the input data. (This is needed to correctly construct the gram matrix.)</p> required <code>root_idx</code> <code>Int[ndarray, '']</code> <p>The index of the root node in the tree expression. By default 0.</p> <code>array(0)</code> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def init_tree(\n    self,\n    tree_expression: Int[jnp.ndarray, \" D\"],\n    kernel_library: KernelLibrary,\n    max_depth: int,\n    num_datapoints: int,\n    root_idx: Int[jnp.ndarray, \"\"] = jnp.array(0),\n) -&gt; None:\n    \"\"\"\n    Initialize the base attributes of the tree kernel, which means\n    setting the\n    - tree_expression,\n    - max_nodes,\n    - root_idx,\n    - max_depth,\n    - is_operator (from the kernel library),\n    - atom_library (from the kernel library),\n    - max_atom_parameters (from the kernel library),\n    - post_order_expression,\n    - post_level_map,\n    - num_nodes,\n    - node_sizes, and\n    - node_heights.\n\n    The pre-order traversal expression and map attributes are initialized\n    but set to None. They get filled when calling the 'display' method.\n\n    Parameters\n    ----------\n    tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression, a level-order expression of the tree.\n    kernel_library : tp.Optional[KernelLibrary], optional\n        An instance of the KernelLibrary class, containing the atoms and\n        operators to evaluate the tree expression.\n    max_depth : int\n        The maximum depth of the tree expression. The length of the\n        tree expression should be 2^(max_depth+1) - 1.\n    num_datapoints : int, optional\n        The number of datapoints in the input data. (This is needed to\n        correctly construct the gram matrix.)\n    root_idx : Int[jnp.ndarray, \"\"], optional\n        The index of the root node in the tree expression. By default 0.\n\n    \"\"\"\n    # set base attributes\n    self.tree_expression = nnx.Variable(tree_expression)\n\n    self.max_depth = nnx.Variable(max_depth)\n    self.max_stack = nnx.Variable(calculate_max_stack_size(max_depth))\n    self.max_nodes = nnx.Variable(calculate_max_nodes(max_depth))\n    self.max_leaves = nnx.Variable(calculate_max_leaves(max_depth))\n\n    self.num_datapoints = nnx.Variable(num_datapoints)\n\n    self.root_idx = nnx.Variable(root_idx)\n\n    # check if the root index is valid\n    # if self.root_idx.value &gt; self.max_nodes - 1:\n    #     raise ValueError(\n    #         \"Root index must be less than the maximum number of nodes.\"\n    #     )\n\n    # get the kernel library attributes\n    self.atoms = tuple(kernel_library.atoms)\n    self.operators = tuple(kernel_library.operators)\n    self.num_atoms = nnx.Variable(kernel_library.num_atoms)\n    self.num_operators = nnx.Variable(kernel_library.num_operators)\n    self.is_operator = nnx.Variable(kernel_library.is_operator)\n    self.max_atom_parameters = nnx.Variable(kernel_library.max_atom_parameters)\n    self.max_total_parameters = nnx.Variable(\n        self.max_leaves.value * self.max_atom_parameters.value  # type: ignore\n    )\n\n    # run the post-order traversal and get node metrics\n    (\n        post_order_expression,\n        num_nodes,\n        post_level_map,\n        node_sizes,\n        node_heights,\n        leaf_level_map,\n    ) = self.get_post_order_and_node_metrics()\n\n    self.post_order_expression = nnx.Variable(post_order_expression)\n    self.post_level_map = nnx.Variable(post_level_map)\n    self.num_nodes = nnx.Variable(num_nodes)\n    self.node_sizes = nnx.Variable(node_sizes)\n    self.node_heights = nnx.Variable(node_heights)\n    self.leaf_level_map = nnx.Variable(leaf_level_map)\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/tree/#gallifrey.kernels.tree.TreeKernel.print_atoms","title":"<code>print_atoms()</code>","text":"<p>Print the atoms in the atom library, together with their parameter names.</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def print_atoms(self) -&gt; None:\n    \"\"\"\n    Print the atoms in the atom library, together with their parameter names.\n    \"\"\"\n    for atom in self.atoms:\n        print(f\"{atom.name}: {atom.parameter_names}\")\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/tree/#gallifrey.kernels.tree.level_order_to_post_order_and_metrics","title":"<code>level_order_to_post_order_and_metrics(level_order, initial_state, is_operator, max_nodes, max_depth)</code>","text":"<p>Convert a level-order expression of a tree to a post-order expression, and other key metrics of the tree. The post-order expression is used to evaluate the tree kernel efficiently.</p> <p>Returns: - the post-order traversal expression of the tree - the number of nodes in the tree - a map from the post-order index to the level-order index - an array containing the sizes of the nodes in the tree - an array containing the heights of the nodes in the tree - a map from the leaf index to the level-order index</p> <p>NOTE: This function should be called via the get_post_order method of the TreeKernel.</p> <p>Parameters:</p> Name Type Description Default <code>level_order</code> <code>Int[ndarray, ' D']</code> <p>The level-order expression of the tree.</p> required <code>initial_state</code> <code>tuple</code> <p>The initial state, containing allocated arrays for: - the post-order expression - the post-order pointer - the post_level_map - the stack - the stack pointer - the last processed index - the node sizes - the node heights - the leaf level map</p> required <code>is_operator</code> <code>Bool[ndarray, ' D']</code> <p>A boolean array indicating whether the node in the tree expression is an operator or not.</p> required <code>max_nodes</code> <code>ScalarInt</code> <p>The maximum number of nodes in the tree expression.</p> required <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The post-order expression of the tree.</p> <code>ScalarInt</code> <p>The index of the last non-empty node in the tree (in the post-order expression).</p> <code>Int[ndarray, ' D']</code> <p>A map from the post-order index to the level-order index. The value at a given index in the post-order expression corresponds to the index of the same node in the level-order expression.</p> <code>Int[ndarray, ' D']</code> <p>An array containing the sizes of the nodes in the tree. The size of a node is the number of nodes in the subtree rooted at that node. The size of a given node is the value at its index in the level-order expression.</p> <code>Int[ndarray, ' D']</code> <p>An array containing the heights of the nodes in the tree. The height of a node is the length of the longest path from the node to a leaf node (i.e. the height of a leaf node is 0). The height of a given node is the value at its index in the level-order expression.</p> <code>Int[ndarray, ' D']</code> <p>A map from the leaf index to the level-order index. The value at a given index in the leaf level map corresponds to the index of the node with these parameters in the level-order expression. NOTE: The leaf index is decided based on the depth of the tree, and might not be in a simple order. See 'gallifrey.utils.tree_helper.get_parameter_leaf_idx' for more information.</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>@jit\ndef level_order_to_post_order_and_metrics(\n    level_order: Int[jnp.ndarray, \" D\"],\n    initial_state: tuple[\n        Int[jnp.ndarray, \" D\"],\n        ScalarInt,\n        Int[jnp.ndarray, \" D\"],\n        Int[jnp.ndarray, \" D\"],\n        ScalarInt,\n        ScalarInt,\n        Int[jnp.ndarray, \" D\"],\n        Int[jnp.ndarray, \" D\"],\n        Int[jnp.ndarray, \" D\"],\n    ],\n    is_operator: Bool[jnp.ndarray, \" D\"],\n    max_nodes: ScalarInt,\n    max_depth: ScalarInt,\n) -&gt; tuple[\n    Int[jnp.ndarray, \" D\"],\n    ScalarInt,\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n]:\n    \"\"\"\n    Convert a level-order expression of a tree to a post-order expression, and\n    other key metrics of the tree. The post-order expression is used to evaluate the\n    tree kernel efficiently.\n\n    Returns:\n    - the post-order traversal expression of the tree\n    - the number of nodes in the tree\n    - a map from the post-order index to the level-order index\n    - an array containing the sizes of the nodes in the tree\n    - an array containing the heights of the nodes in the tree\n    - a map from the leaf index to the level-order index\n\n\n    NOTE: This function should be called via the get_post_order method of the\n    TreeKernel.\n\n    Parameters\n    ----------\n    level_order : Int[jnp.ndarray, \" D\"]\n        The level-order expression of the tree.\n    initial_state : tuple\n        The initial state, containing allocated arrays for:\n        - the post-order expression\n        - the post-order pointer\n        - the post_level_map\n        - the stack\n        - the stack pointer\n        - the last processed index\n        - the node sizes\n        - the node heights\n        - the leaf level map\n    is_operator : Bool[jnp.ndarray, \" D\"]\n        A boolean array indicating whether the node in the tree expression is an\n        operator or not.\n    max_nodes : ScalarInt\n        The maximum number of nodes in the tree expression.\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The post-order expression of the tree.\n    ScalarInt\n        The index of the last non-empty node in the tree (in the post-order expression).\n    Int[jnp.ndarray, \" D\"]\n        A map from the post-order index to the level-order index. The value at a given\n        index in the post-order expression corresponds to the index of the same node in\n        the level-order expression.\n    Int[jnp.ndarray, \" D\"]\n        An array containing the sizes of the nodes in the tree. The size of a node is\n        the number of nodes in the subtree rooted at that node. The size of a given node\n        is the value at its index in the level-order expression.\n    Int[jnp.ndarray, \" D\"]\n        An array containing the heights of the nodes in the tree. The height of a node\n        is the length of the longest path from the node to a leaf node (i.e. the height\n        of a leaf node is 0). The height of a given node is the value at its index in\n        the level-order expression.\n    Int[jnp.ndarray, \" D\"]\n        A map from the leaf index to the level-order index. The value at a given index\n        in the leaf level map corresponds to the index of the node with these parameters\n        in the level-order expression.\n        NOTE: The leaf index is decided based on the depth of the tree, and might not be\n        in a simple order. See 'gallifrey.utils.tree_helper.get_parameter_leaf_idx' for\n        more information.\n\n    \"\"\"\n\n    def traverse_tree(state: tuple) -&gt; tuple:\n        \"\"\"Traverse the tree expression in level-order and convert it to\n        pre-order.\"\"\"\n\n        (\n            _,\n            _,\n            _,\n            stack,\n            stack_pointer,\n            last_processed_idx,\n            _,\n            _,\n            _,\n        ) = state\n\n        # pop the top of the stack\n        level_order_idx = stack[stack_pointer]\n\n        # check if the current node is a leaf\n        right_child_idx = get_child_idx(level_order_idx, \"r\")\n        is_leaf = jnp.logical_or(\n            jnp.logical_or(\n                right_child_idx &gt; max_nodes - 1,\n                level_order[right_child_idx - 1] == -1,\n            ),\n            right_child_idx == last_processed_idx,\n        )\n\n        def process_non_leaf(state: tuple) -&gt; tuple:\n            \"\"\"If the node is not a leaf, push the node back onto the stack\n            and push the children onto the stack.\n            \"\"\"\n\n            (\n                post_order,\n                post_order_pointer,\n                post_level_map,\n                stack,\n                stack_pointer,\n                last_processed_idx,\n                node_sizes,\n                node_heights,\n                leaf_level_map,\n            ) = state\n\n            left_child = get_child_idx(level_order_idx, \"l\")\n            right_child = get_child_idx(level_order_idx, \"r\")\n\n            # Push current node, then right child, then left child onto stack\n            stack_updated = stack.at[stack_pointer].set(level_order_idx)\n            stack_updated = stack_updated.at[stack_pointer + 1].set(right_child)\n            stack_updated = stack_updated.at[stack_pointer + 2].set(left_child)\n            stack_pointer_updated = stack_pointer + 2\n\n            return (\n                post_order,\n                post_order_pointer,\n                post_level_map,\n                stack_updated,\n                stack_pointer_updated,\n                last_processed_idx,\n                node_sizes,\n                node_heights,\n                leaf_level_map,\n            )\n\n        def process_leaf(state: tuple) -&gt; tuple:\n            \"\"\"If the node is a leaf, add the node to the post-order output.\"\"\"\n            (\n                post_order,\n                post_order_pointer,\n                post_level_map,\n                stack,\n                stack_pointer,\n                _,\n                node_sizes,\n                node_heights,\n                leaf_level_map,\n            ) = state\n\n            # add the current node to the post-order output\n            post_order_updated = post_order.at[post_order_pointer].set(\n                level_order[level_order_idx]\n            )\n            post_order_pointer_updated = post_order_pointer + 1\n\n            # update the post_level_map\n            post_level_map_updated = post_level_map.at[post_order_pointer].set(\n                level_order_idx\n            )\n\n            # update stack pointer\n            stack_pointer_updated = stack_pointer - 1\n\n            # update last processed index\n            last_processed_idx_updated = level_order_idx\n\n            # calculate and store the node size and heights\n            left_child_idx = get_child_idx(level_order_idx, \"l\")\n            right_child_idx = get_child_idx(level_order_idx, \"r\")\n\n            left_child_size, left_child_height, right_child_size, right_child_height = (\n                lax.cond(\n                    right_child_idx &lt; max_nodes,\n                    lambda: (\n                        node_sizes[left_child_idx],\n                        node_heights[left_child_idx],\n                        node_sizes[right_child_idx],\n                        node_heights[right_child_idx],\n                    ),\n                    lambda: (0, 0, 0, 0),\n                )\n            )\n\n            bigger_height = jnp.where(\n                left_child_height &gt; right_child_height,\n                left_child_height,\n                right_child_height,\n            )\n\n            node_sizes_updated = node_sizes.at[level_order_idx].set(\n                1 + left_child_size + right_child_size\n            )\n            node_heights_updated = node_heights.at[level_order_idx].set(\n                1 + bigger_height\n            )\n\n            # update leaf level map if the current node is a kernel atom,\n            # not an operator (this differs from being a leaf node, since\n            # in scaffolds, operators can be leaf nodes)\n            leaf_level_map_updated = jnp.where(\n                is_operator[level_order[level_order_idx]],\n                leaf_level_map,\n                leaf_level_map.at[\n                    get_parameter_leaf_idx(\n                        level_order_idx,\n                        max_depth,\n                    )\n                ].set(level_order_idx),\n            )\n\n            return (\n                post_order_updated,\n                post_order_pointer_updated,\n                post_level_map_updated,\n                stack,\n                stack_pointer_updated,\n                last_processed_idx_updated,\n                node_sizes_updated,\n                node_heights_updated,\n                leaf_level_map_updated,\n            )\n\n        # update the stack based on whether the current node is an operator\n        new_state = lax.cond(\n            is_leaf,\n            process_leaf,\n            process_non_leaf,\n            operand=state,\n        )\n\n        return new_state\n\n    def condition(state: tuple) -&gt; ScalarBool:\n        \"\"\"Loop condition: Continue until stack is empty.\"\"\"\n        _, _, _, _, stack_pointer, _, _, _, _ = state\n        return stack_pointer &gt;= 0\n\n    # iterate using lax.while_loop\n    (\n        final_pre_order,\n        num_nodes,\n        final_post_level_map,\n        _,\n        _,\n        _,\n        final_node_sizes,\n        final_node_heights,\n        final_leaf_level_map,\n    ) = lax.while_loop(\n        condition,\n        traverse_tree,\n        initial_state,\n    )\n\n    # to get the longest path from the root to a leaf, subtract 1 from the height,\n    # since we counted to total number of nodes from the root to the leaf, not the\n    # number of edges\n    final_node_heights -= 1\n\n    return (\n        final_pre_order,\n        num_nodes,\n        final_post_level_map,\n        final_node_sizes,\n        final_node_heights,\n        final_leaf_level_map,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/tree/#gallifrey.kernels.tree.level_order_to_pre_order","title":"<code>level_order_to_pre_order(level_order, initial_state, max_nodes)</code>","text":"<p>Convert a level-order expression of a tree to a pre-order expression. Also returns the total number of nodes in the tree. The pre-order expression is used to visualize the tree kernel.</p> <p>NOTE: This function should be called via the get_pre_order method of the TreeKernel.</p> <p>Parameters:</p> Name Type Description Default <code>level_order</code> <code>Int[ndarray, ' D']</code> <p>The level-order expression of the tree.</p> required <code>initial_state</code> <code>tuple</code> <p>The initial state, containing the pre-order expression, the pre-order pointer, the pre_level_map, the stack, and the stack pointer.</p> required <code>max_nodes</code> <code>ScalarInt</code> <p>The maximum number of nodes in the tree expression.</p> required <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The pre-order expression of the tree.</p> <code>ScalarInt</code> <p>The total number of nodes in the tree.</p> <code>Int[ndarray, ' D']</code> <p>A map from the pre-order index to the level-order index. The value at a given index in the pre-order expression corresponds to the index of the same node in the level-order expression.</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>@jit\ndef level_order_to_pre_order(\n    level_order: Int[jnp.ndarray, \" D\"],\n    initial_state: tuple[\n        Int[jnp.ndarray, \" D\"],\n        ScalarInt,\n        Int[jnp.ndarray, \" D\"],\n        Int[jnp.ndarray, \" D\"],\n        ScalarInt,\n    ],\n    max_nodes: ScalarInt,\n) -&gt; tuple[Int[jnp.ndarray, \" D\"], ScalarInt, Int[jnp.ndarray, \" D\"]]:\n    \"\"\"\n    Convert a level-order expression of a tree to a pre-order expression.\n    Also returns the total number of nodes in the tree. The\n    pre-order expression is used to visualize the tree kernel.\n\n    NOTE: This function should be called via the get_pre_order method of the\n    TreeKernel.\n\n    Parameters\n    ----------\n    level_order : Int[jnp.ndarray, \" D\"]\n        The level-order expression of the tree.\n    initial_state : tuple\n        The initial state, containing the pre-order expression, the\n        pre-order pointer, the pre_level_map, the stack, and the stack pointer.\n    max_nodes : ScalarInt\n        The maximum number of nodes in the tree expression.\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The pre-order expression of the tree.\n    ScalarInt\n        The total number of nodes in the tree.\n    Int[jnp.ndarray, \" D\"]\n        A map from the pre-order index to the level-order index. The value at a given\n        index in the pre-order expression corresponds to the index of the same node in\n        the level-order expression.\n\n    \"\"\"\n\n    def traverse_tree(state: tuple) -&gt; tuple:\n        \"\"\"Traverse the tree expression in level-order and convert it to\n        pre-order.\"\"\"\n        pre_order, pre_order_pointer, pre_level_map, stack, stack_pointer = state\n\n        # pop the top of the stack\n        level_order_idx = stack[stack_pointer]\n        stack_pointer = stack_pointer - 1\n\n        # add the current node to the pre-order output\n        new_pre_order = pre_order.at[pre_order_pointer].set(\n            level_order[level_order_idx]\n        )\n        new_pre_order_pointer = pre_order_pointer + 1\n        # update the pre_level_map\n        new_pre_level_map = pre_level_map.at[pre_order_pointer].set(level_order_idx)\n\n        # check if the current node is a leaf\n        right_child_idx = get_child_idx(level_order_idx, \"l\")\n        is_leaf = jnp.logical_or(\n            right_child_idx &gt; max_nodes - 1,\n            level_order[right_child_idx] == -1,\n        )\n\n        def push(stack_and_pointer: tuple) -&gt; tuple:\n            \"\"\"If the node is not a leaf, push the new\n            child nodes onto the stack.\n            \"\"\"\n            stack, pointer = stack_and_pointer\n\n            left_child = get_child_idx(level_order_idx, \"l\")\n            right_child = get_child_idx(level_order_idx, \"r\")\n\n            stack_updated = stack.at[pointer + 1].set(right_child)\n            stack_updated = stack_updated.at[pointer + 2].set(left_child)\n\n            pointer_updated = pointer + 2\n            return stack_updated, pointer_updated\n\n        def dont_push(stack_and_pointer: tuple) -&gt; tuple:\n            \"\"\"If the node is a leaf, do nothing.\"\"\"\n            return stack_and_pointer\n\n        # update the stack based on whether the current node is an operator\n        new_stack, new_pointer = lax.cond(\n            is_leaf,\n            dont_push,\n            push,\n            operand=(stack, stack_pointer),\n        )\n\n        return (\n            new_pre_order,\n            new_pre_order_pointer,\n            new_pre_level_map,\n            new_stack,\n            new_pointer,\n        )\n\n    def condition(state: tuple) -&gt; ScalarBool:\n        \"\"\"Loop condition: Continue until stack is empty.\"\"\"\n        _, _, _, _, stack_pointer = state\n        return stack_pointer &gt;= 0\n\n    # iterate over the sample using lax.while_loop\n    final_pre_order, num_nodes, final_pre_level_map, _, _ = lax.while_loop(\n        condition,\n        traverse_tree,\n        initial_state,\n    )\n    return final_pre_order, num_nodes, final_pre_level_map\n</code></pre>"},{"location":"autoapi/gallifrey/kernels/tree/#gallifrey.kernels.tree.tree_visualization","title":"<code>tree_visualization(pre_order_expression, pre_level_map, atom_library, parameters, is_operator, root_idx, max_depth, include_parameters=True, print_str=True, return_str=False)</code>","text":"<p>Construct a visual representation of a tree expression.</p> <p>Parameters:</p> Name Type Description Default <code>pre_order_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression in pre-order traversal notation.</p> required <code>pre_level_map</code> <code>Int[ndarray, ' D']</code> <p>A map from the pre-order index to the level-order index. The value at a given index in the pre-order expression corresponds to the index of the same node in the level-order expression.</p> required <code>atom_library</code> <code>tuple[AbstractAtom | AbstractOperator, ...]</code> <p>A tuple of kernel atom functions and operators to evaluate the tree expression.</p> required <code>parameters</code> <code>Float[ndarray, ' M N']</code> <p>The parameters of the kernel functions in the tree kernel.</p> required <code>is_operator</code> <code>Bool[ndarray, ' D']</code> <p>A boolean array indicating whether a item in the atom library is an operator.</p> required <code>root_idx</code> <code>Int[ndarray, '']</code> <p>The index of the root node in the tree expression.</p> required <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the tree.</p> required <code>include_parameters</code> <code>bool</code> <p>Whether to include the kernel parameters in the string, by default True.</p> <code>True</code> <code>print_str</code> <code>bool</code> <p>Whether to print the visual representation of the tree, by default True.</p> <code>True</code> <code>return_str</code> <code>bool</code> <p>Whether to return the visual representation as a string, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>A string containing the visual representation of the tree expression (if return_str is True).</p> Source code in <code>gallifrey/kernels/tree.py</code> <pre><code>def tree_visualization(\n    pre_order_expression: Int[jnp.ndarray, \" D\"],\n    pre_level_map: Int[jnp.ndarray, \" D\"],\n    atom_library: tuple[AbstractAtom | AbstractOperator, ...],\n    parameters: Float[jnp.ndarray, \" M N\"],\n    is_operator: Bool[jnp.ndarray, \" D\"],\n    root_idx: Int[jnp.ndarray, \"\"],\n    max_depth: ScalarInt,\n    include_parameters: bool = True,\n    print_str: bool = True,\n    return_str: bool = False,\n) -&gt; None | str:\n    \"\"\"\n    Construct a visual representation of a tree expression.\n\n    Parameters\n    ----------\n    pre_order_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression in pre-order traversal notation.\n    pre_level_map : Int[jnp.ndarray, \" D\"]\n        A map from the pre-order index to the level-order index. The value at a given\n        index in the pre-order expression corresponds to the index of the same node in\n        the level-order expression.\n    atom_library : tuple[AbstractAtom | AbstractOperator, ...]\n        A tuple of kernel atom functions and operators to evaluate the tree expression.\n    parameters : Float[jnp.ndarray, \" M N\"]\n        The parameters of the kernel functions in the tree kernel.\n    is_operator : Bool[jnp.ndarray, \" D\"]\n        A boolean array indicating whether a item in the atom library is an operator.\n    root_idx : Int[jnp.ndarray, \"\"]\n        The index of the root node in the tree expression.\n    max_depth : ScalarInt\n        The maximum depth of the tree.\n    include_parameters : bool, optional\n        Whether to include the kernel parameters in the string, by default True.\n    print_str : bool, optional\n        Whether to print the visual representation of the tree, by default True.\n    return_str : bool, optional\n        Whether to return the visual representation as a string, by default False\n\n    Returns\n    -------\n    str\n        A string containing the visual representation of the tree expression (if\n        return_str is True).\n    \"\"\"\n    descr_str = \"\"\n\n    for pre_order_idx, node_value in enumerate(pre_order_expression):\n        tree_level_idx = pre_level_map[pre_order_idx]\n        atom = atom_library[node_value]\n\n        depth = get_depth(tree_level_idx) - get_depth(root_idx)\n        prefix = \"    \" * (depth - 1) + \"\u2514\u2500\u2500 \" if depth &gt; 0 else \"\"\n\n        node_representation = f\"{atom.name}\"\n\n        if not is_operator[node_value] and include_parameters:\n            node_parameters = parameters[\n                get_parameter_leaf_idx(tree_level_idx, max_depth)\n            ]\n            parameter = node_parameters[: atom.num_parameter]\n            node_representation += f\": {parameter}\"\n\n        descr_str += f\"{prefix}{node_representation}\\n\"\n\n    if print_str:\n        print(descr_str)\n    if return_str:\n        return descr_str\n    return None\n</code></pre>"},{"location":"autoapi/gallifrey/moves/","title":"moves","text":""},{"location":"autoapi/gallifrey/moves/acceptance_probability/","title":"acceptance_probability","text":""},{"location":"autoapi/gallifrey/moves/acceptance_probability/#gallifrey.moves.acceptance_probability.calculate_acceptance_prob_contribution","title":"<code>calculate_acceptance_prob_contribution(particle_state, proposed_noise_variance, kernel_prior, noise_prior, data, fix_noise, verbosity=0)</code>","text":"<p>Calculate the contribution to the acceptance probability for a specific particle state and noise variance proposal.</p> <p>Parameters:</p> Name Type Description Default <code>particle_state</code> <code>State</code> <p>The state of the particle.</p> required <code>proposed_noise_variance</code> <code>ScalarFloat</code> <p>The noise variance of the proposed state. (which is not the current state!)</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior.</p> required <code>noise_prior</code> <code>Distribution</code> <p>The prior distribution of the noise variance.</p> required <code>data</code> <code>Dataset</code> <p>The observational data.</p> required <code>fix_noise</code> <code>bool</code> <p>Whether to fix the noise variance is fixed or not.</p> required <code>verbosity</code> <code>int</code> <p>The verbosity level, by default 0. Debugging information is printed if <code>verbosity &gt; 1</code>.</p> <code>0</code> <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The contribution to the acceptance probability for the given particle state and noise variance proposal.</p> Source code in <code>gallifrey/moves/acceptance_probability.py</code> <pre><code>@partial(\n    jit,\n    static_argnames=(\n        \"kernel_prior\",\n        \"data\",\n        \"fix_noise\",\n        \"verbosity\",\n    ),\n)\ndef calculate_acceptance_prob_contribution(\n    particle_state: nnx.State,\n    proposed_noise_variance: ScalarFloat,\n    kernel_prior: KernelPrior,\n    noise_prior: Distribution,\n    data: Dataset,\n    fix_noise: bool,\n    verbosity: int = 0,\n) -&gt; ScalarFloat:\n    \"\"\"\n    Calculate the contribution to the acceptance probability\n    for a specific particle state and noise variance proposal.\n\n    Parameters\n    ----------\n    particle_state : nnx.State\n        The state of the particle.\n    proposed_noise_variance : ScalarFloat\n        The noise variance of the proposed state. (which is not\n        the current state!)\n    kernel_prior : KernelPrior\n        The kernel prior.\n    noise_prior : Distribution\n        The prior distribution of the noise variance.\n    data : Dataset\n        The observational data.\n    fix_noise : bool\n        Whether to fix the noise variance is fixed or not.\n    verbosity : int, optional\n        The verbosity level, by default 0. Debugging information\n        is printed if `verbosity &gt; 1`.\n\n    Returns\n    -------\n    ScalarFloat\n        The contribution to the acceptance probability for the\n        given particle state and noise variance proposal.\n\n    \"\"\"\n\n    # unpack particle state\n    kernel_state = particle_state.kernel\n    current_noise_variance = jnp.asarray(particle_state.noise_variance.value)\n\n    kernel = kernel_prior.reconstruct_kernel(kernel_state)\n\n    # calculate gram matrix for kernel\n    kernel_gram = kernel._gram_train(data.x)\n\n    # calculate acceptance probability terms\n    log_tree_size = jnp.log(kernel.node_sizes[0])\n    marginal_log_likelihood = calculate_marginal_log_likelihood(\n        kernel_gram,\n        current_noise_variance,\n        data,\n    )\n    log_prob_noise_prior = lax.cond(\n        fix_noise,\n        lambda x: jnp.array(0.0),\n        lambda x: jnp.asarray(\n            noise_prior.log_prob(x),\n            dtype=data.x.dtype,\n        ).squeeze(),\n        current_noise_variance,\n    )\n\n    # calculate noise posterior probability\n    # NOTE: We have to calculate the probability of the current\n    # noise variance coming from the proposed noise variance,\n    # so the two are swapped in the function call.\n    # (eq 21 and 22 in Saad2023)\n    log_prob_noise = lax.cond(\n        fix_noise,\n        lambda: jnp.array(0.0),\n        lambda: noise_variance_probability(\n            kernel_gram=kernel_gram,\n            current_noise_variance=proposed_noise_variance,\n            proposed_noise_variance=current_noise_variance,\n            data=data,\n        ),\n    )\n    # calculate acceptance probability contribution,\n    # (eq 22. and Preposition 2 in Saad2023)\n    log_acceptance_prob_contribution = jnp.array(\n        [\n            marginal_log_likelihood,\n            -log_tree_size,\n            log_prob_noise_prior,\n            -log_prob_noise,\n        ]\n    ).sum()\n\n    if verbosity &gt; 1:\n        debug.print(\"Current Terms:\")\n        debug.print(\"Marginal log likelihood: {}\", marginal_log_likelihood)\n        debug.print(\"Log (tree size^-1): {}\", -log_tree_size)\n        debug.print(\"Log noise prior probability: {}\", log_prob_noise_prior)\n        debug.print(\"Log (probability of noise posterior^-1): {}\", -log_prob_noise)\n        debug.print(\"Sum of terms: {}\", log_acceptance_prob_contribution)\n        debug.print(\"-\" * 50)\n\n    return log_acceptance_prob_contribution\n</code></pre>"},{"location":"autoapi/gallifrey/moves/acceptance_probability/#gallifrey.moves.acceptance_probability.calculate_acceptance_probability","title":"<code>calculate_acceptance_probability(hastings_ratio_numerator, hastings_ratio_denominator)</code>","text":"<p>Calculate the acceptance ratio for a given pair of hastings ratio terms. In the context of the particle move, the numerator is associated with the proposed state, and the denominator with the current state.</p> <p>Parameters:</p> Name Type Description Default <code>hastings_ratio_numerator</code> <code>ScalarFloat</code> <p>The hastings ratio contribution for the proposed state.</p> required <code>hastings_ratio_denominator</code> <code>ScalarFloat</code> <p>The hastings ratio contribution for the current state.</p> required <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The MCMC acceptance probability.</p> Source code in <code>gallifrey/moves/acceptance_probability.py</code> <pre><code>@jit\ndef calculate_acceptance_probability(\n    hastings_ratio_numerator: ScalarFloat,\n    hastings_ratio_denominator: ScalarFloat,\n) -&gt; ScalarFloat:\n    \"\"\"\n    Calculate the acceptance ratio for a given pair of\n    hastings ratio terms.\n    In the context of the particle move, the numerator\n    is associated with the proposed state, and the denominator\n    with the current state.\n\n    Parameters\n    ----------\n    hastings_ratio_numerator : ScalarFloat\n        The hastings ratio contribution for the proposed state.\n    hastings_ratio_denominator : ScalarFloat\n        The hastings ratio contribution for the current state.\n    Returns\n    -------\n    ScalarFloat\n        The MCMC acceptance probability.\n    \"\"\"\n\n    # dealing with NaNs in such a way that the kernel with NaN is rejected,\n    # if both are NaN (shouldn't happen), accept proposal and hope for the best\n    # NOTE: in general, NaNs should not occur, but they can occur in the\n    # kernel gram matrix if the kernel is not positive definite (e.g. due to\n    # numerical instabilities)\n\n    log_hastings_ratio = jnp.nan_to_num(\n        hastings_ratio_numerator, nan=-jnp.inf\n    ) - jnp.nan_to_num(hastings_ratio_denominator, nan=-jnp.inf)\n    log_hastings_ratio = jnp.nan_to_num(log_hastings_ratio, nan=0.0)\n\n    acceptance_probability = jnp.clip(\n        jnp.exp(log_hastings_ratio),\n        max=1.0,\n    )\n\n    return acceptance_probability\n</code></pre>"},{"location":"autoapi/gallifrey/moves/detach_attach/","title":"detach_attach","text":""},{"location":"autoapi/gallifrey/moves/detach_attach/#gallifrey.moves.detach_attach.attach_move","title":"<code>attach_move(key, kernel_state, kernel_prior, verbosity=0)</code>","text":"<p>Perform attach move. In this move, a subtree is detached from the tree expression (at idx a), a scaffold is attached to the tree expression (at idx a), and the subtree originally at idx a is reattached to the scaffold (at idx b).</p> <p>Also returns the probabilities associated with the proposals q_A (going from the original tree to the new tree using the attach move) and q_D (going from the new tree to the original tree using a detach move).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key.</p> required <code>kernel_state</code> <code>State</code> <p>The original kernel state, must have attributes: - parameters - leaf_level_map - node_sizes - tree_expression - is_operator</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The prior to sample the new subtree from.</p> required <code>verbosity</code> <code>int</code> <p>The verbosity level, by default 0. Debug information is printed if verbosity &gt; 2.</p> <code>0</code> <p>Returns:</p> Type Description <code>State</code> <p>The updated kernel state after the move.</p> <code>ScalarFloat</code> <p>The probability associated with the proposal q_D (b|a, k', theta'), i.e. the transition probability from the new tree to the original tree using a detach move.</p> <code>ScalarFloat</code> <p>The probability associated with the proposal q_A (b|a, k, theta), i.e. going from the original tree to the new tree using the attach move.</p> Source code in <code>gallifrey/moves/detach_attach.py</code> <pre><code>@partial(jit, static_argnames=(\"kernel_prior\", \"verbosity\"))\ndef attach_move(\n    key: PRNGKeyArray,\n    kernel_state: nnx.State,\n    kernel_prior: KernelPrior,\n    verbosity: int = 0,\n) -&gt; tuple[nnx.State, ScalarFloat, ScalarFloat]:\n    \"\"\"\n    Perform attach move. In this move, a subtree is detached from the\n    tree expression (at idx a), a scaffold is attached to the tree\n    expression (at idx a), and the subtree originally at idx a is\n    reattached to the scaffold (at idx b).\n\n    Also returns the probabilities associated with the proposals q_A\n    (going from the original tree to the new tree using the attach move)\n    and q_D (going from the new tree to the original tree using a detach move).\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key.\n    kernel_state : nnx.State\n        The original kernel state, must have attributes:\n        - parameters\n        - leaf_level_map\n        - node_sizes\n        - tree_expression\n        - is_operator\n    kernel_prior : KernelPrior\n        The prior to sample the new subtree from.\n    verbosity : int, optional\n        The verbosity level, by default 0. Debug information is printed\n        if verbosity &gt; 2.\n\n\n    Returns\n    -------\n    nnx.State\n        The updated kernel state after the move.\n    ScalarFloat\n        The probability associated with the proposal q_D (b|a, k', theta'),\n        i.e. the transition probability from the new tree to the original tree\n        using a detach move.\n    ScalarFloat\n        The probability associated with the proposal q_A (b|a, k, theta),\n        i.e. going from the original tree to the new tree using the attach move.\n\n    \"\"\"\n    key, parameter_key, structure_key = jr.split(key, 3)\n\n    # get the new structure and the mapping between the old and new indices\n    new_structure, changes, index_mapping, log_p_path, log_p_scaffold, idx_a, _ = (\n        structure_attach_move(\n            structure_key,\n            kernel_state.tree_expression.value,  # type: ignore\n            kernel_state.post_level_map.value,  # type: ignore\n            kernel_state.node_heights.value,  # type: ignore\n            kernel_prior,\n            verbosity=verbosity,\n        )\n    )\n\n    # get attributes of new tree\n    new_kernel = nnx.merge(kernel_prior.graphdef, kernel_state)\n    new_kernel.init_tree(\n        new_structure,\n        kernel_prior.kernel_library,\n        kernel_prior.max_depth,\n        kernel_prior.num_datapoints,\n    )\n\n    # move parameters of subtree from old indices to new indices\n    _, new_kernel_state = nnx.split(new_kernel)\n    new_kernel_state = move_parameters(\n        new_kernel_state,\n        index_mapping.T,\n        kernel_prior.max_depth,\n    )\n\n    # sample new kernel parameters, but first remove the nodes\n    # from changes that were already moved to a new place (since we\n    # want to keep the parameters of the moved nodes and only sample\n    # parameters for completely new nodes introduced by scaffold)\n    changes = jnp.where(\n        jnp.isin(changes, index_mapping[1]),\n        -1,\n        changes,\n    )\n\n    new_kernel_state, log_prob_parameter = kernel_prior.parameter_prior.sample_subset(\n        parameter_key,\n        new_kernel_state,\n        considered_nodes=changes,\n    )\n    # calculate q_A (b|a, k, theta), i.e. probability associated with the\n    # proposal going from the original tree to the new tree using the attach move\n    log_q_A = log_p_path + log_p_scaffold + log_prob_parameter\n\n    # calculate q_D (b|a, k', theta'), i.e. the transition probability\n    # from the new tree to the original tree using a detach move\n    log_q_D = jnp.log(1 / new_kernel.node_sizes.value[idx_a])\n\n    return new_kernel_state, log_q_D, log_q_A\n</code></pre>"},{"location":"autoapi/gallifrey/moves/detach_attach/#gallifrey.moves.detach_attach.detach_attach_move","title":"<code>detach_attach_move(key, kernel_state, kernel_prior, detach_prob=0.5, verbosity=0)</code>","text":"<p>Perform detach-attach move. In this move, a subtree is detached from the tree expression (at idx a), and a scaffold is attached to the tree expression (at idx a). The subtree is then reattached to the scaffold.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key.</p> required <code>kernel_state</code> <code>State</code> <p>The original kernel state, must have attributes: - parameters - leaf_level_map - node_sizes - tree_expression - is_operator</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior object that contains the kernel structure prior.</p> required <code>detach_prob</code> <code>ScalarFloat</code> <p>The probability of performing the detach move, by default 0.5.</p> <code>0.5</code> <code>verbosity</code> <code>int</code> <p>The verbosity level, by default 0. Debug information is printed if verbosity &gt; 2.</p> <code>0</code> <p>Returns:</p> Type Description <code>State</code> <p>The updated kernel state after the move.</p> <code>ScalarFloat</code> <p>The log acceptance ratio contribution of the move, needs to be added to the log acceptance ratio of the subtree-replace move.</p> Source code in <code>gallifrey/moves/detach_attach.py</code> <pre><code>@partial(jit, static_argnames=(\"kernel_prior\", \"verbosity\"))\ndef detach_attach_move(\n    key: PRNGKeyArray,\n    kernel_state: nnx.State,\n    kernel_prior: KernelPrior,\n    detach_prob: ScalarFloat = 0.5,\n    verbosity: int = 0,\n) -&gt; tuple[nnx.State, ScalarFloat]:\n    \"\"\"\n    Perform detach-attach move. In this move, a subtree is detached\n    from the tree expression (at idx a), and a scaffold is attached\n    to the tree expression (at idx a). The subtree is then reattached\n    to the scaffold.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key.\n    kernel_state : nnx.State\n        The original kernel state, must have attributes:\n        - parameters\n        - leaf_level_map\n        - node_sizes\n        - tree_expression\n        - is_operator\n    kernel_prior : KernelPrior\n        The kernel prior object that contains the kernel structure prior.\n    detach_prob : ScalarFloat, optional\n        The probability of performing the detach move, by default 0.5.\n    verbosity : int, optional\n        The verbosity level, by default 0. Debug information is printed\n        if verbosity &gt; 2.\n\n    Returns\n    -------\n    nnx.State\n        The updated kernel state after the move.\n    ScalarFloat\n        The log acceptance ratio contribution of the move,\n        needs to be added to the log acceptance ratio of the\n        subtree-replace move.\n\n    \"\"\"\n    key, selection_subkey, move_subkey = jr.split(key, 3)\n\n    # Cannot perform detach-attach move on kernels with max depth = 1.\n    perform_detach = jr.bernoulli(\n        selection_subkey,\n        jnp.where(\n            kernel_state.node_sizes.value[0] == 1,\n            jnp.array(0.0),\n            jnp.asarray(detach_prob),\n        ),\n    )\n    if verbosity &gt; 2:\n        lax.cond(\n            perform_detach,\n            lambda _: debug.print(\"Performing detach move.\"),\n            lambda _: debug.print(\"Performing attach move.\"),\n            perform_detach,\n        )\n    # ratio of the probabilities of proposing detach and attach moves,\n    # needed for acceptance ratio calculation (Saad2023 - Proposition 2)\n    log_detach_vs_attach_prob = jnp.log(1 - detach_prob) - jnp.log(detach_prob)\n\n    # perform detach or attach move\n    new_kernel_state, log_q_D, log_q_A = lax.cond(\n        perform_detach,\n        lambda key: detach_move(key, kernel_state, kernel_prior, verbosity=verbosity),\n        lambda key: attach_move(key, kernel_state, kernel_prior, verbosity=verbosity),\n        move_subkey,\n    )\n\n    # calculate the log acceptance ratio contribution for the move\n    # (Saad2023 - Proposition 2)\n    log_acceptance_ratio_contribution = lax.cond(\n        perform_detach,\n        lambda log_p: log_p,\n        lambda log_p: -log_p,\n        log_q_A - log_q_D + log_detach_vs_attach_prob,\n    )\n\n    return new_kernel_state, log_acceptance_ratio_contribution\n</code></pre>"},{"location":"autoapi/gallifrey/moves/detach_attach/#gallifrey.moves.detach_attach.detach_move","title":"<code>detach_move(key, kernel_state, kernel_prior, verbosity=0)</code>","text":"<p>Perform detach move. In this move, a scaffold is detached from the tree expression (at idx a), and a subtree from the scaffold (at idx b) is detached and then reattached to the root (idx a) of the scaffold.</p> <p>Also returns the probabilities associated with the proposals q_D (going from the original tree to the new tree using the detach move) and q_A (going from the new tree to the original tree using an attach move).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key.</p> required <code>kernel_state</code> <code>State</code> <p>The original kernel state, must have attributes: - parameters - leaf_level_map - node_sizes - tree_expression - is_operator</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The prior to sample the new subtree from.</p> required <code>verbosity</code> <code>int</code> <p>The verbosity level, by default 0. Debug information is printed if verbosity &gt; 2.</p> <code>0</code> <p>Returns:</p> Type Description <code>State</code> <p>The updated kernel state after the move.</p> <code>ScalarFloat</code> <p>The probability associated with the proposal q_D (b|a, k, theta), i.e. going from the original tree to the new tree using the detach move.</p> <code>ScalarFloat</code> <p>The probability associated with the proposal q_A (b|a, k', theta'), i.e. the transition probability from the new tree to the original tree using an attach move.</p> Source code in <code>gallifrey/moves/detach_attach.py</code> <pre><code>@partial(jit, static_argnames=(\"kernel_prior\", \"verbosity\"))\ndef detach_move(\n    key: PRNGKeyArray,\n    kernel_state: nnx.State,\n    kernel_prior: KernelPrior,\n    verbosity: int = 0,\n) -&gt; tuple[nnx.State, ScalarFloat, ScalarFloat]:\n    \"\"\"\n    Perform detach move. In this move, a scaffold is detached\n    from the tree expression (at idx a), and a subtree from the\n    scaffold (at idx b) is detached and then reattached to the\n    root (idx a) of the scaffold.\n\n    Also returns the probabilities associated with the proposals q_D\n    (going from the original tree to the new tree using the detach move)\n    and q_A (going from the new tree to the original tree using an attach\n    move).\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key.\n    kernel_state : nnx.State\n        The original kernel state, must have attributes:\n        - parameters\n        - leaf_level_map\n        - node_sizes\n        - tree_expression\n        - is_operator\n    kernel_prior : KernelPrior\n        The prior to sample the new subtree from.\n    verbosity : int, optional\n        The verbosity level, by default 0. Debug information is printed\n        if verbosity &gt; 2.\n\n    Returns\n    -------\n    nnx.State\n        The updated kernel state after the move.\n    ScalarFloat\n        The probability associated with the proposal q_D (b|a, k, theta),\n        i.e. going from the original tree to the new tree using the detach move.\n    ScalarFloat\n        The probability associated with the proposal q_A (b|a, k', theta'),\n        i.e. the transition probability from the new tree to the original tree\n        using an attach move.\n\n    \"\"\"\n    # get the new structure and the mapping between the old and new indices\n    new_structure, index_mapping, idx_a, idx_b = structure_detach_move(\n        key,\n        kernel_state.tree_expression.value,  # type: ignore\n        kernel_state.post_level_map.value,  # type: ignore\n        verbosity=verbosity,\n    )\n\n    # get attributes of new tree\n    new_kernel = kernel_prior.reconstruct_kernel(kernel_state)\n    new_kernel.init_tree(\n        new_structure,\n        kernel_prior.kernel_library,\n        kernel_prior.max_depth,\n        kernel_prior.num_datapoints,\n    )\n\n    # move parameters from old indices to new indices\n    _, new_kernel_state = nnx.split(new_kernel)\n    new_kernel_state = move_parameters(\n        new_kernel_state,\n        index_mapping.T,\n        kernel_prior.max_depth,\n    )\n\n    # calculate q_D (b|a, k, theta), i.e. the probability associated with the\n    # detach move going from the original tree to the new tree using the detach move\n    # (which is equal to the probability of sampling idx_b given idx_a\n    # was sampled, which is simply the inverse of the number of nodes in the\n    # subtree, assuming uniform sampling)\n    log_q_D = jnp.log(1 / kernel_state.node_sizes.value[idx_a])  # type: ignore\n\n    # calculate q_A (b|a, k', theta'), i.e. involution attach move\n    # going from the new tree to the original tree using an attach move\n    path, log_p_path = reconstruct_path(\n        idx_a,\n        idx_b,\n        max_depth=kernel_prior.kernel_structure_prior.max_depth,\n    )\n    # we use the original kernel here, since we are calculating the probability\n    # of sampling exactly the (now) missing scaffold, so we have to traverse\n    # the scaffold from the original kernel\n    log_p_scaffold = kernel_prior.kernel_structure_prior.log_prob_single(\n        kernel_state.tree_expression.value,  # type: ignore\n        path_to_hole=path,\n        hole_idx=idx_b,\n    )\n    log_q_A = log_p_path + log_p_scaffold\n\n    return new_kernel_state, log_q_D, log_q_A\n</code></pre>"},{"location":"autoapi/gallifrey/moves/detach_attach/#gallifrey.moves.detach_attach.detach_subtree","title":"<code>detach_subtree(tree_expression, subtree_root_idx, fill_value=-1)</code>","text":"<p>Cut a subtree from a tree expression, starting at the subtree root node.</p> <p>Returns the tree_expression with the subtree removed, the subtree expression, and the indices of the nodes in the subtree.</p> <p>Parameters:</p> Name Type Description Default <code>tree_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression, given in level order notation.</p> required <code>subtree_root_idx</code> <code>ScalarInt</code> <p>The index of node where the subtree is rooted.</p> required <code>fill_value</code> <code>ScalarInt</code> <p>The value to fill the hole with in the subtree, by default -1. (Must be negative.)</p> <code>-1</code> <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The (level order) tree expression with the subtree removed.</p> <code>Int[ndarray, ' D']</code> <p>The (level order) tree expression of the subtree, same length as the input tree expression.</p> <code>Int[ndarray, ' D']</code> <p>The indices of the nodes in the subtree.</p> Source code in <code>gallifrey/moves/detach_attach.py</code> <pre><code>def detach_subtree(\n    tree_expression: Int[jnp.ndarray, \" D\"],\n    subtree_root_idx: ScalarInt,\n    fill_value: ScalarInt = -1,\n) -&gt; tuple[\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n]:\n    \"\"\"\n    Cut a subtree from a tree expression, starting at the subtree root\n    node.\n\n    Returns the tree_expression with the subtree removed, the subtree\n    expression, and the indices of the nodes in the subtree.\n\n    Parameters\n    ----------\n    tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression, given in level order notation.\n    subtree_root_idx : ScalarInt\n        The index of node where the subtree is rooted.\n    fill_value : ScalarInt, optional\n        The value to fill the hole with in the subtree, by default -1.\n        (Must be negative.)\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The (level order) tree expression with the subtree removed.\n    Int[jnp.ndarray, \" D\"]\n        The (level order) tree expression of the subtree, same length as the\n        input tree expression.\n    Int[jnp.ndarray, \" D\"]\n        The indices of the nodes in the subtree.\n\n    \"\"\"\n    max_nodes = tree_expression.size\n\n    # if tree_expression[subtree_root_idx] == -1:\n    #     raise ValueError(\"The start index must be a non-empty node.\")\n    # if fill_value &gt;= 0:\n    #     raise ValueError(\"The fill value must be negative.\")\n\n    # create initial state\n    subtree_expression = jnp.full(max_nodes, -1)\n    index_array = jnp.copy(subtree_expression)\n    index_pointer = 0\n    stack = jnp.copy(subtree_expression).at[0].set(subtree_root_idx)\n    stack_pointer = 0\n\n    initial_state = (\n        tree_expression,\n        subtree_expression,\n        index_array,\n        index_pointer,\n        stack,\n        stack_pointer,\n    )\n\n    tree_expression, subtree_expression, index_array, _ = _detach_subtree(\n        initial_state,\n        max_nodes,\n        fill_value,\n    )\n\n    return tree_expression, subtree_expression, index_array\n</code></pre>"},{"location":"autoapi/gallifrey/moves/detach_attach/#gallifrey.moves.detach_attach.scaffold_proposal","title":"<code>scaffold_proposal(key, max_depth, path_to_hole, hole_idx, probs, is_operator, empty_value=-1)</code>","text":"<p>Propose the tree structure for the scaffold in the attach move. Also returns the log-probability of the proposal.</p> <p>The scaffold is a tree structure proposal very similar to the samples created from the kernel prior (see gallifrey.kernels.prior.TreeStructurePrior). The difference is that this prior is conditioned by the path to the hole, meaning some branches are fixed to be operator nodes, so that the hole can be reached. The root of the scaffold is assumed to be first index in the path_to_hole.</p> <p>The hole itself is fixed to be a leaf node, and empty (filled by the empty_value) in the scaffold. NOTE: This means the returned scaffold is not a valid kernel structure in itself, the hole must be filled by a valid subtree.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>Random key for sampling.</p> required <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the nested kernel structure tree.</p> required <code>path_to_hole</code> <code> Int[jnp.ndarray, \" D\"]</code> <p>The path to the hole in the tree. A list of indices that describe the path from the root to the hole (level order indices).</p> required <code>hole_idx</code> <code>ScalarInt</code> <p>The index of the hole in the tree (level order index).</p> required <code>probs</code> <code> Float[jnp.ndarray, \" D\"]</code> <p>The probabilities of sampling each kernel in the library.</p> required <code>is_operator</code> <code>Bool[ndarray, ' D']</code> <p>An array that indicates whether each kernel in the library is an operator.</p> required <code>empty_value</code> <code>ScalarInt</code> <p>The value to fill the hole with in the scaffold, by default -1.</p> <code>-1</code> <p>Returns:</p> Type Description <code> Int[jnp.ndarray, \" D\"]</code> <p>An array that describes the scaffold structure.</p> <code>ScalarFloat</code> <p>The log probability associated with the scaffold.</p> Source code in <code>gallifrey/moves/detach_attach.py</code> <pre><code>def scaffold_proposal(\n    key: PRNGKeyArray,\n    max_depth: ScalarInt,\n    path_to_hole: Int[jnp.ndarray, \" D\"],\n    hole_idx: ScalarInt,\n    probs: Float[jnp.ndarray, \" D\"],\n    is_operator: Bool[jnp.ndarray, \" D\"],\n    empty_value: ScalarInt = -1,\n) -&gt; tuple[\n    Int[jnp.ndarray, \" D\"],\n    ScalarFloat,\n]:\n    \"\"\"\n    Propose the tree structure for the scaffold in the\n    attach move. Also returns the log-probability of the proposal.\n\n    The scaffold is a tree structure proposal very similar\n    to the samples created from the kernel prior (see\n    gallifrey.kernels.prior.TreeStructurePrior). The difference\n    is that this prior is conditioned by the path to the hole, meaning\n    some branches are fixed to be operator nodes, so that the hole can\n    be reached.\n    The root of the scaffold is assumed to be first index in the path_to_hole.\n\n    The hole itself is fixed to be a leaf node, and empty (filled by the\n    empty_value) in the scaffold.\n    NOTE: This means the returned scaffold is not a valid kernel structure\n    in itself, the hole must be filled by a valid subtree.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        Random key for sampling.\n    max_depth : ScalarInt\n        The maximum depth of the nested kernel structure tree.\n    path_to_hole :  Int[jnp.ndarray, \" D\"]\n        The path to the hole in the tree. A list of indices that\n        describe the path from the root to the hole (level order\n        indices).\n    hole_idx : ScalarInt\n        The index of the hole in the tree (level order index).\n    probs :  Float[jnp.ndarray, \" D\"]\n        The probabilities of sampling each kernel in the library.\n    is_operator : Bool[jnp.ndarray, \" D\"]\n        An array that indicates whether each kernel in the library is an operator.\n    empty_value : ScalarInt, optional\n        The value to fill the hole with in the scaffold, by default -1.\n\n    Returns\n    -------\n     Int[jnp.ndarray, \" D\"]\n        An array that describes the scaffold structure.\n    ScalarFloat\n        The log probability associated with the scaffold.\n    \"\"\"\n    max_nodes = calculate_max_nodes(max_depth)\n\n    # create sample array to be filled, this will be the output (empty\n    # nodes are labeled -1)\n    sample = jnp.full(max_nodes, -1)\n    # create initial stack: empty except for the root node, start at beginning of path\n    initial_stack = jnp.copy(sample).at[0].set(path_to_hole[0])\n    pointer = 0  # initial position of the stack pointer\n    initial_log_p = 0.0  # initial probability\n\n    initial_state = (key, sample, initial_stack, pointer, initial_log_p)\n\n    return _scaffold_proposal(\n        initial_state,\n        probs,\n        is_operator,\n        path_to_hole,\n        hole_idx,\n        max_depth,\n        empty_value,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/moves/detach_attach/#gallifrey.moves.detach_attach.structure_attach_move","title":"<code>structure_attach_move(key, tree_expression, post_level_map, node_heights, kernel_prior, verbosity=0)</code>","text":"<p>Perform attach move on the kernel structure array. In this move, a subtree is detached from the tree expression (at idx a), a scaffold is attached to the tree expression (at idx a), and the subtree originally at idx a is reattached to the scaffold (at idx b).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key.</p> required <code>tree_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression, given in level order notation.</p> required <code>post_level_map</code> <code>Int[ndarray, ' D']</code> <p>The array that maps the post order index to the level order index. (This is useful, since the post_level_map contains all the indices of the nodes in the tree, which we can sample).</p> required <code>node_heights</code> <code>Int[ndarray, ' D']</code> <p>The heights of each node in the tree. (See gallifrey.kernels.tree.TreeKernel for more information.)</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior object that contains the kernel structure prior.</p> required <code>verbosity</code> <code>int</code> <p>The verbosity level, by default 0. Debug information is printed if verbosity &gt; 2.</p> <code>0</code> <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The new tree expression with the scaffold attached and the subtree reattached.</p> <code>Int[ndarray, ' D']</code> <p>The changes in the tree structure, a list of indices that differ between the original tree and the new tree.</p> <code>Int[ndarray, '2 D']</code> <p>The mapping between the old and new indices of the nodes in the subtree. The first column contains the old indices, and the second column contains the new indices.</p> <code>ScalarFloat</code> <p>The log probability associated with the path to the hole in the scaffold.</p> <code>ScalarFloat</code> <p>The log probability associated with the scaffold.</p> <code>ScalarInt</code> <p>The index of the root of the subtree that was detached/scaffold attached (idx a).</p> <code>ScalarInt</code> <p>The index where the subtree was reattached (idx b).</p> Source code in <code>gallifrey/moves/detach_attach.py</code> <pre><code>def structure_attach_move(\n    key: PRNGKeyArray,\n    tree_expression: Int[jnp.ndarray, \" D\"],\n    post_level_map: Int[jnp.ndarray, \" D\"],\n    node_heights: Int[jnp.ndarray, \" D\"],\n    kernel_prior: KernelPrior,\n    verbosity: int = 0,\n) -&gt; tuple[\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \"2 D\"],\n    ScalarFloat,\n    ScalarFloat,\n    ScalarInt,\n    ScalarInt,\n]:\n    \"\"\"\n    Perform attach move on the kernel structure array. In this move,\n    a subtree is detached from the tree expression (at idx a), a\n    scaffold is attached to the tree expression (at idx a), and the\n    subtree originally at idx a is reattached to the scaffold (at idx b).\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key.\n    tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression, given in level order notation.\n    post_level_map : Int[jnp.ndarray, \" D\"]\n        The array that maps the post order index to the level order index.\n        (This is useful, since the post_level_map contains all the indices\n        of the nodes in the tree, which we can sample).\n    node_heights : Int[jnp.ndarray, \" D\"]\n        The heights of each node in the tree. (See\n        gallifrey.kernels.tree.TreeKernel for more information.)\n    kernel_prior : KernelPrior\n        The kernel prior object that contains the kernel structure prior.\n    verbosity : int, optional\n        The verbosity level, by default 0. Debug information is printed\n        if verbosity &gt; 2.\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The new tree expression with the scaffold attached and the subtree\n        reattached.\n    Int[jnp.ndarray, \" D\"]\n        The changes in the tree structure, a list of indices that differ\n        between the original tree and the new tree.\n    Int[jnp.ndarray, \"2 D\"]\n        The mapping between the old and new indices of the nodes in the\n        subtree. The first column contains the old indices, and the second\n        column contains the new indices.\n    ScalarFloat\n        The log probability associated with the path to the hole in the\n        scaffold.\n    ScalarFloat\n        The log probability associated with the scaffold.\n    ScalarInt\n        The index of the root of the subtree that was detached/scaffold\n        attached (idx a).\n    ScalarInt\n        The index where the subtree was reattached (idx b).\n    \"\"\"\n    key, idx_a_subkey, path_subkey, scaffold_subkey = jr.split(key, 4)\n\n    # select where the scaffold will be attached\n    idx_a = pick_random_node(idx_a_subkey, post_level_map)\n\n    # generate a random path to the hole in the scaffold (this is where\n    # the subtree originally at idx_a will be re-attached),\n    # the path needs to respect the node height of the detached subtree,\n    # since it shouldn't overshoot the max depth when reattached\n    idx_b, path, path_log_prob = generate_random_path(\n        path_subkey,\n        idx_a,\n        max_depth=kernel_prior.max_depth,\n        node_height=node_heights[idx_a],\n    )\n\n    scaffold, scaffold_log_p = scaffold_proposal(\n        key,\n        max_depth=kernel_prior.kernel_structure_prior.max_depth,\n        path_to_hole=path,\n        hole_idx=idx_b,\n        probs=kernel_prior.kernel_structure_prior.probs,\n        is_operator=kernel_prior.kernel_structure_prior.is_operator,\n    )\n\n    # transplant the subtree from the original tree at idx_a to the scaffold\n    # at idx_b\n    scaffold_with_subtree, index_mapping = transplant_subtree(\n        tree_expression,\n        scaffold,\n        idx_a,\n        idx_b,\n        int(calculate_max_nodes(kernel_prior.max_depth)),\n    )\n\n    # replace the subtree at idx_a with the scaffold + subtree at idx_b\n    new_tree_expression, changes = replace_subtree_structure_with(\n        tree_expression,\n        scaffold_with_subtree,\n        idx_a,\n    )\n\n    if verbosity &gt; 2:\n        debug.print(\"idx_a: {}\", idx_a)\n        debug.print(\"idx_b: {}\", idx_b)\n        debug.print(\"path: {}\", path)\n        debug.print(\"scaffold: {}\", scaffold)\n        debug.print(\"scaffold_with_subtree: {}\", scaffold_with_subtree)\n        debug.print(\"path_log_prob: {}\", path_log_prob)\n        debug.print(\"scaffold_log_p: {}\", scaffold_log_p)\n\n    return (\n        new_tree_expression,\n        changes,\n        index_mapping,\n        path_log_prob,\n        scaffold_log_p,\n        idx_a,\n        idx_b,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/moves/detach_attach/#gallifrey.moves.detach_attach.structure_detach_move","title":"<code>structure_detach_move(key, tree_expression, post_level_map, empty_node_value=-1, verbosity=0)</code>","text":"<p>Perform detach move on the kernel structure array. In this move, a scaffold is detached from the tree expression (at idx a), and a subtree from the scaffold (at idx b) is detached and then reattached to the root (idx a) of the scaffold.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key for sampling.</p> required <code>tree_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression, given in level order notation.</p> required <code>post_level_map</code> <code>Int[ndarray, ' D']</code> <p>The array that maps the post order index to the level order index. (This is useful, since the post_level_map contains all the indices of the nodes in the tree, which we can sample).</p> required <code>empty_node_value</code> <code>ScalarInt</code> <p>The value to fill the hole with in the subtree, by default -1. (Must be negative.)</p> <code>-1</code> <code>verbosity</code> <code>int</code> <p>The verbosity level, by default 0. Debug information is printed if verbosity &gt; 2.</p> <code>0</code> <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The new tree expression with the scaffold removed and the subtree reattached.</p> <code>Int[ndarray, '2 D']</code> <p>The mapping between the old and new indices of the nodes in the subtree. The first column contains the old indices, and the second column contains the new indices.</p> <code>ScalarInt</code> <p>The index of the root of the scaffold (idx a).</p> <code>ScalarInt</code> <p>The index of the root of the subtree that was detached from the scaffold (idx b).</p> Source code in <code>gallifrey/moves/detach_attach.py</code> <pre><code>def structure_detach_move(\n    key: PRNGKeyArray,\n    tree_expression: Int[jnp.ndarray, \" D\"],\n    post_level_map: Int[jnp.ndarray, \" D\"],\n    empty_node_value: ScalarInt = -1,\n    verbosity: int = 0,\n) -&gt; tuple[\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \"2 D\"],\n    ScalarInt,\n    ScalarInt,\n]:\n    \"\"\"\n    Perform detach move on the kernel structure array. In this move,\n    a scaffold is detached from the tree expression (at idx a), and a\n    subtree from the scaffold (at idx b) is detached and then\n    reattached to the root (idx a) of the scaffold.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key for sampling.\n    tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression, given in level order notation.\n    post_level_map : Int[jnp.ndarray, \" D\"]\n        The array that maps the post order index to the level order index.\n        (This is useful, since the post_level_map contains all the indices\n        of the nodes in the tree, which we can sample).\n    empty_node_value : ScalarInt, optional\n        The value to fill the hole with in the subtree, by default -1.\n        (Must be negative.)\n    verbosity : int, optional\n        The verbosity level, by default 0. Debug information is printed\n        if verbosity &gt; 2.\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The new tree expression with the scaffold removed and the subtree\n        reattached.\n    Int[jnp.ndarray, \"2 D\"]\n        The mapping between the old and new indices of the nodes in the\n        subtree. The first column contains the old indices, and the second\n        column contains the new indices.\n    ScalarInt\n        The index of the root of the scaffold (idx a).\n    ScalarInt\n        The index of the root of the subtree that was detached from the\n        scaffold (idx b).\n    \"\"\"\n\n    key, idx_a_subkey, idx_b_subkey = jr.split(key, 3)\n\n    # choose the root of the scaffold to detach\n    idx_a = pick_random_node(idx_a_subkey, post_level_map)\n\n    # detach the subtree\n    amputee_tree, subtree_at_a, subtree_at_a_indices = detach_subtree(\n        tree_expression,\n        idx_a,\n        fill_value=empty_node_value,\n    )\n\n    # select the root of the subtree to detach from the scaffold\n    idx_b = pick_random_node(idx_b_subkey, subtree_at_a_indices)\n\n    # get the subtree from the scaffold and move it to the root of the scaffold,\n    # also get mapping between old and new indices\n    subtree_at_b, index_mapping = get_subtree(subtree_at_a, idx_b, new_root_idx=idx_a)\n\n    if verbosity &gt; 2:\n        debug.print(\"idx_a: {}\", idx_a)\n        debug.print(\"idx_b: {}\", idx_b)\n        debug.print(\"Remaining Original Tree: {}\", amputee_tree)\n        debug.print(\"Subtree at a: {}\", subtree_at_a)\n        debug.print(\"Subtree at b: {}\", subtree_at_b)\n\n    # attach the subtree at the root of the scaffold (this assumes the empty nodes in\n    # the original tree are -1, and all filled nodes are &gt;= 0)\n    new_tree = jnp.where(subtree_at_b &gt; amputee_tree, subtree_at_b, amputee_tree)\n\n    return new_tree, index_mapping, idx_a, idx_b\n</code></pre>"},{"location":"autoapi/gallifrey/moves/detach_attach/#gallifrey.moves.detach_attach.transplant_subtree","title":"<code>transplant_subtree(donor_tree_expression, recipient_tree_expression, donor_root_idx, recipient_root_idx, max_nodes)</code>","text":"<p>Transplant a subtree from one tree to another. The subtree is rooted at the donor root index and re-rooted at the recipient root index. The recipient tree must have an empty node at the recipient root index.</p> <p>Parameters:</p> Name Type Description Default <code>donor_tree_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression of the donor tree.</p> required <code>recipient_tree_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression of the recipient tree.</p> required <code>donor_root_idx</code> <code>ScalarInt</code> <p>The index of the root of the subtree to be moved.</p> required <code>recipient_root_idx</code> <code>ScalarInt</code> <p>The index of the root of the subtree to be moved to.</p> required <code>max_nodes</code> <code>int</code> <p>The maximum number of nodes in the tree.</p> required <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The tree expression of the recipient tree with the attached subtree.</p> <code>Int[ndarray, '2 D']</code> <p>The indices of the moved nodes. Two rows, the first row contains the indices of the subtree in the donor tree, the second row contains the indices of the subtree in the recipient tree.</p> Source code in <code>gallifrey/moves/detach_attach.py</code> <pre><code>@partial(jit, static_argnames=(\"max_nodes\",))\ndef transplant_subtree(\n    donor_tree_expression: Int[jnp.ndarray, \" D\"],\n    recipient_tree_expression: Int[jnp.ndarray, \" D\"],\n    donor_root_idx: ScalarInt,\n    recipient_root_idx: ScalarInt,\n    max_nodes: int,\n) -&gt; tuple[\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \"2 D\"],\n]:\n    \"\"\"\n    Transplant a subtree from one tree to another. The subtree is\n    rooted at the donor root index and re-rooted at the recipient\n    root index.\n    The recipient tree must have an empty node at the recipient root index.\n\n    Parameters\n    ----------\n    donor_tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression of the donor tree.\n    recipient_tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression of the recipient tree.\n    donor_root_idx : ScalarInt\n        The index of the root of the subtree to be moved.\n    recipient_root_idx : ScalarInt\n        The index of the root of the subtree to be moved to.\n    max_nodes : int\n        The maximum number of nodes in the tree.\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The tree expression of the recipient tree with the attached subtree.\n    Int[jnp.ndarray, \"2 D\"]\n        The indices of the moved nodes. Two rows, the first row\n        contains the indices of the subtree in the donor tree, the second\n        row contains the indices of the subtree in the recipient tree.\n\n    \"\"\"\n    # if donor_tree_expression[donor_root_idx] == -1:\n    #     raise ValueError(\"The donor root index must be a non-empty node.\")\n    # if recipient_tree_expression[recipient_root_idx] != -1:\n    #     raise ValueError(\"The recipient root index must be an empty node.\")\n\n    # create initial state\n    index_array = jnp.full((2, max_nodes), -1)\n    index_pointer = 0\n    stack = jnp.full(max_nodes, -1).at[0].set(donor_root_idx)\n    new_stack = jnp.copy(stack).at[0].set(recipient_root_idx)\n    stack_pointer = 0\n\n    initial_state = (\n        recipient_tree_expression,\n        index_array,\n        index_pointer,\n        stack,\n        new_stack,\n        stack_pointer,\n    )\n\n    tree_expression, index_array, _ = _transplant_subtree(\n        donor_tree_expression,\n        initial_state,\n        max_nodes,\n    )\n\n    index_array = index_array\n    # if jnp.any(index_array &gt;= max_nodes):\n    #     raise ValueError(\n    #         \"The index array is out of bounds. This could happen if the tree \"\n    #         \"is re-rooted to a spot where one of its leaves exceeds the \"\n    #         \"maximum number of nodes.\"\n    #     )\n    return tree_expression, index_array\n</code></pre>"},{"location":"autoapi/gallifrey/moves/noise_proposal/","title":"noise_proposal","text":""},{"location":"autoapi/gallifrey/moves/noise_proposal/#gallifrey.moves.noise_proposal.noise_variance_probability","title":"<code>noise_variance_probability(kernel_gram, current_noise_variance, proposed_noise_variance, data)</code>","text":"<p>Calculate the posterior probability for a proposed noise variance, given the current kernel gram matrix and current noise variance.</p> <p>See equation 21 in Saad2023.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_gram</code> <code>Float[ndarray, 'D D']</code> <p>The gram matrix of the kernel.</p> required <code>current_noise_variance</code> <code>ScalarFloat</code> <p>The current noise variance.</p> required <code>proposed_noise_variance</code> <code>ScalarFloat</code> <p>The proposed noise variance.</p> required <code>data</code> <code>Dataset</code> <p>The data containing the observations, as a Dataset object.</p> required <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The posterior probability for the proposed noise variance.</p> Source code in <code>gallifrey/moves/noise_proposal.py</code> <pre><code>def noise_variance_probability(\n    kernel_gram: Float[jnp.ndarray, \"D D\"],\n    current_noise_variance: ScalarFloat,\n    proposed_noise_variance: ScalarFloat,\n    data: Dataset,\n) -&gt; ScalarFloat:\n    \"\"\"\n    Calculate the posterior probability for a\n    proposed noise variance, given the current\n    kernel gram matrix and current noise variance.\n\n    See equation 21 in Saad2023.\n\n    Parameters\n    ----------\n    kernel_gram : Float[jnp.ndarray, \"D D\"]\n        The gram matrix of the kernel.\n    current_noise_variance : ScalarFloat\n        The current noise variance.\n    proposed_noise_variance : ScalarFloat\n        The proposed noise variance.\n    data : Dataset\n        The data containing the observations,\n        as a Dataset object.\n\n    Returns\n    -------\n    ScalarFloat\n        The posterior probability for the proposed noise variance.\n    \"\"\"\n\n    # calculate \u03a3\u207b\u00b9y | [n,1]\n    inv_sigma_r = calculate_inv_sigma_r(\n        data.y,\n        kernel_gram,\n        current_noise_variance,\n    )\n\n    # calculate the means of the Gaussian process, \u00b5 = Kxx*\u03a3\u207b\u00b9y | [n,1]\n    means = kernel_gram @ inv_sigma_r\n\n    # create inverse gamma distribution\n    diff = data.y - means\n    inv_gamma = InverseGamma(\n        concentration=1 + data.y.size / 2,\n        scale=1 + 0.5 * diff @ diff,\n    )\n    # calculate the log probability of the proposal\n    log_prob_proposal = inv_gamma.log_prob(proposed_noise_variance)\n\n    return log_prob_proposal\n</code></pre>"},{"location":"autoapi/gallifrey/moves/noise_proposal/#gallifrey.moves.noise_proposal.noise_variance_proposal","title":"<code>noise_variance_proposal(key, kernel_gram, noise_variance, data)</code>","text":"<p>Sample a new proposal for the noise variance, and the log probability of the proposal.</p> <p>See equation 21 in Saad et al. 2023.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key.</p> required <code>kernel_gram</code> <code>Float[ndarray, 'D D']</code> <p>The gram matrix of the kernel.</p> required <code>noise_variance</code> <code>ScalarFloat</code> <p>The current noise variance.</p> required <code>data</code> <code>Dataset</code> <p>The data containing the observations, as a Dataset object.</p> required <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The new noise variance.</p> <code>ScalarFloat</code> <p>The log probability of the proposal.</p> Source code in <code>gallifrey/moves/noise_proposal.py</code> <pre><code>def noise_variance_proposal(\n    key: PRNGKeyArray,\n    kernel_gram: Float[jnp.ndarray, \"D D\"],\n    noise_variance: ScalarFloat,\n    data: Dataset,\n) -&gt; tuple[ScalarFloat, ScalarFloat]:\n    \"\"\"\n    Sample a new proposal for the noise variance,\n    and the log probability of the proposal.\n\n    See equation 21 in Saad et al. 2023.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key.\n    kernel_gram : Float[jnp.ndarray, \"D D\"]\n        The gram matrix of the kernel.\n    noise_variance : ScalarFloat\n        The current noise variance.\n    data : Dataset\n        The data containing the observations,\n        as a Dataset object.\n\n    Returns\n    -------\n    ScalarFloat\n        The new noise variance.\n    ScalarFloat\n        The log probability of the proposal.\n    \"\"\"\n\n    # calculate \u03a3\u207b\u00b9y | [n,1]\n    inv_sigma_r = calculate_inv_sigma_r(\n        data.y,\n        kernel_gram,\n        noise_variance,\n    )\n\n    # calculate the means of the Gaussian process, \u00b5 = Kxx*\u03a3\u207b\u00b9y | [n,1]\n    means = kernel_gram @ inv_sigma_r\n\n    # sample new noise variance from inverse gamma distribution\n    diff = data.y - means\n    inv_gamma = InverseGamma(\n        concentration=1 + data.y.size / 2,\n        scale=1 + 0.5 * diff @ diff,\n    )\n    new_noise_variance = inv_gamma.sample(seed=key, sample_shape=())\n    log_prob = inv_gamma.log_prob(new_noise_variance)\n\n    return new_noise_variance, log_prob  # type: ignore\n</code></pre>"},{"location":"autoapi/gallifrey/moves/particle_move/","title":"particle_move","text":""},{"location":"autoapi/gallifrey/moves/particle_move/#gallifrey.moves.particle_move.kernel_structure_proposal","title":"<code>kernel_structure_proposal(key, kernel_state, kernel_prior, p_detach_attach=0.5, verbosity=0)</code>","text":"<p>Perform a move on the kernel structure, either detach-attach or subtree-replace.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key.</p> required <code>kernel_state</code> <code>State</code> <p>The original kernel state.</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior.</p> required <code>p_detach_attach</code> <code>ScalarFloat</code> <p>The probability of performing a detach-attach move (vs a subtree-replace move), by default 0.5.</p> <code>0.5</code> <code>verbosity</code> <code>int</code> <p>The verbosity level, by default 0. Debug information for the detach-attach and subtree-replace moves are printed if verbosity &gt; 2.</p> <code>0</code> <p>Returns:</p> Type Description <code>State</code> <p>The proposed kernel state.</p> <code>ScalarFloat</code> <p>The log probability associated with the detach-attach move (0.0 if a subtree-replace move is performed).</p> Source code in <code>gallifrey/moves/particle_move.py</code> <pre><code>@partial(jit, static_argnames=(\"kernel_prior\", \"verbosity\", \"p_detach_attach\"))\ndef kernel_structure_proposal(\n    key: PRNGKeyArray,\n    kernel_state: nnx.State,\n    kernel_prior: KernelPrior,\n    p_detach_attach: ScalarFloat = 0.5,\n    verbosity: int = 0,\n) -&gt; tuple[nnx.State, ScalarFloat, ScalarBool]:\n    \"\"\"\n    Perform a move on the kernel structure,\n    either detach-attach or subtree-replace.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key.\n    kernel_state : nnx.State\n        The original kernel state.\n    kernel_prior : KernelPrior\n        The kernel prior.\n    p_detach_attach : ScalarFloat, optional\n        The probability of performing a detach-attach move\n        (vs a subtree-replace move), by default 0.5.\n    verbosity : int, optional\n        The verbosity level, by default 0. Debug information\n        for the detach-attach and subtree-replace moves\n        are printed if verbosity &gt; 2.\n\n    Returns\n    -------\n    nnx.State\n        The proposed kernel state.\n    ScalarFloat\n        The log probability associated with the detach-attach move\n        (0.0 if a subtree-replace move is performed).\n    \"\"\"\n\n    key, detach_attach_key, move_key = jr.split(key, 3)\n\n    # choose move to perform (detach-attach or subtree-replace), with\n    # detach-attach move being impossible if max_depth == 1\n    p_detach_attach = jnp.where(\n        kernel_prior.kernel_structure_prior.max_depth == 1,\n        jnp.array(0.0),\n        p_detach_attach,\n    )\n    perform_detach_attach = jr.bernoulli(\n        detach_attach_key,\n        p_detach_attach,\n    )\n\n    # perform structure move\n    proposed_kernel_state, detach_attach_log_prob = lax.cond(\n        perform_detach_attach,\n        lambda key: detach_attach_move(\n            key,\n            kernel_state,\n            kernel_prior,\n            verbosity=verbosity,\n        ),\n        lambda key: (\n            subtree_replace_move(\n                key,\n                kernel_state,\n                kernel_prior,\n                verbosity=verbosity,\n            ),\n            jnp.array(0.0),  # no detach-attach log prob if subtree-replace,\n        ),  # (Preposition 1&amp;2 in Saad2023)\n        move_key,\n    )\n\n    return proposed_kernel_state, detach_attach_log_prob, perform_detach_attach\n</code></pre>"},{"location":"autoapi/gallifrey/moves/particle_move/#gallifrey.moves.particle_move.particle_state_proposal","title":"<code>particle_state_proposal(key, particle_state, kernel_prior, noise_prior, data, fix_noise, p_detach_attach=0.5, verbosity=0)</code>","text":"<p>Propose a new particle state by proposing a new kernel structure and noise variance, and calculating the contribution to the acceptance probability.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key.</p> required <code>particle_state</code> <code>State</code> <p>The current particle state.</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior.</p> required <code>noise_prior</code> <code>Distribution</code> <p>The noise variance prior.</p> required <code>data</code> <code>Dataset</code> <p>The data.</p> required <code>fix_noise</code> <code>bool</code> <p>Whether to fix the noise variance. In case of fixed noise, the noise variance is not updated.</p> required <code>p_detach_attach</code> <code>ScalarFloat</code> <p>The probability of performing a detach-attach move (vs a subtree-replace move), by default 0.5.</p> <code>0.5</code> <code>verbosity</code> <code>int</code> <p>The verbosity level, by default 0. Debug information is printed if verbosity &gt; 1.</p> <code>0</code> <p>Returns:</p> Type Description <code>State</code> <p>The proposed particle state.</p> <code>ScalarFloat</code> <p>The log acceptance probability numerator.</p> Source code in <code>gallifrey/moves/particle_move.py</code> <pre><code>@partial(\n    jit,\n    static_argnames=(\n        \"kernel_prior\",\n        \"data\",\n        \"fix_noise\",\n        \"verbosity\",\n    ),\n)\ndef particle_state_proposal(\n    key: PRNGKeyArray,\n    particle_state: nnx.State,\n    kernel_prior: KernelPrior,\n    noise_prior: Distribution,\n    data: Dataset,\n    fix_noise: bool,\n    p_detach_attach: ScalarFloat = 0.5,\n    verbosity: int = 0,\n) -&gt; tuple[nnx.State, ScalarFloat]:\n    \"\"\"\n    Propose a new particle state by proposing a new kernel structure\n    and noise variance, and calculating the contribution to the\n    acceptance probability.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key.\n    particle_state : nnx.State\n        The current particle state.\n    kernel_prior : KernelPrior\n        The kernel prior.\n    noise_prior : Distribution\n        The noise variance prior.\n    data : Dataset\n        The data.\n    fix_noise : bool\n        Whether to fix the noise variance. In case\n        of fixed noise, the noise variance is not updated.\n    p_detach_attach : ScalarFloat, optional\n        The probability of performing a detach-attach move\n        (vs a subtree-replace move), by default 0.5.\n    verbosity : int, optional\n        The verbosity level, by default 0. Debug information\n        is printed if verbosity &gt; 1.\n\n    Returns\n    -------\n    nnx.State\n        The proposed particle state.\n    ScalarFloat\n        The log acceptance probability numerator.\n    \"\"\"\n\n    # get necessary keys\n    key, structure_key, noise_key = jr.split(key, 3)\n\n    # unpack particle state\n    kernel_state = particle_state.kernel\n    noise_variance = jnp.array(particle_state.noise_variance.value)  # type: ignore\n\n    # propose new kernel structure\n    proposed_kernel_state, detach_attach_log_prob, performed_detach_attach = (\n        kernel_structure_proposal(\n            structure_key,\n            kernel_state,\n            kernel_prior,\n            p_detach_attach,\n            verbosity=verbosity,\n        )\n    )\n\n    proposed_kernel = kernel_prior.reconstruct_kernel(proposed_kernel_state)\n\n    # calculate gram matrix for new kernel\n    kernel_gram = proposed_kernel._gram_train(data.x)\n\n    # propose new noise variance\n    proposed_noise_variance, log_prob_noise = lax.cond(\n        fix_noise,\n        lambda key: (noise_variance, 0.0),  # return noise variance as is\n        lambda key: noise_variance_proposal(\n            key,\n            kernel_gram,\n            noise_variance,\n            data,\n        ),\n        noise_key,\n    )\n\n    # update particle state\n    new_par_state = deepcopy(particle_state)\n    new_par_state.kernel = proposed_kernel_state  # type: ignore\n    new_par_state.noise_variance = (  # type: ignore\n        particle_state.noise_variance.replace(  # type: ignore\n            proposed_noise_variance,\n        )\n    )\n\n    # calculate acceptance probability terms\n    log_tree_size = jnp.log(proposed_kernel.node_sizes[0])\n    marginal_log_likelihood = calculate_marginal_log_likelihood(\n        kernel_gram,\n        proposed_noise_variance,\n        data,\n    )\n    log_prob_noise_prior = lax.cond(\n        fix_noise,\n        lambda x: jnp.array(0.0),\n        lambda x: jnp.asarray(\n            noise_prior.log_prob(x),\n            dtype=data.x.dtype,\n        ).squeeze(),\n        proposed_noise_variance,\n    )\n\n    # calculate acceptance probability contribution,\n    # (eq 22. and Preposition 2 in Saad2023)\n    log_acceptance_prob_numerator = jnp.array(\n        [\n            marginal_log_likelihood,\n            -log_tree_size,\n            log_prob_noise_prior,\n            -log_prob_noise,\n            detach_attach_log_prob,\n        ]\n    ).sum()\n\n    if verbosity &gt; 1:\n        lax.cond(\n            performed_detach_attach,\n            lambda: debug.print(\"Move Type: Detach-attach\"),\n            lambda: debug.print(\"Move Type: Subtree-replace\"),\n        )\n        debug.print(\"-\" * 50)\n        debug.print(\"Proposal Terms:\")\n        debug.print(\"Marginal log likelihood: {}\", marginal_log_likelihood)\n        debug.print(\"Log (tree size^-1): {}\", -log_tree_size)\n        debug.print(\"Log noise prior probability: {}\", log_prob_noise_prior)\n        debug.print(\"Log (probability of noise posterior^-1): {}\", -log_prob_noise)\n        debug.print(\"Log detach-attach probability: {}\", detach_attach_log_prob)\n        debug.print(\"Sum of terms: {}\", log_acceptance_prob_numerator)\n        debug.print(\"-\" * 50)\n\n    return new_par_state, log_acceptance_prob_numerator\n</code></pre>"},{"location":"autoapi/gallifrey/moves/particle_move/#gallifrey.moves.particle_move.structure_move","title":"<code>structure_move(key, particle_state, kernel_prior, noise_prior, data, fix_noise, verbosity=0)</code>","text":"<p>Perform a structure move on the particle state, by proposing a new kernel structure (via a detach-attach or subtree-replace move), and a new noise variance.</p> <p>The acceptance probability is calculated based on the current and proposed particle states, and the proposal is accepted stochasticly based on that.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key for the move.</p> required <code>particle_state</code> <code>State</code> <p>The current particle state.</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The kernel prior used for sampling a new kernel structure.</p> required <code>noise_prior</code> <code>Distribution</code> <p>The noise variance prior, used for calculating the contribution of the noise variance to the acceptance probability.</p> required <code>data</code> <code>Dataset</code> <p>The data, a Dataset object containing the observations. (The observations y should already be centered around the deterministic mean function, i.e. it should be the residuals.)</p> required <code>fix_noise</code> <code>bool</code> <p>Whether to fix the noise variance, or sample it.</p> required <code>verbosity</code> <code>int</code> <p>The verbosity level, by default 0. Debug information is printed if verbosity &gt; 1.</p> <code>0</code> <p>Returns:</p> Type Description <code>State</code> <p>The new particle state after the move.</p> <code>ScalarBool</code> <p>Whether the move was accepted.</p> Source code in <code>gallifrey/moves/particle_move.py</code> <pre><code>@partial(\n    jit,\n    static_argnames=(\n        \"kernel_prior\",\n        \"data\",\n        \"fix_noise\",\n        \"verbosity\",\n    ),\n)\ndef structure_move(\n    key: PRNGKeyArray,\n    particle_state: nnx.State,\n    kernel_prior: KernelPrior,\n    noise_prior: Distribution,\n    data: Dataset,\n    fix_noise: bool,\n    verbosity: int = 0,\n) -&gt; tuple[nnx.State, ScalarBool]:\n    \"\"\"\n    Perform a structure move on the particle state,\n    by proposing a new kernel structure (via a detach-attach\n    or subtree-replace move), and a new noise variance.\n\n    The acceptance probability is calculated based on the current\n    and proposed particle states, and the proposal is accepted\n    stochasticly based on that.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key for the move.\n    particle_state : nnx.State\n        The current particle state.\n    kernel_prior : KernelPrior\n        The kernel prior used for sampling a new\n        kernel structure.\n    noise_prior : Distribution\n        The noise variance prior, used for calculating\n        the contribution of the noise variance to the\n        acceptance probability.\n    data : Dataset\n        The data, a Dataset object containing the\n        observations. (The observations y should already\n        be centered around the deterministic mean function,\n        i.e. it should be the residuals.)\n    fix_noise : bool\n        Whether to fix the noise variance, or sample it.\n    verbosity : int, optional\n        The verbosity level, by default 0. Debug information\n        is printed if verbosity &gt; 1.\n\n    Returns\n    -------\n    nnx.State\n        The new particle state after the move.\n    ScalarBool\n        Whether the move was accepted.\n    \"\"\"\n\n    key, proposal_key, acceptance_key = jr.split(key, 3)\n\n    proposed_particle_state, log_hastings_ratio_numerator = particle_state_proposal(\n        proposal_key,\n        particle_state,\n        kernel_prior,\n        noise_prior,\n        data,\n        fix_noise=fix_noise,\n        verbosity=verbosity,\n    )\n\n    log_hastings_ratio_denominator = calculate_acceptance_prob_contribution(\n        particle_state,\n        proposed_particle_state.noise_variance.value,  # type: ignore\n        kernel_prior,\n        noise_prior,\n        data,\n        fix_noise=fix_noise,\n        verbosity=verbosity,\n    )\n\n    acceptance_probability = calculate_acceptance_probability(\n        log_hastings_ratio_numerator,\n        log_hastings_ratio_denominator,\n    )\n\n    accepted = jr.bernoulli(acceptance_key, acceptance_probability)\n    if verbosity &gt; 1:\n        debug.print(\"-\" * 50)\n        debug.print(\"Structure move accepted: {}\", accepted)\n        debug.print(\"Acceptance Probability: {}\", acceptance_probability)\n        debug.print(\"-\" * 50)\n\n    new_particle_state = lax.cond(\n        accepted,\n        lambda: proposed_particle_state,\n        lambda: particle_state,\n    )\n\n    return new_particle_state, accepted\n</code></pre>"},{"location":"autoapi/gallifrey/moves/subtree_replace/","title":"subtree_replace","text":""},{"location":"autoapi/gallifrey/moves/subtree_replace/#gallifrey.moves.subtree_replace.replace_subtree_structure","title":"<code>replace_subtree_structure(key, tree_expression, replace_idx, tree_prior, verbosity=0)</code>","text":"<p>Replace a subtree in the tree expression with a new subtree sampled from the prior.</p> <p>In contrast to the subtree_replace_move function, this function uses a fixed index for the root of the subtree to replace.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key.</p> required <code>tree_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression (level order).</p> required <code>replace_idx</code> <code>ScalarInt</code> <p>The index of the node where to attach the new subtree.</p> required <code>tree_prior</code> <code>TreeStructurePrior</code> <p>The prior to sample the new subtree from.</p> required <code>verbosity</code> <code>int</code> <p>The verbosity level, by default 0. Debug information is printed if verbosity &gt; 2.</p> <code>0</code> <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The updated tree expression.</p> <code>Int[ndarray, ' D']</code> <p>An array containing the indices of the nodes that were changed.</p> Source code in <code>gallifrey/moves/subtree_replace.py</code> <pre><code>def replace_subtree_structure(\n    key: PRNGKeyArray,\n    tree_expression: Int[jnp.ndarray, \" D\"],\n    replace_idx: ScalarInt,\n    tree_prior: TreeStructurePrior,\n    verbosity: int = 0,\n) -&gt; tuple[\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n]:\n    \"\"\"\n    Replace a subtree in the tree expression with a\n    new subtree sampled from the prior.\n\n    In contrast to the subtree_replace_move function,\n    this function uses a fixed index for\n    the root of the subtree to replace.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key.\n    tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression (level order).\n    replace_idx : ScalarInt\n        The index of the node where to attach the new subtree.\n    tree_prior : TreeStructurePrior\n        The prior to sample the new subtree from.\n    verbosity : int, optional\n        The verbosity level, by default 0. Debug information is printed\n        if verbosity &gt; 2.\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The updated tree expression.\n    Int[jnp.ndarray, \" D\"]\n        An array containing the indices of the nodes that were changed.\n    \"\"\"\n\n    new_subtree = tree_prior.sample_single(\n        key,\n        root_idx=replace_idx,\n    )\n\n    if verbosity &gt; 2:\n        debug.print(\"New subtree: {}\", new_subtree)\n\n    return replace_subtree_structure_with(\n        tree_expression,\n        new_subtree,\n        replace_idx,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/moves/subtree_replace/#gallifrey.moves.subtree_replace.replace_subtree_structure_with","title":"<code>replace_subtree_structure_with(tree_expression, subtree_expression, replace_idx)</code>","text":"<p>Replace a subtree in the tree_expression with another subtree. The subtree to replace is specified by the replace_idx.</p> <p>In contrast to the subtree_replace_move and replace_subtree functions, this function takes the tree_expression and subtree_expression as inputs. No random key is required.</p> <p>Parameters:</p> Name Type Description Default <code>tree_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression (level order).</p> required <code>subtree_expression</code> <code>Int[ndarray, ' D']</code> <p>The subtree expression (level order). The array must have the same length as the tree_expression.</p> required <code>replace_idx</code> <code>ScalarInt</code> <p>The index of the node where to attach the new subtree.</p> required <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The updated tree expression, with the subtree replaced.</p> <code>Int[ndarray, ' D']</code> <p>An array containing the indices of the nodes that were changed.</p> Source code in <code>gallifrey/moves/subtree_replace.py</code> <pre><code>def replace_subtree_structure_with(\n    tree_expression: Int[jnp.ndarray, \" D\"],\n    subtree_expression: Int[jnp.ndarray, \" D\"],\n    replace_idx: ScalarInt,\n) -&gt; tuple[\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n]:\n    \"\"\"\n    Replace a subtree in the tree_expression with\n    another subtree. The subtree to replace is specified\n    by the replace_idx.\n\n    In contrast to the subtree_replace_move and replace_subtree functions,\n    this function takes the tree_expression and subtree_expression as inputs.\n    No random key is required.\n\n    Parameters\n    ----------\n    tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression (level order).\n    subtree_expression : Int[jnp.ndarray, \" D\"]\n        The subtree expression (level order). The array must\n        have the same length as the tree_expression.\n    replace_idx : ScalarInt\n        The index of the node where to attach the new subtree.\n\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The updated tree expression, with the subtree replaced.\n    Int[jnp.ndarray, \" D\"]\n        An array containing the indices of the nodes that were changed.\n\n    \"\"\"\n\n    max_nodes = tree_expression.size\n\n    # if max_nodes != subtree_expression.size:\n    #     raise ValueError(\n    #         \"tree_expression and subtree_expression must have the same length.\"\n    #     )\n\n    # create initial state\n    changed_values = jnp.full(max_nodes, -1, dtype=tree_expression.dtype)\n    changed_pointer = 0\n    stack = jnp.copy(tree_expression).at[0].set(replace_idx)\n    stack_pointer = 0\n    initial_state = (\n        tree_expression,\n        subtree_expression,\n        changed_values,\n        changed_pointer,\n        stack,\n        stack_pointer,\n    )\n\n    new_tree_expression, changed_values, _ = _replace_subtree_structure_with(\n        initial_state,\n        max_nodes,\n    )\n\n    return new_tree_expression, changed_values\n</code></pre>"},{"location":"autoapi/gallifrey/moves/subtree_replace/#gallifrey.moves.subtree_replace.structure_subtree_replace_move","title":"<code>structure_subtree_replace_move(key, tree_expression, post_level_map, tree_prior, verbosity=0)</code>","text":"<p>Perform a subtree replacement move on the kernel structure array. In this move, a subtree is replaced with a new subtree sampled from the prior. The root of the subtree to be replaced is chosen uniformly at random.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key.</p> required <code>tree_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression (level order).</p> required <code>post_level_map</code> <code>Int[ndarray, ' D']</code> <p>The array that maps the post order index to the level order index. (This is useful, since the post_level_map contains all the indices of the nodes in the tree).</p> required <code>tree_prior</code> <code>TreeStructurePrior</code> <p>The prior to sample the new subtree from.</p> required <code>verbosity</code> <code>int</code> <p>The verbosity level, by default 0. Debug information is printed if verbosity &gt; 2.</p> <code>0</code> <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The updated tree expression after the move.</p> <code>Int[ndarray, ' D']</code> <p>An array containing the indices of the nodes that were changed.</p> <code>ScalarInt</code> <p>The index where the new subtree was attached.</p> Source code in <code>gallifrey/moves/subtree_replace.py</code> <pre><code>def structure_subtree_replace_move(\n    key: PRNGKeyArray,\n    tree_expression: Int[jnp.ndarray, \" D\"],\n    post_level_map: Int[jnp.ndarray, \" D\"],\n    tree_prior: TreeStructurePrior,\n    verbosity: int = 0,\n) -&gt; tuple[\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \" D\"],\n    ScalarInt,\n]:\n    \"\"\"\n    Perform a subtree replacement move on the kernel structure array.\n    In this move, a subtree is replaced with a new\n    subtree sampled from the prior. The root of the subtree\n    to be replaced is chosen uniformly at random.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key.\n    tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression (level order).\n    post_level_map : Int[jnp.ndarray, \" D\"]\n        The array that maps the post order index to the level order index.\n        (This is useful, since the post_level_map contains all the indices\n        of the nodes in the tree).\n    tree_prior : TreeStructurePrior\n        The prior to sample the new subtree from.\n    verbosity : int, optional\n        The verbosity level, by default 0. Debug information is printed\n        if verbosity &gt; 2.\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The updated tree expression after the move.\n    Int[jnp.ndarray, \" D\"]\n        An array containing the indices of the nodes that were changed.\n    ScalarInt\n        The index where the new subtree was attached.\n\n    \"\"\"\n    key, idx_key, subtree_key = jr.split(key, 3)\n\n    # sample a node to replace uniformly from non-empty nodes\n    replace_idx = pick_random_node(idx_key, post_level_map)\n\n    # if tree_expression[replace_idx] == empty_node_value:\n    #     raise RuntimeError(\n    #         \"The chosen index is that of an empty node, \"\n    #         \"that should not be possible. Check inputs.\"\n    #     )\n\n    if verbosity &gt; 2:\n        debug.print(\"Replacing node at index {}\", replace_idx)\n\n    updated_tree_expression, changed_nodes = replace_subtree_structure(\n        subtree_key,\n        tree_expression,\n        replace_idx,\n        tree_prior,\n        verbosity=verbosity,\n    )\n    return updated_tree_expression, changed_nodes, replace_idx\n</code></pre>"},{"location":"autoapi/gallifrey/moves/subtree_replace/#gallifrey.moves.subtree_replace.subtree_replace_move","title":"<code>subtree_replace_move(key, kernel_state, kernel_prior, verbosity=0)</code>","text":"<p>Perform a subtree replacement move on a kernel state.</p> <p>In this move, a subtree is replaced with a new subtree sampled from the prior. The root of the subtree to be replaced is chosen uniformly at random.</p> <p>The parameter from the old tree are copied to the new tree, while the parameters for the new subtree are sampled from the prior.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKeyArray</code> <p>The random key.</p> required <code>kernel_state</code> <code>State</code> <p>The original kernel state, must have attributes: - tree_expression - post_level_map</p> required <code>kernel_prior</code> <code>KernelPrior</code> <p>The prior to sample the new subtree from.</p> required <code>verbosity</code> <code>int</code> <p>The verbosity level, by default 0. Debug information is printed if verbosity &gt; 2.</p> <code>0</code> <p>Returns:</p> Type Description <code>State</code> <p>The updated kernel state after the move.</p> Source code in <code>gallifrey/moves/subtree_replace.py</code> <pre><code>@partial(jit, static_argnames=(\"kernel_prior\", \"verbosity\"))\ndef subtree_replace_move(\n    key: PRNGKeyArray,\n    kernel_state: nnx.State,\n    kernel_prior: KernelPrior,\n    verbosity: int = 0,\n) -&gt; nnx.State:\n    \"\"\"\n    Perform a subtree replacement move on a kernel state.\n\n    In this move, a subtree is replaced with a new\n    subtree sampled from the prior. The root of the subtree\n    to be replaced is chosen uniformly at random.\n\n    The parameter from the old tree are copied to the new tree,\n    while the parameters for the new subtree are sampled from the prior.\n\n    Parameters\n    ----------\n    key : PRNGKeyArray\n        The random key.\n    kernel_state : nnx.State\n        The original kernel state, must have attributes:\n        - tree_expression\n        - post_level_map\n    kernel_prior : KernelPrior\n        The prior to sample the new subtree from.\n    verbosity : int, optional\n        The verbosity level, by default 0. Debug information is printed\n        if verbosity &gt; 2.\n\n\n    Returns\n    -------\n    nnx.State\n        The updated kernel state after the move.\n\n    \"\"\"\n    key, structure_key, parameter_key = jr.split(key, 3)\n\n    # perform the subtree replacement move on the structure\n    new_structure, changes, _ = structure_subtree_replace_move(\n        structure_key,\n        kernel_state.tree_expression.value,  # type: ignore\n        kernel_state.post_level_map.value,  # type: ignore\n        kernel_prior.kernel_structure_prior,\n        verbosity=verbosity,\n    )\n\n    # create a new kernel with the updated structure\n    new_kernel = nnx.merge(kernel_prior.graphdef, kernel_state)\n    new_kernel.init_tree(\n        new_structure,\n        kernel_prior.kernel_library,\n        kernel_prior.max_depth,\n        kernel_prior.num_datapoints,\n    )\n\n    # sample the new parameters, while keeping the old ones\n    _, blank_new_state = nnx.split(new_kernel)\n    new_state, _ = kernel_prior.parameter_prior.sample_subset(\n        parameter_key,\n        blank_new_state,\n        considered_nodes=changes,\n    )\n    return new_state\n</code></pre>"},{"location":"autoapi/gallifrey/utils/","title":"utils","text":""},{"location":"autoapi/gallifrey/utils/probability_calculations/","title":"probability_calculations","text":""},{"location":"autoapi/gallifrey/utils/probability_calculations/#gallifrey.utils.probability_calculations.calculate_covariance_matrix","title":"<code>calculate_covariance_matrix(kernel_gram, noise_variance, jitter=1e-06)</code>","text":"<p>Calculate the covariance matrix for a Gaussian process.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_gram</code> <code>LinearOperator</code> <p>The kernel gram matrix.</p> required <code>noise_variance</code> <code>ScalarFloat</code> <p>The noise variance.</p> required <code>jitter</code> <code>float</code> <p>The jitter term to add to the kernel gram matrix, by default 1e-6. (This is to ensure the matrix is positive definite.)</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>Float[ndarray, 'D D']</code> <p>The covariance matrix for the Gaussian process.</p> Source code in <code>gallifrey/utils/probability_calculations.py</code> <pre><code>@partial(jit, static_argnames=(\"jitter\"))\ndef calculate_covariance_matrix(\n    kernel_gram: Float[jnp.ndarray, \" D D\"],\n    noise_variance: ScalarFloat,\n    jitter: float = 1e-6,\n) -&gt; Float[jnp.ndarray, \"D D\"]:\n    \"\"\"\n    Calculate the covariance matrix for a Gaussian process.\n\n    Parameters\n    ----------\n    kernel_gram : LinearOperator\n        The kernel gram matrix.\n    noise_variance : ScalarFloat\n        The noise variance.\n    jitter : float, optional\n        The jitter term to add to the kernel gram matrix, by default 1e-6.\n        (This is to ensure the matrix is positive definite.)\n\n    Returns\n    -------\n    Float[jnp.ndarray, \"D D\"]\n        The covariance matrix for the Gaussian process.\n\n    \"\"\"\n    return kernel_gram + jnp.eye(kernel_gram.shape[0]) * (noise_variance + jitter)\n</code></pre>"},{"location":"autoapi/gallifrey/utils/probability_calculations/#gallifrey.utils.probability_calculations.calculate_inv_sigma_r","title":"<code>calculate_inv_sigma_r(r, kernel_gram, noise_variance, jitter=1e-06)</code>","text":"<p>Calculate the covariance matrix \u03a3 for the Gaussian process, and solve for \u03a3\u207b\u00b9r.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code> Float[jnp.ndarray, \" D\"]</code> <p>The input observational residuals (observations y - deterministic mean function).</p> required <code>kernel_gram</code> <code>Float[ndarray, 'D D']</code> <p>The kernel gram matrix.</p> required <code>noise_variance</code> <code>ScalarFloat</code> <p>The noise variance.</p> required <code>jitter</code> <code>float</code> <p>The jitter term to add to the kernel gram matrix, by default 1e-6. (This is to ensure the matrix is positive definite.)</p> <code>1e-06</code> <p>Returns:</p> Type Description <code> Float[jnp.ndarray, \" D\"]</code> <p>The solution for \u03a3\u207b\u00b9r.</p> Source code in <code>gallifrey/utils/probability_calculations.py</code> <pre><code>@partial(jit, static_argnames=(\"jitter\"))\ndef calculate_inv_sigma_r(\n    r: Float[jnp.ndarray, \" D\"],\n    kernel_gram: Float[jnp.ndarray, \" D D\"],\n    noise_variance: ScalarFloat,\n    jitter: float = 1e-6,\n) -&gt; Float[jnp.ndarray, \" D\"]:\n    \"\"\"\n    Calculate the covariance matrix \u03a3 for the\n    Gaussian process, and solve for \u03a3\u207b\u00b9r.\n\n    Parameters\n    ----------\n    r :  Float[jnp.ndarray, \" D\"]\n        The input observational residuals\n        (observations y - deterministic mean function).\n    kernel_gram : Float[jnp.ndarray, \"D D\"]\n        The kernel gram matrix.\n    noise_variance : ScalarFloat\n        The noise variance.\n    jitter : float, optional\n        The jitter term to add to the kernel gram matrix, by default 1e-6.\n        (This is to ensure the matrix is positive definite.)\n\n    Returns\n    -------\n     Float[jnp.ndarray, \" D\"]\n        The solution for \u03a3\u207b\u00b9r.\n    \"\"\"\n\n    # calculate Sigma matrix for GP, \u03a3 = (Kxx + Io\u00b2) | [n,n]\n    sigma_matrix = calculate_covariance_matrix(\n        kernel_gram,\n        noise_variance,\n        jitter,\n    )\n    # solve for \u03a3\u207b\u00b9r | [n,1] using cholesky decomposition,\n    # where \u03a3\u207b\u00b9r = L\u207b\u1d40 L\u207b\u00b9 r, with L = cholesky(\u03a3)\n    cho_fac = linalg.cho_factor(sigma_matrix)\n    sigma_inv_r = linalg.cho_solve(cho_fac, r)\n    return sigma_inv_r\n</code></pre>"},{"location":"autoapi/gallifrey/utils/probability_calculations/#gallifrey.utils.probability_calculations.calculate_marginal_log_likelihood","title":"<code>calculate_marginal_log_likelihood(kernel_gram, noise_variance, data, jitter=1e-06)</code>","text":"<p>Calculate the log marginal likelihood p(y) for the state (k, theta, y, noise).</p> <p>Parameters:</p> Name Type Description Default <code>kernel_gram</code> <code> Float[jnp.ndarray, \"D D\"]</code> <p>The kernel gram matrix.</p> required <code>noise_variance</code> <code>ScalarFloat</code> <p>The noise variance.</p> required <code>data</code> <code>Dataset</code> <p>A dataset object containing the input x and output y.</p> required <code>jitter</code> <code>float</code> <p>The jitter term to add to the kernel gram matrix, by default 1e-6. (This is to ensure the matrix is positive definite.)</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The log marginal likelihood p(y) for the current state.</p> Source code in <code>gallifrey/utils/probability_calculations.py</code> <pre><code>def calculate_marginal_log_likelihood(\n    kernel_gram: Float[jnp.ndarray, \"D D\"],\n    noise_variance: ScalarFloat,\n    data: Dataset,\n    jitter: float = 1e-6,\n) -&gt; ScalarFloat:\n    \"\"\"\n    Calculate the log marginal likelihood p(y) for the\n    state (k, theta, y, noise).\n\n    Parameters\n    ----------\n    kernel_gram :  Float[jnp.ndarray, \"D D\"]\n        The kernel gram matrix.\n    noise_variance : ScalarFloat\n        The noise variance.\n    data : Dataset\n        A dataset object containing the input x and output y.\n    jitter : float, optional\n        The jitter term to add to the kernel gram matrix, by default 1e-6.\n        (This is to ensure the matrix is positive definite.)\n\n    Returns\n    -------\n    ScalarFloat\n        The log marginal likelihood p(y) for the current state.\n\n    \"\"\"\n\n    sigma_logdet, sigma_inv_r = calculate_sigma_logdet_and_inv_sigma_r(\n        data.y,\n        kernel_gram,\n        noise_variance,\n        jitter,\n    )\n    # compute marginal log likelihood, -1/2[ n log(2\u03c0) + log|\u03a3| + ( r\u1d40 \u03a3\u207b\u00b9 r ]\n    log_prob = -0.5 * (\n        data.n * jnp.log(2.0 * jnp.pi) + sigma_logdet + data.y.T @ sigma_inv_r\n    )\n    return log_prob\n</code></pre>"},{"location":"autoapi/gallifrey/utils/probability_calculations/#gallifrey.utils.probability_calculations.calculate_sigma_logdet_and_inv_sigma_r","title":"<code>calculate_sigma_logdet_and_inv_sigma_r(r, kernel_gram, noise_variance, jitter=1e-06)</code>","text":"<p>Calculate the log determinant of the covariance matrix \u03a3 for the Gaussian process, and solve for \u03a3\u207b\u00b9r.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code> Float[jnp.ndarray, \" D\"]</code> <p>The input observational residuals (observations y - deterministic mean function).</p> required <code>kernel_gram</code> <code>Float[ndarray, 'D D']</code> <p>The kernel gram matrix.</p> required <code>noise_variance</code> <code>ScalarFloat</code> <p>The noise variance.</p> required <code>jitter</code> <code>float</code> <p>The jitter term to add to the kernel gram matrix, by default 1e-6. (This is to ensure the matrix is positive definite.)</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>The log determinant of the covariance matrix \u03a3.</p> <code>Float[ndarray, ' D']</code> <p>The solution for \u03a3\u207b\u00b9r.</p> Source code in <code>gallifrey/utils/probability_calculations.py</code> <pre><code>@partial(jit, static_argnames=(\"jitter\"))\ndef calculate_sigma_logdet_and_inv_sigma_r(\n    r: Float[jnp.ndarray, \" D\"],\n    kernel_gram: Float[jnp.ndarray, \" D D\"],\n    noise_variance: ScalarFloat,\n    jitter: float = 1e-6,\n) -&gt; tuple[ScalarFloat, Float[jnp.ndarray, \" D\"]]:\n    \"\"\"\n    Calculate the log determinant of the covariance matrix \u03a3 for the\n    Gaussian process, and solve for \u03a3\u207b\u00b9r.\n\n    Parameters\n    ----------\n    r :  Float[jnp.ndarray, \" D\"]\n        The input observational residuals\n        (observations y - deterministic mean function).\n    kernel_gram : Float[jnp.ndarray, \"D D\"]\n        The kernel gram matrix.\n    noise_variance : ScalarFloat\n        The noise variance.\n    jitter : float, optional\n        The jitter term to add to the kernel gram matrix, by default 1e-6.\n        (This is to ensure the matrix is positive definite.)\n\n    Returns\n    -------\n    ScalarFloat\n        The log determinant of the covariance matrix \u03a3.\n    Float[jnp.ndarray, \" D\"]\n        The solution for \u03a3\u207b\u00b9r.\n    \"\"\"\n\n    # calculate Sigma matrix for GP, \u03a3 = (Kxx + Io\u00b2) | [n,n]\n    sigma_matrix = calculate_covariance_matrix(\n        kernel_gram,\n        noise_variance,\n        jitter,\n    )\n\n    # solve for \u03a3\u207b\u00b9r | [n,1] using cholesky decomposition,\n    # where \u03a3\u207b\u00b9r = L\u207b\u1d40 L\u207b\u00b9 r, with L = cholesky(\u03a3)\n    cho_fac = linalg.cho_factor(sigma_matrix)\n    sigma_inv_r = linalg.cho_solve(cho_fac, r)\n\n    # calculate log determinant | [1,1]\n    # log(|\u03a3|) =  log(|L|**2) = 2 log(|L|), with L = cholesky(\u03a3)\n    sigma_logdet = jnp.sum(jnp.log(jnp.diag(cho_fac[0]))) * 2.0\n    return sigma_logdet, sigma_inv_r\n</code></pre>"},{"location":"autoapi/gallifrey/utils/tree_helper/","title":"tree_helper","text":""},{"location":"autoapi/gallifrey/utils/tree_helper/#gallifrey.utils.tree_helper.calculate_max_depth","title":"<code>calculate_max_depth(max_nodes, tolerance=1e-10)</code>","text":"<p>Calculate the maximum depth of the tree that describes the kernel structure, given the maximum number of nodes in the tree.</p> <p>Parameters:</p> Name Type Description Default <code>max_nodes</code> <code>ScalarInt</code> <p>The maximum number of nodes in the tree that describes the kernel structure.</p> required <code>tolerance</code> <code>ScalarFloat</code> <p>On occasion, floating point errors may lead to a depth that is off by one. This can happen whenever the index is a power of 2. To avoid this, a tolerance is added to the index before taking the logarithm. By default 1e-10.</p> <code>1e-10</code> <p>Returns:</p> Type Description <code>ScalarInt</code> <p>The maximum depth of the nested kernel structure tree.</p> Source code in <code>gallifrey/utils/tree_helper.py</code> <pre><code>def calculate_max_depth(\n    max_nodes: ScalarInt,\n    tolerance: ScalarFloat = 1e-10,\n) -&gt; ScalarInt:\n    \"\"\"\n    Calculate the maximum depth of the tree that describes the kernel structure,\n    given the maximum number of nodes in the tree.\n\n    Parameters\n    ----------\n    max_nodes : ScalarInt\n        The maximum number of nodes in the tree that describes the kernel structure.\n    tolerance : ScalarFloat, optional\n        On occasion, floating point errors may lead to a depth that is off by one.\n        This can happen whenever the index is a power of 2. To avoid this, a tolerance\n        is added to the index before taking the logarithm. By default 1e-10.\n\n    Returns\n    -------\n    ScalarInt\n        The maximum depth of the nested kernel structure tree.\n    \"\"\"\n    return get_depth(max_nodes - 1, tolerance)  # idx of last node is max_nodes - 1\n</code></pre>"},{"location":"autoapi/gallifrey/utils/tree_helper/#gallifrey.utils.tree_helper.calculate_max_leaves","title":"<code>calculate_max_leaves(max_depth)</code>","text":"<p>Calculate the maximum number of leaves in a binary tree based on the maximum depth of the tree. This corresponds to the number of nodes at the maximum depth of the tree, which is given by 2^max_depth.</p> <p>Parameters:</p> Name Type Description Default <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the nested kernel structure tree.</p> required <p>Returns:</p> Type Description <code>ScalarInt</code> <p>The maximum number of leaves in the tree that describes the kernel structure.</p> Source code in <code>gallifrey/utils/tree_helper.py</code> <pre><code>def calculate_max_leaves(max_depth: ScalarInt) -&gt; ScalarInt:\n    \"\"\"\n    Calculate the maximum number of leaves in a binary tree based\n    on the maximum depth of the tree.\n    This corresponds to the number of nodes at the maximum depth\n    of the tree, which is given by 2^max_depth.\n\n    Parameters\n    ----------\n    max_depth : ScalarInt\n        The maximum depth of the nested kernel structure tree.\n\n    Returns\n    -------\n    ScalarInt\n        The maximum number of leaves in the tree that describes the\n        kernel structure.\n    \"\"\"\n    return 2**max_depth\n</code></pre>"},{"location":"autoapi/gallifrey/utils/tree_helper/#gallifrey.utils.tree_helper.calculate_max_nodes","title":"<code>calculate_max_nodes(max_depth)</code>","text":"<p>Calculate the maximum number of nodes in a binary tree based on the maximum depth of the tree.  Since each node in the tree can have at most two children, the maximum number of nodes in the tree is given by 2^(max_depth + 1) - 1.</p> <p>Parameters:</p> Name Type Description Default <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the nested kernel structure tree.</p> required <p>Returns:</p> Type Description <code>ScalarInt</code> <p>The maximum number of nodes in the tree that describes the kernel structure.</p> Source code in <code>gallifrey/utils/tree_helper.py</code> <pre><code>def calculate_max_nodes(max_depth: ScalarInt) -&gt; ScalarInt:\n    \"\"\"\n    Calculate the maximum number of nodes in a binary tree based\n    on the maximum depth of the tree.\n     Since each node in the tree can have at most two children,\n    the maximum number of nodes in the tree is given by\n    2^(max_depth + 1) - 1.\n\n    Parameters\n    ----------\n    max_depth : ScalarInt\n        The maximum depth of the nested kernel structure tree.\n\n    Returns\n    -------\n    ScalarInt\n        The maximum number of nodes in the tree that describes the kernel structure.\n    \"\"\"\n    return 2 ** (max_depth + 1) - 1\n</code></pre>"},{"location":"autoapi/gallifrey/utils/tree_helper/#gallifrey.utils.tree_helper.calculate_max_stack_size","title":"<code>calculate_max_stack_size(max_depth)</code>","text":"<p>For a given tree depth, calculate the maximum size of the stack needed to traverse the tree in level order.</p> <p>Parameters:</p> Name Type Description Default <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the nested kernel structure tree.</p> required <p>Returns:</p> Type Description <code>ScalarInt</code> <p>The maximum size of the stack needed to traverse the tree.</p> Source code in <code>gallifrey/utils/tree_helper.py</code> <pre><code>def calculate_max_stack_size(max_depth: ScalarInt) -&gt; ScalarInt:\n    \"\"\"\n    For a given tree depth, calculate the maximum size of the stack\n    needed to traverse the tree in level order.\n\n    Parameters\n    ----------\n    max_depth : ScalarInt\n        The maximum depth of the nested kernel structure tree.\n\n    Returns\n    -------\n    ScalarInt\n        The maximum size of the stack needed to traverse the tree.\n    \"\"\"\n    return max_depth + 1\n</code></pre>"},{"location":"autoapi/gallifrey/utils/tree_helper/#gallifrey.utils.tree_helper.clear_subtree","title":"<code>clear_subtree(tree_expression, clear_idx, fill_value=-1)</code>","text":"<p>Clear a subtree in a level order tree expression.</p> <p>NOTE: The new tree expression will not be a valid kernel expression, as the subtree removal leaves a hole in the expression.</p> <p>Parameters:</p> Name Type Description Default <code>tree_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression, given in level order notation.</p> required <code>clear_idx</code> <code>ScalarInt</code> <p>The index of the root node of the subtree to clear.</p> required <code>fill_value</code> <code>ScalarFloat</code> <p>The value to fill the cleared nodes with. By default -1.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The updated tree expression with the subtree cleared.</p> Source code in <code>gallifrey/utils/tree_helper.py</code> <pre><code>def clear_subtree(\n    tree_expression: Int[jnp.ndarray, \" D\"],\n    clear_idx: ScalarInt,\n    fill_value: ScalarFloat = -1,\n) -&gt; Int[jnp.ndarray, \" D\"]:\n    \"\"\"\n    Clear a subtree in a level order tree expression.\n\n    NOTE: The new tree expression will not be a valid\n    kernel expression, as the subtree removal leaves\n    a hole in the expression.\n\n    Parameters\n    ----------\n    tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression, given in level order notation.\n    clear_idx : ScalarInt\n        The index of the root node of the subtree to clear.\n    fill_value : ScalarFloat, optional\n        The value to fill the cleared nodes with. By default\n        -1.\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The updated tree expression with the subtree cleared.\n\n    \"\"\"\n    max_nodes = tree_expression.size\n\n    # create initial state\n    stack = jnp.zeros(max_nodes, dtype=tree_expression.dtype)\n    stack = stack.at[0].set(clear_idx)\n    stack_pointer = 0\n    initial_state = (tree_expression, stack, stack_pointer)\n\n    return _clear_subtree(\n        initial_state,\n        max_nodes,\n        fill_value,\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/utils/tree_helper/#gallifrey.utils.tree_helper.generate_random_path","title":"<code>generate_random_path(key, start_idx, node_height, max_depth, stop_probability=0.5, go_left_probability=0.5)</code>","text":"<p>Generate a random path in a binary tree starting from a given index. Returns the final chosen index, the path taken and the log probability of reaching the chosen index.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>The random key.</p> required <code>start_idx</code> <code>ScalarInt</code> <p>The index of the starting node.</p> required <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the tree.</p> required <code>node_height</code> <code>ScalarInt</code> <p>The height of the subtree at the starting index. This is used to calculate the maximum number of levels the path can traverse, and still respect the maximum depth of the tree once the kernel is reattached.</p> required <code>stop_probability</code> <code>ScalarFloat</code> <p>The probability of stopping at each level, by default 0.5.</p> <code>0.5</code> <code>go_left_probability</code> <code>ScalarFloat</code> <p>The probability of choosing the left child at each level, by default 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ScalarInt</code> <p>The index of the final chosen node (in level order).</p> <code>Int[ndarray, ' D']</code> <p>An array of indices representing the path taken, all parent indices from the start_idx to the final chosen index (in level order), excluding the final index.</p> <code>ScalarFloat</code> <p>The log probability of reaching the chosen idx. The probability is calculated by multiplying the probabilities of stopping at at each index along the path and the probabilities of choosing the left or right child at each level.</p> Source code in <code>gallifrey/utils/tree_helper.py</code> <pre><code>def generate_random_path(\n    key: PRNGKeyArray,\n    start_idx: ScalarInt,\n    node_height: ScalarInt,\n    max_depth: ScalarInt,\n    stop_probability: ScalarFloat = 0.5,\n    go_left_probability: ScalarFloat = 0.5,\n) -&gt; tuple[\n    ScalarInt,\n    Int[jnp.ndarray, \" D\"],\n    ScalarFloat,\n]:\n    \"\"\"\n    Generate a random path in a binary tree starting from a given index.\n    Returns the final chosen index, the path taken and the log probability\n    of reaching the chosen index.\n\n    Parameters\n    ----------\n    key : jr.PRNGKey\n        The random key.\n    start_idx : ScalarInt\n        The index of the starting node.\n    max_depth : ScalarInt\n        The maximum depth of the tree.\n    node_height : ScalarInt\n        The height of the subtree at the starting index. This is used\n        to calculate the maximum number of levels the path can traverse,\n        and still respect the maximum depth of the tree once the kernel\n        is reattached.\n    stop_probability : ScalarFloat, optional\n        The probability of stopping at each level, by default 0.5.\n    go_left_probability : ScalarFloat, optional\n        The probability of choosing the left child at each\n        level, by default 0.5.\n\n    Returns\n    -------\n    ScalarInt\n        The index of the final chosen node (in level order).\n    Int[jnp.ndarray, \" D\"]\n        An array of indices representing the path taken, all\n        parent indices from the start_idx to the final chosen index\n        (in level order), excluding the final index.\n    ScalarFloat\n        The log probability of reaching the chosen idx. The probability\n        is calculated by multiplying the probabilities of stopping at\n        at each index along the path and the probabilities\n        of choosing the left or right child at each level.\n    \"\"\"\n\n    index_array = jnp.full(max_depth + 1, -1)\n\n    chosen_idx, index_path, num_indices, idx_log_p = _generate_random_path(\n        key,\n        start_idx,\n        max_depth - node_height,\n        index_array,\n        stop_probability,\n        go_left_probability,\n    )\n\n    index_path = index_path\n\n    return chosen_idx, index_path, idx_log_p\n</code></pre>"},{"location":"autoapi/gallifrey/utils/tree_helper/#gallifrey.utils.tree_helper.get_child_idx","title":"<code>get_child_idx(idx, direction)</code>","text":"<p>Get the index of the left or right child of a node in a binary tree, given by 2 * idx + 1 for left child 2 * idx + 2 for right child.</p> <p>Compare: https://en.wikipedia.org/wiki/Binary_tree#Arrays</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>ScalarInt</code> <p>The index of the node.</p> required <code>direction</code> <code>Literal[l, r]</code> <p>The side of the child to get. \"l\" for left child, \"r\" for right child.</p> required <p>Returns:</p> Type Description <code>ScalarInt</code> <p>The index of the child.</p> Source code in <code>gallifrey/utils/tree_helper.py</code> <pre><code>def get_child_idx(idx: ScalarInt, direction: Literal[\"l\", \"r\"]) -&gt; ScalarInt:\n    \"\"\"\n    Get the index of the left or right child of a node in a binary tree,\n    given by\n    2 * idx + 1 for left child\n    2 * idx + 2 for right child.\n\n    Compare: https://en.wikipedia.org/wiki/Binary_tree#Arrays\n\n    Parameters\n    ----------\n    idx : ScalarInt\n        The index of the node.\n    direction : Literal[\"l\", \"r\"]\n        The side of the child to get. \"l\" for left child, \"r\" for right child.\n\n    Returns\n    -------\n    ScalarInt\n        The index of the child.\n    \"\"\"\n    direction_map = {\"l\": 1, \"r\": 2}\n    return 2 * idx + direction_map[direction]\n</code></pre>"},{"location":"autoapi/gallifrey/utils/tree_helper/#gallifrey.utils.tree_helper.get_depth","title":"<code>get_depth(idx, tolerance=1e-10)</code>","text":"<p>Get the depth of a node in a binary tree.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>ScalarInt</code> <p>The index of the node.</p> required <code>tolerance</code> <code>ScalarFloat</code> <p>On occasion, floating point errors may lead to a depth that is off by one. This can happen whenever the index is a power of 2. To avoid this, a tolerance is added to the index before taking the logarithm. By default 1e-10.</p> <code>1e-10</code> <p>Returns:</p> Type Description <code>int</code> <p>The depth of the node.</p> Source code in <code>gallifrey/utils/tree_helper.py</code> <pre><code>def get_depth(idx: ScalarInt, tolerance: ScalarFloat = 1e-10) -&gt; ScalarInt:\n    \"\"\"\n    Get the depth of a node in a binary tree.\n\n    Parameters\n    ----------\n    idx : ScalarInt\n        The index of the node.\n    tolerance : ScalarFloat, optional\n        On occasion, floating point errors may lead to a depth that is off by one.\n        This can happen whenever the index is a power of 2. To avoid this, a tolerance\n        is added to the index before taking the logarithm. By default 1e-10.\n\n    Returns\n    -------\n    int\n        The depth of the node.\n    \"\"\"\n    return jnp.log2(idx + 1 + tolerance).astype(int)\n</code></pre>"},{"location":"autoapi/gallifrey/utils/tree_helper/#gallifrey.utils.tree_helper.get_leftmost_leaf","title":"<code>get_leftmost_leaf(idx, max_depth)</code>","text":"<p>For a tree with a given maximum depth, get the index of the leftmost child leaf at the maximum depth from a given index (in level order notation).</p> <p>Since the equation for a left child is 2 * idx + 1, the left child of the left child is 4i+3, and in general the left-most child at max depth d is 2^(max_depth-current_depth)idx + 2^(max_depth-current_depth)-1 = 2^(max_depth-current_depth)*(idx+1) - 1., which can be efficiently implemented using bit-shift operations.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>_type_</code> <p>description</p> required <code>max_depth</code> <code>_type_</code> <p>description</p> required <p>Returns:</p> Type Description <code>ScalarInt</code> <p>The index of the leftmost leaf at the maximum depth.</p> Source code in <code>gallifrey/utils/tree_helper.py</code> <pre><code>def get_leftmost_leaf(idx: ScalarInt, max_depth: ScalarInt) -&gt; ScalarInt:\n    \"\"\"\n    For a tree with a given maximum depth, get the index of the leftmost child\n    leaf at the maximum depth from a given index (in level order notation).\n\n    Since the equation for a left child is 2 * idx + 1, the left child of the left\n    child is 4*i+3, and in general the left-most child at max depth d is\n    2^(max_depth-current_depth)*idx + 2^(max_depth-current_depth)-1 =\n    2^(max_depth-current_depth)*(idx+1) - 1., which\n    can be efficiently implemented using bit-shift operations.\n\n\n    Parameters\n    ----------\n    idx : _type_\n        _description_\n    max_depth : _type_\n        _description_\n\n    Returns\n    -------\n    ScalarInt\n        The index of the leftmost leaf at the maximum depth.\n    \"\"\"\n    current_depth = get_depth(idx)\n    diff = max_depth - current_depth\n    return jnp.left_shift(idx + 1, diff) - 1\n</code></pre>"},{"location":"autoapi/gallifrey/utils/tree_helper/#gallifrey.utils.tree_helper.get_parameter_leaf_idx","title":"<code>get_parameter_leaf_idx(tree_level_idx, max_depth)</code>","text":"<p>Get the index in the parameter array for a given node in the tree expression.</p> <p>We work with the convention that, if a leaf node is not at the maximum depth, the parameters are stored in the leftmost leaf at the maximum depth. Substracting the index of the very left most leaf (from the root) therefore gives a unique index for each possible leaf node in the tree.</p> <p>Parameters:</p> Name Type Description Default <code>tree_level_idx</code> <code>ScalarInt</code> <p>The index of the node in the tree expression (level order).</p> required <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the tree.</p> required <p>Returns:</p> Type Description <code>ScalarInt</code> <p>The index of the parameters in the parameter array for a given node.</p> Source code in <code>gallifrey/utils/tree_helper.py</code> <pre><code>def get_parameter_leaf_idx(\n    tree_level_idx: ScalarInt,\n    max_depth: ScalarInt,\n) -&gt; ScalarInt:\n    \"\"\"\n    Get the index in the parameter array for a given node in the tree expression.\n\n    We work with the convention that, if a leaf node is not at the maximum depth,\n    the parameters are stored in the leftmost leaf at the maximum depth. Substracting\n    the index of the very left most leaf (from the root) therefore gives a unique index\n    for each possible leaf node in the tree.\n\n    Parameters\n    ----------\n    tree_level_idx : ScalarInt\n        The index of the node in the tree expression (level order).\n    max_depth : ScalarInt\n        The maximum depth of the tree.\n\n    Returns\n    -------\n    ScalarInt\n        The index of the parameters in the parameter array for a given node.\n\n    \"\"\"\n    return get_leftmost_leaf(tree_level_idx, max_depth) - get_leftmost_leaf(\n        0, max_depth\n    )\n</code></pre>"},{"location":"autoapi/gallifrey/utils/tree_helper/#gallifrey.utils.tree_helper.get_parent_idx","title":"<code>get_parent_idx(idx)</code>","text":"<p>Get the index of the parent of a node in a binary tree, given by floor((idx - 1) / 2).</p> <p>Compare: https://en.wikipedia.org/wiki/Binary_tree#Arrays</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>ScalarInt</code> <p>The index of the node.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The index of the parent.</p> Source code in <code>gallifrey/utils/tree_helper.py</code> <pre><code>def get_parent_idx(idx: ScalarInt) -&gt; ScalarInt:\n    \"\"\"\n    Get the index of the parent of a node in a binary tree,\n    given by floor((idx - 1) / 2).\n\n    Compare: https://en.wikipedia.org/wiki/Binary_tree#Arrays\n\n    Parameters\n    ----------\n    idx : ScalarInt\n        The index of the node.\n\n    Returns\n    -------\n    int\n        The index of the parent.\n    \"\"\"\n    return (idx - 1) // 2\n</code></pre>"},{"location":"autoapi/gallifrey/utils/tree_helper/#gallifrey.utils.tree_helper.get_subtree","title":"<code>get_subtree(tree_expression, subtree_root_idx, new_root_idx=None)</code>","text":"<p>Retrieve a subtree from a level order tree expression, starting at a given attachment node.</p> <p>The tree can be re-rooted at a different node, if wanted.</p> <p>Parameters:</p> Name Type Description Default <code>tree_expression</code> <code>Int[ndarray, ' D']</code> <p>The tree expression, given in level order notation.</p> required <code>subtree_root_idx</code> <code>ScalarInt</code> <p>The index of node where the subtree is rooted.</p> required <code>new_root_idx</code> <code>Optional[ScalarInt]</code> <p>The new index of the root node, if the subtree is to be re-rooted, by default None. If None, the subtree is returned in its original location.</p> <code>None</code> <p>Returns:</p> Type Description <code>Int[ndarray, ' D']</code> <p>The (level order) tree expression of the subtree, same length as the input tree expression.</p> <code>Int[ndarray, '2 D']</code> <p>The indices of the nodes in the subtree. Two rows, the first row contains the indices in the original tree, the second row contains the indices in the subtree. (Will be the same if the tree is not re-rooted.)</p> Source code in <code>gallifrey/utils/tree_helper.py</code> <pre><code>def get_subtree(\n    tree_expression: Int[jnp.ndarray, \" D\"],\n    subtree_root_idx: ScalarInt,\n    new_root_idx: tp.Optional[ScalarInt] = None,\n) -&gt; tuple[\n    Int[jnp.ndarray, \" D\"],\n    Int[jnp.ndarray, \"2 D\"],\n]:\n    \"\"\"\n    Retrieve a subtree from a level order tree expression, starting\n    at a given attachment node.\n\n    The tree can be re-rooted at a different node, if wanted.\n\n    Parameters\n    ----------\n    tree_expression : Int[jnp.ndarray, \" D\"]\n        The tree expression, given in level order notation.\n    subtree_root_idx : ScalarInt\n        The index of node where the subtree is rooted.\n    new_root_idx : tp.Optional[ScalarInt], optional\n        The new index of the root node, if the subtree is to be re-rooted,\n        by default None. If None, the subtree is returned in its original\n        location.\n\n    Returns\n    -------\n    Int[jnp.ndarray, \" D\"]\n        The (level order) tree expression of the subtree, same length as the\n        input tree expression.\n    Int[jnp.ndarray, \"2 D\"]\n        The indices of the nodes in the subtree. Two rows, the first row\n        contains the indices in the original tree, the second row contains\n        the indices in the subtree. (Will be the same if the tree is not\n        re-rooted.)\n\n    \"\"\"\n    max_nodes = tree_expression.size\n\n    # if tree_expression[subtree_root_idx] == -1:\n    #     raise ValueError(\"The start index must be a non-empty node.\")\n\n    new_root_idx = subtree_root_idx if new_root_idx is None else new_root_idx\n\n    # create initial state\n    new_tree_expression = jnp.full(max_nodes, -1)\n    index_array = jnp.full((2, max_nodes), -1)\n    index_pointer = 0\n    stack = jnp.copy(new_tree_expression).at[0].set(subtree_root_idx)\n    new_stack = jnp.copy(stack).at[0].set(new_root_idx)\n    stack_pointer = 0\n\n    initial_state = (\n        new_tree_expression,\n        index_array,\n        index_pointer,\n        stack,\n        new_stack,\n        stack_pointer,\n    )\n\n    subtree_expression, index_array, _ = _get_subtree(\n        tree_expression,\n        initial_state,\n        max_nodes,\n    )\n\n    index_array = index_array\n    # if jnp.any(index_array &gt;= max_nodes):\n    #     raise ValueError(\n    #         \"The index array is out of bounds. This could happen if the tree \"\n    #         \"is re-rooted to a spot where one of its leaves exceeds the \"\n    #         \"maximum number of nodes.\"\n    #     )\n    return subtree_expression, index_array\n</code></pre>"},{"location":"autoapi/gallifrey/utils/tree_helper/#gallifrey.utils.tree_helper.pick_random_node","title":"<code>pick_random_node(rng_key, post_level_map)</code>","text":"<p>Randomly pick a node from the post-order expression, return the corresponding node in level order. In the current implementation this is done by sampling uniformly from the non-empty nodes.</p> <p>The post_level_map contains all indices used in the the level order expression, which makes it extremely easy to sample a node uniformly at random. It is assumed at empty nodes are represented by -1 in the post_level_map, while all other nodes are have values &gt;= 0.</p> <p>Parameters:</p> Name Type Description Default <code>rng_key</code> <code>PRNGKeyArray</code> <p>Random key.</p> required <code>post_level_map</code> <code> Int[jnp.ndarray, \" D\"]</code> <p>The map from post-order to level-order.</p> required <p>Returns:</p> Type Description <code>ScalarInt</code> <p>The randomly chosen node (in level order notation).</p> Source code in <code>gallifrey/utils/tree_helper.py</code> <pre><code>@jit\ndef pick_random_node(\n    rng_key: PRNGKeyArray,\n    post_level_map: Int[jnp.ndarray, \" D\"],\n) -&gt; ScalarInt:\n    \"\"\"\n    Randomly pick a node from the post-order expression, return the\n    corresponding node in level order. In the current implementation\n    this is done by sampling uniformly from the non-empty nodes.\n\n    The post_level_map contains all indices used in the the level\n    order expression, which makes it extremely easy to sample a node\n    uniformly at random. It is assumed at empty nodes are represented\n    by -1 in the post_level_map, while all other nodes are have\n    values &gt;= 0.\n\n    Parameters\n    ----------\n    rng_key : PRNGKeyArray\n        Random key.\n    post_level_map :  Int[jnp.ndarray, \" D\"]\n        The map from post-order to level-order.\n\n    Returns\n    -------\n    ScalarInt\n        The randomly chosen node (in level order notation).\n\n    \"\"\"\n    # set weights, mask empty nodes\n    weights = jnp.where(post_level_map &gt;= 0, 1.0, 0.0)\n    return jr.choice(rng_key, post_level_map, p=weights)\n</code></pre>"},{"location":"autoapi/gallifrey/utils/tree_helper/#gallifrey.utils.tree_helper.reconstruct_path","title":"<code>reconstruct_path(start_idx, end_idx, max_depth, stop_probability=0.5, go_left_probability=0.5)</code>","text":"<p>Given the start index and the end index of a path in a binary tree, reconstruct the path taken from the start index to the end index and calculate the log probability of reaching the end index. (This is the inverse of the generate_random_path function.)</p> <p>Parameters:</p> Name Type Description Default <code>start_idx</code> <code>ScalarInt</code> <p>The index of the starting node.</p> required <code>end_idx</code> <code>ScalarInt</code> <p>The index of the final node.</p> required <code>max_depth</code> <code>ScalarInt</code> <p>The maximum depth of the tree.</p> required <code>stop_probability</code> <code>ScalarFloat</code> <p>The probability of stopping at each level, by default 0.5.</p> <code>0.5</code> <code>go_left_probability</code> <code>ScalarFloat</code> <p>The probability of choosing the left child at each level, by default 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code> Int[jnp.ndarray, \" D\"]</code> <p>An array of indices representing the path taken, all parent indices from the start_idx to the final chosen index (in level order), excluding the final index.</p> <code>ScalarFloat</code> <p>The log probability of reaching the chosen idx. The probability is calculated by multiplying the probabilities of stopping at at each index along the path and the probabilities of choosing the left or right child at each level.</p> Source code in <code>gallifrey/utils/tree_helper.py</code> <pre><code>def reconstruct_path(\n    start_idx: ScalarInt,\n    end_idx: ScalarInt,\n    max_depth: ScalarInt,\n    stop_probability: ScalarFloat = 0.5,\n    go_left_probability: ScalarFloat = 0.5,\n) -&gt; tuple[\n    Int[jnp.ndarray, \" D\"],\n    ScalarFloat,\n]:\n    \"\"\"\n    Given the start index and the end index of a path in a binary tree,\n    reconstruct the path taken from the start index to the end index and\n    calculate the log probability of reaching the end index.\n    (This is the inverse of the generate_random_path function.)\n\n    Parameters\n    ----------\n    start_idx : ScalarInt\n        The index of the starting node.\n    end_idx : ScalarInt\n        The index of the final node.\n    max_depth : ScalarInt\n        The maximum depth of the tree.\n    stop_probability : ScalarFloat, optional\n        The probability of stopping at each level, by default 0.5.\n    go_left_probability : ScalarFloat, optional\n        The probability of choosing the left child at each\n        level, by default 0.5.\n\n    Returns\n    -------\n     Int[jnp.ndarray, \" D\"]\n        An array of indices representing the path taken, all\n        parent indices from the start_idx to the final chosen index\n        (in level order), excluding the final index.\n    ScalarFloat\n        The log probability of reaching the chosen idx. The probability\n        is calculated by multiplying the probabilities of stopping at\n        at each index along the path and the probabilities\n        of choosing the left or right child at each level.\n    \"\"\"\n\n    index_array = jnp.full(max_depth + 1, -1)\n\n    path_array, num_indices, log_probability = _reconstruct_path(\n        start_idx,\n        end_idx,\n        max_depth,\n        index_array,\n        stop_probability,\n        go_left_probability,\n    )\n\n    # reverse the path array to get the path from start to end\n    # (NOTE: To keep array static but only reverse filled part,\n    # we reverse the array, then roll last part back to the front).\n    path_array = jnp.roll(path_array[::-1], num_indices)\n    return path_array, log_probability\n</code></pre>"},{"location":"autoapi/gallifrey/utils/typing/","title":"typing","text":""}]}